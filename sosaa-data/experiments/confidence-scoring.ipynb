{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e664bc",
   "metadata": {
    "papermill": {
     "duration": 0.023998,
     "end_time": "2023-01-25T10:00:58.914605",
     "exception": false,
     "start_time": "2023-01-25T10:00:58.890607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import copy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995374a",
   "metadata": {
    "papermill": {
     "duration": 11.586791,
     "end_time": "2023-01-25T10:01:10.517703",
     "exception": false,
     "start_time": "2023-01-25T10:00:58.930912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import joblib\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patheffects\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlined = [\n",
    "    mpl.patheffects.Stroke(linewidth=3, foreground=\"white\"),\n",
    "    mpl.patheffects.Normal(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8e97b",
   "metadata": {
    "papermill": {
     "duration": 0.017674,
     "end_time": "2023-01-25T10:01:10.592952",
     "exception": false,
     "start_time": "2023-01-25T10:01:10.575278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrajectoryPaths = namedtuple(\"TrajectoryPaths\", [\"date\", \"out\", \"aer\", \"ant\", \"bio\", \"met\"])\n",
    "TrajectoryDatasets = namedtuple(\"TrajectoryDatasets\", [\"date\", \"out\", \"aer\", \"ant\", \"bio\", \"met\"])\n",
    "MLDataset = namedtuple(\"MLDataset\", [\"date\", \"paths\", \"X_raw\", \"Y_raw\", \"X_train\", \"X_valid\", \"X_test\", \"Y_train\", \"Y_valid\", \"Y_test\", \"X_scaler\", \"Y_scaler\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384f3e9",
   "metadata": {
    "papermill": {
     "duration": 0.01736,
     "end_time": "2023-01-25T10:01:10.625304",
     "exception": false,
     "start_time": "2023-01-25T10:01:10.607944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTDIR_PATTERN = re.compile(r\"(\\d{4})(\\d{2})(\\d{2})_T(\\d{2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04359d6",
   "metadata": {
    "papermill": {
     "duration": 2.78927,
     "end_time": "2023-01-25T10:01:13.502247",
     "exception": false,
     "start_time": "2023-01-25T10:01:10.712977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "traj_datetimes = dict()\n",
    "\n",
    "base = Path.cwd().parent / \"trajectories\"\n",
    "\n",
    "for child in (base / \"outputs\" / \"baseline\").iterdir():\n",
    "    if not child.is_dir():\n",
    "        continue\n",
    "    \n",
    "    match = OUTDIR_PATTERN.match(child.name)\n",
    "    \n",
    "    if match is None:\n",
    "        continue\n",
    "        \n",
    "    date = datetime.datetime(\n",
    "        year=int(match.group(1)),\n",
    "        month=int(match.group(2)),\n",
    "        day=int(match.group(3)),\n",
    "        hour=int(match.group(4)),\n",
    "    )\n",
    "    \n",
    "    out_path = child / \"output.nc\"\n",
    "    aer_path = (\n",
    "        base / \"inputs\" / \"baseline\" / \"HYDE_BASE_Y2018\" /\n",
    "        f\"OUTPUT_bwd_{date.strftime('%Y%m%d')}\" /\n",
    "        \"EMISSIONS_0422\" /\n",
    "        f\"{date.strftime('%Y%m%d')}_7daybwd_Hyde_traj_AER_{24-date.hour:02}_L3.nc\"\n",
    "    )\n",
    "    ant_path = (\n",
    "        base / \"inputs\" / \"baseline\" / \"HYDE_BASE_Y2018\" /\n",
    "        f\"OUTPUT_bwd_{date.strftime('%Y%m%d')}\" /\n",
    "        \"EMISSIONS_0422\" /\n",
    "        f\"{date.strftime('%Y%m%d')}_7daybwd_Hyde_traj_ANT_{24-date.hour:02}_L3.nc\"\n",
    "    )\n",
    "    bio_path = (\n",
    "        base / \"inputs\" / \"baseline\" / \"HYDE_BASE_Y2018\" /\n",
    "        f\"OUTPUT_bwd_{date.strftime('%Y%m%d')}\" /\n",
    "        \"EMISSIONS_0422\" /\n",
    "        f\"{date.strftime('%Y%m%d')}_7daybwd_Hyde_traj_BIO_{24-date.hour:02}_L3.nc\"\n",
    "    )\n",
    "    met_path = (\n",
    "        base / \"inputs\" / \"baseline\" / \"HYDE_BASE_Y2018\" /\n",
    "        f\"OUTPUT_bwd_{date.strftime('%Y%m%d')}\" /\n",
    "        \"METEO\" /\n",
    "        f\"METEO_{date.strftime('%Y%m%d')}_R{24-date.hour:02}.nc\"\n",
    "    )\n",
    "    \n",
    "    if (\n",
    "        (not out_path.exists()) or (not aer_path.exists()) or\n",
    "        (not ant_path.exists()) or (not bio_path.exists()) or\n",
    "        (not met_path.exists())\n",
    "    ):\n",
    "        raise Exception(out_path, aer_path, ant_path, bio_path, met_path)\n",
    "    \n",
    "    traj_datetimes[date] = TrajectoryPaths(\n",
    "        date=date, out=out_path, aer=aer_path, ant=ant_path, bio=bio_path, met=met_path,\n",
    "    )\n",
    "\n",
    "traj_dates = sorted(set(d.date() for d in traj_datetimes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d551d7d",
   "metadata": {
    "papermill": {
     "duration": 0.018564,
     "end_time": "2023-01-25T10:01:13.535960",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.517396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_trajectory_dataset(paths: TrajectoryPaths) -> TrajectoryDatasets:\n",
    "    outds = Dataset(paths.out, \"r\", format=\"NETCDF4\")\n",
    "    aerds = Dataset(paths.aer, \"r\", format=\"NETCDF4\")\n",
    "    antds = Dataset(paths.ant, \"r\", format=\"NETCDF4\")\n",
    "    biods = Dataset(paths.bio, \"r\", format=\"NETCDF4\")\n",
    "    metds = Dataset(paths.met, \"r\", format=\"NETCDF4\")\n",
    "    \n",
    "    return TrajectoryDatasets(\n",
    "        date=paths.date, out=outds, aer=aerds, ant=antds, bio=biods, met=metds,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ceb5a",
   "metadata": {
    "papermill": {
     "duration": 0.017935,
     "end_time": "2023-01-25T10:01:13.568160",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.550225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_proj = ccrs.PlateCarree()\n",
    "projection = ccrs.LambertConformal(\n",
    "    central_latitude=50, central_longitude=20, standard_parallels=(25, 25)\n",
    ")\n",
    "extent = [-60, 60, 40, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ccn_concentration(ds: TrajectoryDatasets):\n",
    "    ccn_bin_indices, = np.nonzero(ds.out[\"dp_dry_fs\"][:].data > 80e-9)\n",
    "    ccn_concentration = np.sum(ds.out[\"nconc_par\"][:].data[:,ccn_bin_indices,:], axis=1)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"ccn\": ccn_concentration.flatten(),\n",
    "    }).set_index([\"time\", \"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6040303",
   "metadata": {
    "papermill": {
     "duration": 0.018385,
     "end_time": "2023-01-25T10:01:13.601713",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.583328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_output_time(ds: TrajectoryDatasets):\n",
    "    fdom = datetime.datetime.strptime(\n",
    "        ds.out[\"time\"].__dict__[\"first_day_of_month\"], \"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "    dt = (ds.date - fdom).total_seconds()\n",
    "    \n",
    "    out_t = ds.out[\"time\"][:].data\n",
    "    \n",
    "    return out_t - dt\n",
    "\n",
    "def interpolate_meteorology_values(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    met_t = ds.met[\"time\"][:].data\n",
    "    met_h = ds.met[\"lev\"][:].data\n",
    "    \n",
    "    met_t_h = ds.met[key][:]\n",
    "    \n",
    "    met_t_h_int = sp.interpolate.interp2d(\n",
    "        x=met_h, y=met_t, z=met_t_h, kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    return met_t_h_int(x=out_h, y=out_t)\n",
    "\n",
    "def interpolate_meteorology_time_values(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    met_t = ds.met[\"time\"][:].data\n",
    "    \n",
    "    met_t_v = ds.met[key][:]\n",
    "    \n",
    "    met_t_int = sp.interpolate.interp1d(\n",
    "        x=met_t, y=met_t_v, kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    return np.repeat(\n",
    "        met_t_int(x=out_t).reshape(-1, 1),\n",
    "        out_h.shape[0], axis=1,\n",
    "    )\n",
    "\n",
    "def interpolate_biogenic_emissions(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    # depth of each box layer, assuming level heights are midpoints and end points are clamped\n",
    "    out_d = (np.array(list(out_h[1:])+[out_h[-1]]) - np.array([out_h[0]]+list(out_h[:-1]))) / 2.0\n",
    "    \n",
    "    bio_t = ds.bio[\"time\"][:].data\n",
    "    \n",
    "    # Biogenic emissions are limited to boxes at <= 10m height\n",
    "    biogenic_emission_layers = np.nonzero(out_h <= 10.0)\n",
    "    biogenic_emission_layer_height_cumsum = np.cumsum(out_d[biogenic_emission_layers])\n",
    "    biogenic_emission_layer_proportion = biogenic_emission_layer_height_cumsum / biogenic_emission_layer_height_cumsum[-1]\n",
    "    num_biogenic_emission_layers = sum(out_h <= 10.0)\n",
    "    \n",
    "    bio_t_h = np.zeros(shape=(out_t.size, out_h.size))\n",
    "    \n",
    "    bio_t_int = sp.interpolate.interp1d(\n",
    "        x=bio_t, y=ds.bio[key][:], kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    # Split up the biogenic emissions relative to the depth of the boxes\n",
    "    bio_t_h[:,biogenic_emission_layers] = (\n",
    "        np.tile(bio_t_int(x=out_t), (num_biogenic_emission_layers, 1, 1)) * biogenic_emission_layer_proportion.reshape(-1, 1, 1)\n",
    "    ).T\n",
    "    \n",
    "    return bio_t_h\n",
    "\n",
    "def interpolate_aerosol_emissions(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    aer_t = ds.aer[\"time\"][:].data\n",
    "    aer_h = ds.aer[\"mid_layer_height\"][:].data\n",
    "    \n",
    "    aer_t_h = ds.aer[key][:].T\n",
    "    \n",
    "    aer_t_h_int = sp.interpolate.interp2d(\n",
    "        x=aer_h, y=aer_t, z=aer_t_h, kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    return aer_t_h_int(x=out_h, y=out_t)\n",
    "\n",
    "def interpolate_anthropogenic_emissions(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    ant_t = ds.ant[\"time\"][:].data\n",
    "    ant_h = ds.ant[\"mid_layer_height\"][:].data\n",
    "    \n",
    "    ant_t_h = ds.ant[key][:].T\n",
    "    \n",
    "    ant_t_h_int = sp.interpolate.interp2d(\n",
    "        x=ant_h, y=ant_t, z=ant_t_h, kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    return ant_t_h_int(x=out_h, y=out_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953273d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meteorology_features(ds: TrajectoryDatasets):\n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"met_t\": interpolate_meteorology_values(ds, \"t\").flatten(),\n",
    "        # \"met_u\": interpolate_meteorology_values(ds, \"u\").flatten(),\n",
    "        # \"met_v\": interpolate_meteorology_values(ds, \"v\").flatten(),\n",
    "        \"met_q\": interpolate_meteorology_values(ds, \"q\").flatten(),\n",
    "        # \"met_qc\": interpolate_meteorology_values(ds, \"qc\").flatten(),\n",
    "        # \"met_sp\": interpolate_meteorology_time_values(ds, \"sp\").flatten(),\n",
    "        # \"met_cp\": interpolate_meteorology_time_values(ds, \"cp\").flatten(),\n",
    "        # \"met_sshf\": interpolate_meteorology_time_values(ds, \"sshf\").flatten(),\n",
    "        \"met_ssr\": interpolate_meteorology_time_values(ds, \"ssr\").flatten(),\n",
    "        # \"met_lsp\": interpolate_meteorology_time_values(ds, \"lsp\").flatten(),\n",
    "        # \"met_ewss\": interpolate_meteorology_time_values(ds, \"ewss\").flatten(),\n",
    "        # \"met_nsss\": interpolate_meteorology_time_values(ds, \"nsss\").flatten(),\n",
    "        # \"met_tcc\": interpolate_meteorology_time_values(ds, \"tcc\").flatten(),\n",
    "        \"met_lsm\": interpolate_meteorology_time_values(ds, \"lsm\").flatten(),\n",
    "        # \"met_omega\": interpolate_meteorology_values(ds, \"omega\").flatten(),\n",
    "        # \"met_z\": interpolate_meteorology_time_values(ds, \"z\").flatten(),\n",
    "        # \"met_mla\": interpolate_meteorology_values(ds, \"mla\").flatten(),\n",
    "        # NOTE: lp is excluded because it allows the model to overfit\n",
    "        # \"met_lp\": interpolate_meteorology_values(ds, \"lp\").flatten(),\n",
    "        \"met_blh\": interpolate_meteorology_time_values(ds, \"blh\").flatten(),\n",
    "    }).set_index([\"time\", \"level\"])\n",
    "\n",
    "def get_bio_emissions_features(ds: TrajectoryDatasets):\n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"bio_acetaldehyde\": interpolate_biogenic_emissions(ds, \"acetaldehyde\").flatten(),\n",
    "        \"bio_acetone\": interpolate_biogenic_emissions(ds, \"acetone\").flatten(),\n",
    "        \"bio_butanes_and_higher_alkanes\": interpolate_biogenic_emissions(ds, \"butanes-and-higher-alkanes\").flatten(),\n",
    "        \"bio_butanes_and_higher_alkenes\": interpolate_biogenic_emissions(ds, \"butenes-and-higher-alkenes\").flatten(),\n",
    "        \"bio_ch4\": interpolate_biogenic_emissions(ds, \"CH4\").flatten(),\n",
    "        \"bio_co\": interpolate_biogenic_emissions(ds, \"CO\").flatten(),\n",
    "        \"bio_ethane\": interpolate_biogenic_emissions(ds, \"ethane\").flatten(),\n",
    "        \"bio_ethanol\": interpolate_biogenic_emissions(ds, \"ethanol\").flatten(),\n",
    "        \"bio_ethene\": interpolate_biogenic_emissions(ds, \"ethene\").flatten(),\n",
    "        \"bio_formaldehyde\": interpolate_biogenic_emissions(ds, \"formaldehyde\").flatten(),\n",
    "        \"bio_hydrogen_cyanide\": interpolate_biogenic_emissions(ds, \"hydrogen-cyanide\").flatten(),\n",
    "        \"bio_iosprene\": interpolate_biogenic_emissions(ds, \"isoprene\").flatten(),\n",
    "        \"bio_mbo\": interpolate_biogenic_emissions(ds, \"MBO\").flatten(),\n",
    "        \"bio_methanol\": interpolate_biogenic_emissions(ds, \"methanol\").flatten(),\n",
    "        \"bio_methyl_bromide\": interpolate_biogenic_emissions(ds, \"methyl-bromide\").flatten(),\n",
    "        \"bio_methyl_chloride\": interpolate_biogenic_emissions(ds, \"methyl-chloride\").flatten(),\n",
    "        \"bio_methyl_iodide\": interpolate_biogenic_emissions(ds, \"methyl-iodide\").flatten(),\n",
    "        \"bio_other_aldehydes\": interpolate_biogenic_emissions(ds, \"other-aldehydes\").flatten(),\n",
    "        \"bio_other_ketones\": interpolate_biogenic_emissions(ds, \"other-ketones\").flatten(),\n",
    "        \"bio_other_monoterpenes\": interpolate_biogenic_emissions(ds, \"other-monoterpenes\").flatten(),\n",
    "        \"bio_pinene_a\": interpolate_biogenic_emissions(ds, \"pinene-a\").flatten(),\n",
    "        \"bio_pinene_b\": interpolate_biogenic_emissions(ds, \"pinene-b\").flatten(),\n",
    "        \"bio_propane\": interpolate_biogenic_emissions(ds, \"propane\").flatten(),\n",
    "        \"bio_propene\": interpolate_biogenic_emissions(ds, \"propene\").flatten(),\n",
    "        \"bio_sesquiterpenes\": interpolate_biogenic_emissions(ds, \"sesquiterpenes\").flatten(),\n",
    "        \"bio_toluene\": interpolate_biogenic_emissions(ds, \"toluene\").flatten(),\n",
    "        \"bio_ch2br2\": interpolate_biogenic_emissions(ds, \"CH2Br2\").flatten(),\n",
    "        \"bio_ch3i\": interpolate_biogenic_emissions(ds, \"CH3I\").flatten(),\n",
    "        \"bio_chbr3\": interpolate_biogenic_emissions(ds, \"CHBr3\").flatten(),\n",
    "        \"bio_dms\": interpolate_biogenic_emissions(ds, \"DMS\").flatten(),\n",
    "    }).set_index([\"time\", \"level\"])\n",
    "\n",
    "def get_aer_emissions_features(ds: TrajectoryDatasets):\n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"aer_3_10_nm\": interpolate_aerosol_emissions(ds, \"3-10nm\").flatten(),\n",
    "        \"aer_10_20_nm\": interpolate_aerosol_emissions(ds, \"10-20nm\").flatten(),\n",
    "        \"aer_20_30_nm\": interpolate_aerosol_emissions(ds, \"20-30nm\").flatten(),\n",
    "        \"aer_30_50_nm\": interpolate_aerosol_emissions(ds, \"30-50nm\").flatten(),\n",
    "        \"aer_50_70_nm\": interpolate_aerosol_emissions(ds, \"50-70nm\").flatten(),\n",
    "        \"aer_70_100_nm\": interpolate_aerosol_emissions(ds, \"70-100nm\").flatten(),\n",
    "        \"aer_100_200_nm\": interpolate_aerosol_emissions(ds, \"100-200nm\").flatten(),\n",
    "        \"aer_200_400_nm\": interpolate_aerosol_emissions(ds, \"200-400nm\").flatten(),\n",
    "        \"aer_400_1000_nm\": interpolate_aerosol_emissions(ds, \"400-1000nm\").flatten(),\n",
    "    }).set_index([\"time\", \"level\"])\n",
    "\n",
    "def get_ant_emissions_features(ds: TrajectoryDatasets):\n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"ant_co\": interpolate_anthropogenic_emissions(ds, \"co\").flatten(),\n",
    "        \"ant_nox\": interpolate_anthropogenic_emissions(ds, \"nox\").flatten(),\n",
    "        \"ant_co2\": interpolate_anthropogenic_emissions(ds, \"co2\").flatten(),\n",
    "        \"ant_nh3\": interpolate_anthropogenic_emissions(ds, \"nh3\").flatten(),\n",
    "        \"ant_ch4\": interpolate_anthropogenic_emissions(ds, \"ch4\").flatten(),\n",
    "        \"ant_so2\": interpolate_anthropogenic_emissions(ds, \"so2\").flatten(),\n",
    "        \"ant_nmvoc\": interpolate_anthropogenic_emissions(ds, \"nmvoc\").flatten(),\n",
    "        \"ant_alcohols\": interpolate_anthropogenic_emissions(ds, \"alcohols\").flatten(),\n",
    "        \"ant_ethane\": interpolate_anthropogenic_emissions(ds, \"ethane\").flatten(),\n",
    "        \"ant_propane\": interpolate_anthropogenic_emissions(ds, \"propane\").flatten(),\n",
    "        \"ant_butanes\": interpolate_anthropogenic_emissions(ds, \"butanes\").flatten(),\n",
    "        \"ant_pentanes\": interpolate_anthropogenic_emissions(ds, \"pentanes\").flatten(),\n",
    "        \"ant_hexanes\": interpolate_anthropogenic_emissions(ds, \"hexanes\").flatten(),\n",
    "        \"ant_ethene\": interpolate_anthropogenic_emissions(ds, \"ethene\").flatten(),\n",
    "        \"ant_propene\": interpolate_anthropogenic_emissions(ds, \"propene\").flatten(),\n",
    "        \"ant_acetylene\": interpolate_anthropogenic_emissions(ds, \"acetylene\").flatten(),\n",
    "        \"ant_isoprene\": interpolate_anthropogenic_emissions(ds, \"isoprene\").flatten(),\n",
    "        \"ant_monoterpenes\": interpolate_anthropogenic_emissions(ds, \"monoterpenes\").flatten(),\n",
    "        \"ant_other_alkenes_and_alkynes\": interpolate_anthropogenic_emissions(ds, \"other-alkenes-and-alkynes\").flatten(),\n",
    "        \"ant_benzene\": interpolate_anthropogenic_emissions(ds, \"benzene\").flatten(),\n",
    "        \"ant_toluene\": interpolate_anthropogenic_emissions(ds, \"toluene\").flatten(),\n",
    "        \"ant_xylene\": interpolate_anthropogenic_emissions(ds, \"xylene\").flatten(),\n",
    "        \"ant_trimethylbenzene\": interpolate_anthropogenic_emissions(ds, \"trimethylbenzene\").flatten(),\n",
    "        \"ant_other_aromatics\": interpolate_anthropogenic_emissions(ds, \"other-aromatics\").flatten(),\n",
    "        \"ant_esters\": interpolate_anthropogenic_emissions(ds, \"esters\").flatten(),\n",
    "        \"ant_ethers\": interpolate_anthropogenic_emissions(ds, \"ethers\").flatten(),\n",
    "        \"ant_formaldehyde\": interpolate_anthropogenic_emissions(ds, \"formaldehyde\").flatten(),\n",
    "        \"ant_other_aldehydes\": interpolate_anthropogenic_emissions(ds, \"other-aldehydes\").flatten(),\n",
    "        \"ant_total_ketones\": interpolate_anthropogenic_emissions(ds, \"total-ketones\").flatten(),\n",
    "        \"ant_total_acids\": interpolate_anthropogenic_emissions(ds, \"total-acids\").flatten(),\n",
    "        \"ant_other_vocs\": interpolate_anthropogenic_emissions(ds, \"other-VOCs\").flatten(),\n",
    "    }).set_index([\"time\", \"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e2a63",
   "metadata": {
    "papermill": {
     "duration": 0.029729,
     "end_time": "2023-01-25T10:01:13.769583",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.739854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/67809235\n",
    "def df_to_numpy(df):\n",
    "    try:\n",
    "        shape = [len(level) for level in df.index.levels]\n",
    "    except AttributeError:\n",
    "        shape = [len(df.index)]\n",
    "    ncol = df.shape[-1]\n",
    "    if ncol > 1:\n",
    "        shape.append(ncol)\n",
    "    return df.to_numpy().reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a92854",
   "metadata": {
    "papermill": {
     "duration": 0.019778,
     "end_time": "2023-01-25T10:01:13.804285",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.784507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_time_level_windows():\n",
    "    # -0.5h, -1.5h, -3h, -6h, -12h, -24h, -48h\n",
    "    # 0, -2, -5, -11, -23, -47, -95\n",
    "    time_windows = [(0, 0), (-2, -1), (-5, -3), (-11, -6), (-23, -12), (-47, -24), (-95, -48)]\n",
    "    \n",
    "    # +1l, +2l, +4l, +8l, +16l, +32l, +64\n",
    "    top_windows = [(1, 1), (1, 2), (1, 4), (2, 8), (2, 16), (3, 32), (3, 64)]\n",
    "    mid_windows = [(0, 0), (0, 0), (0, 0), (-1, 1), (-1, 1), (-2, 2), (-2, 2)]\n",
    "    bot_windows = [(-1, -1), (-2, -1), (-4, -1), (-8, -2), (-16, -2), (-32, -3), (-64, -3)]\n",
    "    \n",
    "    return list(itertools.chain(\n",
    "        zip(time_windows, top_windows), zip(time_windows, mid_windows), zip(time_windows, bot_windows),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162ef7f",
   "metadata": {
    "papermill": {
     "duration": 0.018687,
     "end_time": "2023-01-25T10:01:13.838147",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.819460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_windowed_feature_names(columns):\n",
    "    time_windows = [\"-0.5h\", \"-1.5h\", \"-3h\", \"-6h\", \"-12h\", \"-24h\", \"-48h\"]\n",
    "    \n",
    "    top_windows = [\"+1l\", \"+2l\", \"+4l\", \"+8l\", \"+16l\", \"+32l\", \"+64l\"]\n",
    "    mid_windows = [\"+0l\", \"+0l\", \"+0l\", \"±1l\", \"±1l\", \"±2l\", \"±2l\"]\n",
    "    bot_windows = [\"-1l\", \"-2l\", \"-4l\", \"-8l\", \"-16l\", \"-32l\", \"-64l\"]\n",
    "    \n",
    "    names = []\n",
    "    \n",
    "    for (t, l) in itertools.chain(\n",
    "        zip(time_windows, top_windows), zip(time_windows, mid_windows), zip(time_windows, bot_windows),\n",
    "    ):\n",
    "        for c in columns:\n",
    "            names.append(f\"{c}{t}{l}\")\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07592d92",
   "metadata": {
    "papermill": {
     "duration": 0.023248,
     "end_time": "2023-01-25T10:01:13.876476",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.853228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_level_window_mean_v1(input, t_range, l_range):\n",
    "    output = np.zeros(shape=input.shape)\n",
    "\n",
    "    for t in range(input.shape[0]):\n",
    "        for l in range(input.shape[1]):\n",
    "            for f in range(input.shape[2]):\n",
    "                window = input[\n",
    "                    min(max(0, t+t_range[0]), input.shape[0]):max(0, min(t+1+t_range[1], input.shape[0])),\n",
    "                    min(max(0, l+l_range[0]), input.shape[1]):max(0, min(l+1+l_range[1], input.shape[1])),\n",
    "                    f\n",
    "                ]\n",
    "\n",
    "                output[t,l,f] = np.mean(window) if window.size > 0 else 0.0\n",
    "    \n",
    "    return output\n",
    "\n",
    "def time_level_window_mean_v2(input, t_range, l_range):\n",
    "    output = np.zeros(shape=input.shape)\n",
    "\n",
    "    for t in range(input.shape[0]):\n",
    "        mint = min(max(0, t+t_range[0]), input.shape[0])\n",
    "        maxt = max(0, min(t+1+t_range[1], input.shape[0]))\n",
    "        \n",
    "        if mint == maxt:\n",
    "            continue\n",
    "        \n",
    "        for l in range(input.shape[1]):\n",
    "            minl = min(max(0, l+l_range[0]), input.shape[1])\n",
    "            maxl = max(0, min(l+1+l_range[1], input.shape[1]))\n",
    "            \n",
    "            if minl == maxl:\n",
    "                continue\n",
    "                \n",
    "            output[t,l,:] = np.mean(input[mint:maxt,minl:maxl,:], axis=(0,1))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def time_level_window_mean_v3(input, t_range, l_range):\n",
    "    min_t = min(t_range[0], 0)\n",
    "    max_t = max(0, t_range[1])\n",
    "    abs_t = max(abs(min_t), abs(max_t))\n",
    "    \n",
    "    min_l = min(l_range[0], 0)\n",
    "    max_l = max(0, l_range[1])\n",
    "    abs_l = max(abs(min_l), abs(max_l))\n",
    "    \n",
    "    kernel = np.zeros(shape=(abs_t*2 + 1, abs_l*2 + 1, 1))\n",
    "    kernel[t_range[0]+abs_t:t_range[1]+abs_t+1,l_range[0]+abs_l:l_range[1]+abs_l+1,:] = 1.0\n",
    "    kernel = kernel[::-1,::-1]\n",
    "    \n",
    "    quot = sp.ndimage.convolve(np.ones_like(input), kernel, mode='constant', cval=0.0)\n",
    "    \n",
    "    result = np.zeros_like(input)\n",
    "    \n",
    "    np.divide(\n",
    "        sp.ndimage.convolve(input, kernel, mode='constant', cval=0.0),\n",
    "        quot, out=result, where=quot > 0,\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243a406",
   "metadata": {
    "papermill": {
     "duration": 0.018056,
     "end_time": "2023-01-25T10:01:13.909591",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.891535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_features_for_dataset(ds: TrajectoryDatasets):\n",
    "    bio_features = get_bio_emissions_features(ds)\n",
    "    aer_features = get_aer_emissions_features(ds) * 1e21\n",
    "    ant_features = get_ant_emissions_features(ds)\n",
    "    met_features = get_meteorology_features(ds)\n",
    "    \n",
    "    return pd.concat([\n",
    "        bio_features, aer_features, ant_features, met_features,\n",
    "    ], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cf4a1",
   "metadata": {
    "papermill": {
     "duration": 0.019594,
     "end_time": "2023-01-25T10:01:13.943986",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.924392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features_from_raw_features(raw_features):\n",
    "    raw_features_np = df_to_numpy(raw_features)\n",
    "    \n",
    "    features_np = np.concatenate([\n",
    "        raw_features.index.get_level_values(0).to_numpy().reshape(\n",
    "            (raw_features.index.levels[0].size, raw_features.index.levels[1].size, 1)\n",
    "        ),\n",
    "        raw_features.index.get_level_values(1).to_numpy().reshape(\n",
    "            (raw_features.index.levels[0].size, raw_features.index.levels[1].size, 1)\n",
    "        )\n",
    "    ] + joblib.Parallel(n_jobs=-1)([\n",
    "        joblib.delayed(time_level_window_mean_v2)(raw_features_np, t, l) for t, l in generate_time_level_windows()\n",
    "    ]), axis=2)\n",
    "    \n",
    "    # Trim off the first two days, for which the time features are ill-defined\n",
    "    features_np_trimmed = features_np[95:-1,:,:]\n",
    "    \n",
    "    feature_names = [\"time\", \"level\"] + generate_windowed_feature_names(raw_features.columns)\n",
    "    \n",
    "    features = pd.DataFrame(features_np_trimmed.reshape(\n",
    "        features_np_trimmed.shape[0]*features_np_trimmed.shape[1], features_np_trimmed.shape[2],\n",
    "    ), columns=feature_names).set_index([\"time\", \"level\"])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da30b1",
   "metadata": {
    "papermill": {
     "duration": 0.01929,
     "end_time": "2023-01-25T10:01:13.978350",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.959060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_labels_for_dataset(ds: TrajectoryDatasets):\n",
    "    ccn_concentration = get_ccn_concentration(ds)\n",
    "    \n",
    "    ccn_concentration_np = df_to_numpy(ccn_concentration)\n",
    "    \n",
    "    labels_np = np.concatenate([\n",
    "        ccn_concentration.index.get_level_values(0).to_numpy().reshape(\n",
    "            (ccn_concentration.index.levels[0].size, ccn_concentration.index.levels[1].size, 1)\n",
    "        ),\n",
    "        ccn_concentration.index.get_level_values(1).to_numpy().reshape(\n",
    "            (ccn_concentration.index.levels[0].size, ccn_concentration.index.levels[1].size, 1)\n",
    "        ),\n",
    "        ccn_concentration_np.reshape(\n",
    "            (ccn_concentration_np.shape[0], ccn_concentration_np.shape[1], 1)\n",
    "        ),\n",
    "    ], axis=2)\n",
    "    \n",
    "    # Trim off the first two days, for which the time features are ill-defined\n",
    "    labels_np_trimmed = labels_np[96:,:,:]\n",
    "    \n",
    "    label_names = [\"time\", \"level\", \"ccn\"]\n",
    "    \n",
    "    labels = pd.DataFrame(labels_np_trimmed.reshape(\n",
    "        labels_np_trimmed.shape[0]*labels_np_trimmed.shape[1], labels_np_trimmed.shape[2],\n",
    "    ), columns=label_names).set_index([\"time\", \"level\"])\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8315b",
   "metadata": {
    "papermill": {
     "duration": 0.018426,
     "end_time": "2023-01-25T10:01:14.013864",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.995438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hash_for_dt(dt):\n",
    "    if not(isinstance(dt, tuple) or isinstance(dt, list)):\n",
    "        dt = [dt]\n",
    "    \n",
    "    dt_str = '.'.join(dtt.strftime('%d.%m.%Y-%H:00%z') for dtt in dt)\n",
    "    \n",
    "    h = hashlib.shake_256()\n",
    "    h.update(dt_str.encode('ascii'))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e2e8a",
   "metadata": {
    "papermill": {
     "duration": 0.019423,
     "end_time": "2023-01-25T10:01:14.048460",
     "exception": false,
     "start_time": "2023-01-25T10:01:14.029037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clumped 0/1 sampler using a Markov Process\n",
    "\n",
    "P(0) = p and P(1) = 1-p\n",
    "clump = 0 => IID samples\n",
    "clump -> 1 => highly correlated samples\n",
    "\n",
    "\"\"\"\n",
    "class Clump:\n",
    "    def __init__(self, p=0.5, clump=0.0, rng=None):\n",
    "        a = 1 - (1-p)*(1-clump)\n",
    "        b = (1-a)*p/(1-p)\n",
    "        \n",
    "        self.C = np.array([[a, 1-a],[b, 1-b]])\n",
    "        \n",
    "        self.i = 0 if rng.random() < p else 1\n",
    "    \n",
    "    def sample(self, rng):\n",
    "        p = self.C[self.i,0]\n",
    "        u = rng.random()\n",
    "        \n",
    "        self.i = 0 if u < p else 1\n",
    "        \n",
    "        return self.i\n",
    "    \n",
    "    def steady(self, X):\n",
    "        return np.matmul(X, self.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a98ef",
   "metadata": {
    "papermill": {
     "duration": 0.021247,
     "end_time": "2023-01-25T10:01:14.084943",
     "exception": false,
     "start_time": "2023-01-25T10:01:14.063696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, Y, test_size=0.25, random_state=None, shuffle=True, clump=0.0):\n",
    "    assert len(X) == len(Y)\n",
    "    assert type(X) == type(Y)\n",
    "    assert test_size > 0.0\n",
    "    assert test_size < 1.0\n",
    "    assert random_state is not None\n",
    "    assert clump >= 0.0\n",
    "    assert clump < 1.0\n",
    "    \n",
    "    c = Clump(p=test_size, clump=clump, rng=random_state)\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        assert X.index.values.shape == Y.index.values.shape\n",
    "        \n",
    "        # Split only based on the first-level index instead of flattening\n",
    "        n1 = len(X.index.levels[1])\n",
    "        n0 = len(X) // n1\n",
    "        \n",
    "        C = np.array([c.sample(random_state) for _ in range(n0)])\n",
    "        I_train, = np.nonzero(C)\n",
    "        I_train = np.repeat(I_train, n1) * n1 + np.tile(np.arange(n1), len(I_train))\n",
    "        I_test, = np.nonzero(1-C)\n",
    "        I_test = np.repeat(I_test, n1) * n1 + np.tile(np.arange(n1), len(I_test))\n",
    "    else:\n",
    "        C = np.array([c.sample(random_state) for _ in range(len(X))])\n",
    "        I_train, = np.nonzero(C)\n",
    "        I_test, = np.nonzero(1-C)\n",
    "    \n",
    "    if shuffle:\n",
    "        random_state.shuffle(I_train)\n",
    "        random_state.shuffle(I_test)\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_train = X.iloc[I_train]\n",
    "        X_test = X.iloc[I_test]\n",
    "        \n",
    "        Y_train = Y.iloc[I_train]\n",
    "        Y_test = Y.iloc[I_test]\n",
    "    else:\n",
    "        X_train = X[I_train]\n",
    "        X_test = X[I_test]\n",
    "        \n",
    "        Y_train = Y[I_train]\n",
    "        Y_test = Y[I_test]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92aa97",
   "metadata": {
    "papermill": {
     "duration": 0.024469,
     "end_time": "2023-01-25T10:01:14.124122",
     "exception": false,
     "start_time": "2023-01-25T10:01:14.099653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_cache_dataset(dt: datetime.datetime, clump: float, datasets: dict) -> MLDataset:\n",
    "    if isinstance(dt, tuple) or isinstance(dt, list):\n",
    "        dt = tuple(sorted(dt))\n",
    "    \n",
    "    cached = datasets.get((dt, clump))\n",
    "    \n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    \n",
    "    if isinstance(dt, tuple) or isinstance(dt, list):\n",
    "        mls = [load_and_cache_dataset(dtt, clump, datasets) for dtt in dt]\n",
    "\n",
    "        dp = tuple(ml.paths for ml in mls)\n",
    "        X_raw = pd.concat([ml.X_raw for ml in mls], axis='index')\n",
    "        Y = pd.concat([ml.Y_raw for ml in mls], axis='index')\n",
    "\n",
    "        train_features = np.concatenate([ml.X_scaler.inverse_transform(ml.X_train) for ml in mls], axis=0)\n",
    "        train_labels = np.concatenate([ml.Y_scaler.inverse_transform(ml.Y_train) for ml in mls], axis=0)\n",
    "        valid_features = np.concatenate([ml.X_scaler.inverse_transform(ml.X_valid) for ml in mls], axis=0)\n",
    "        valid_labels = np.concatenate([ml.Y_scaler.inverse_transform(ml.Y_valid) for ml in mls], axis=0)\n",
    "        test_features = np.concatenate([ml.X_scaler.inverse_transform(ml.X_test) for ml in mls], axis=0)\n",
    "        test_labels = np.concatenate([ml.Y_scaler.inverse_transform(ml.Y_test) for ml in mls], axis=0)\n",
    "    else:\n",
    "        dp = traj_datetimes[dt]\n",
    "        ds = load_trajectory_dataset(dp)\n",
    "\n",
    "        X_raw = get_raw_features_for_dataset(ds)\n",
    "\n",
    "        X = get_features_from_raw_features(X_raw)\n",
    "        Y = np.log10(get_labels_for_dataset(ds) + 1)\n",
    "\n",
    "        rng = np.random.RandomState(seed=int.from_bytes(hash_for_dt(dt).digest(4), 'little'))\n",
    "\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            X, Y, test_size=0.25, random_state=rng, clump=clump,\n",
    "        )\n",
    "        train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "            train_features, train_labels, test_size=1.0/3.0, random_state=rng, clump=clump,\n",
    "        )\n",
    "\n",
    "        # Close the NetCDF dataset\n",
    "        ds.out.close()\n",
    "\n",
    "    # Scale features to N(0,1)\n",
    "    # - only fit on training data\n",
    "    # - OOD inputs for constants at training time are blown up\n",
    "    feature_scaler = StandardScaler().fit(train_features)\n",
    "    feature_scaler.scale_[np.nonzero(feature_scaler.var_ == 0.0)] = np.nan_to_num(np.inf)\n",
    "\n",
    "    label_scaler = StandardScaler().fit(train_labels)\n",
    "\n",
    "    train_features = feature_scaler.transform(train_features)\n",
    "    train_labels = label_scaler.transform(train_labels)\n",
    "    valid_features = feature_scaler.transform(valid_features)\n",
    "    valid_labels = label_scaler.transform(valid_labels)\n",
    "    test_features = feature_scaler.transform(test_features)\n",
    "    test_labels = label_scaler.transform(test_labels)\n",
    "\n",
    "    dataset = MLDataset(\n",
    "        date=dt, paths=dp, X_raw=X_raw, Y_raw=Y,\n",
    "        X_train=train_features, X_valid=valid_features, X_test=test_features,\n",
    "        Y_train=train_labels, Y_valid=valid_labels, Y_test=test_labels,\n",
    "        X_scaler=feature_scaler, Y_scaler=label_scaler,\n",
    "    )\n",
    "\n",
    "    datasets[(dt, clump)] = dataset\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8216f",
   "metadata": {
    "papermill": {
     "duration": 0.017777,
     "end_time": "2023-01-25T10:01:14.157251",
     "exception": false,
     "start_time": "2023-01-25T10:01:14.139474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class OutOfDistributionDetector(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        return self\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def score(self, X_test):\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return cls.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "class GaussianProcessPercentileDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.model = GaussianProcessRegressor(*args, kernel=(\n",
    "            RBF() + WhiteKernel()\n",
    "        ), random_state=rng, **kwargs)\n",
    "        self.rng = rng\n",
    "    \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        num_X = len(X_id)\n",
    "        \n",
    "        # Safety hatch to limit computing time\n",
    "        if num_X >= 10_000:\n",
    "            I_train = self.rng.choice(len(X_id), size=num_X//6)\n",
    "            I_valid = np.ones(len(X_id))\n",
    "            I_valid[I_train] = 0\n",
    "            I_valid, = np.nonzero(I_valid)\n",
    "            \n",
    "            X_id_valid = np.concatenate([X_id_valid, X_id[I_valid]], axis=0)\n",
    "            Y_id_valid = np.concatenate([Y_id_valid, Y_id[I_valid]], axis=0)\n",
    "            \n",
    "            X_id = X_id[I_train]\n",
    "            Y_id = Y_id[I_train]\n",
    "        \n",
    "        self.model.fit(X_id, Y_id)\n",
    "        \n",
    "        self.uq_valid = np.sort(self.model.predict(X_id_valid, return_std=True)[1])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        return 1.0 - np.searchsorted(\n",
    "            self.uq_valid, self.model.predict(X_test, return_std=True)[1],\n",
    "        ) / len(self.uq_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffed659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "class AutoAssociativeMahalanobisPercentileDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.rng = rng\n",
    "        \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        print(\"Fitting PCA\")\n",
    "        \n",
    "        bn = np.searchsorted(np.cumsum(\n",
    "            PCA(random_state=self.rng).fit(X_id).explained_variance_ratio_\n",
    "        ), 0.95)\n",
    "        \n",
    "        print(f\"Fitting AA with bn={bn}\")\n",
    "        \n",
    "        self.model = MLPRegressor(\n",
    "            hidden_layer_sizes=[X_id.shape[1]*2, bn, X_id.shape[1]*2],\n",
    "            activation=\"relu\", solver=\"adam\", random_state=self.rng,\n",
    "        ).fit(X_id, X_id)\n",
    "        \n",
    "        print(\"Fitting covariance\")\n",
    "        \n",
    "        self.cov = EmpiricalCovariance().fit((self.model.predict(X_id) - X_id))\n",
    "        \n",
    "        self.err_valid = np.sort(self.cov.mahalanobis(\n",
    "            self.model.predict(X_id_valid) - X_id_valid\n",
    "        ))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        return 1.0 - np.searchsorted(\n",
    "            self.err_valid, self.cov.mahalanobis((self.model.predict(X_test) - X_test)),\n",
    "        ) / len(self.err_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "class LogisticClassifierDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.rng = rng\n",
    "        \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        print(\"Fitting PCA\")\n",
    "        \n",
    "        bn = np.searchsorted(np.cumsum(\n",
    "            PCA(random_state=self.rng).fit(X_id).explained_variance_ratio_\n",
    "        ), 0.95)\n",
    "        \n",
    "        print(f\"Fitting AA with bn={bn}\")\n",
    "        \n",
    "        tf.random.set_seed(int.from_bytes(self.rng.bytes(4), \"little\"))\n",
    "        \n",
    "        self.aa_model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=X_id.shape[1:]),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-enc\"),\n",
    "                tf.keras.layers.Dense(bn, activation=\"relu\", name=\"aa-bn\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-dec\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1], name=\"aa-X\"),\n",
    "            ]\n",
    "        )\n",
    "        self.aa_model.compile(optimizer='adam', loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
    "        self.aa_model.fit(\n",
    "            X_id, X_id,\n",
    "            validation_data=(X_id_valid, X_id_valid),\n",
    "            batch_size=200, epochs=200, verbose=1, callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=10,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        print(\"Generate FGSM inputs\")\n",
    "        \n",
    "        x_id = tf.constant(X_id_valid, dtype=tf.float32)\n",
    "        x_id = tf.random.normal(mean=x_id, stddev=0.01, shape=x_id.shape)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_id)\n",
    "\n",
    "            x_pred_id = self.aa_model(x_id)\n",
    "            x_mse_id = (x_pred_id - x_id) ** 2\n",
    "\n",
    "        adv_grad = tape.gradient(x_mse_id, x_id)\n",
    "\n",
    "        X_ood = (x_id + tf.math.sign(adv_grad) * tf.math.abs(tf.random.normal(\n",
    "            mean=2.0, stddev=0.5, shape=[len(X_id_valid), 1],\n",
    "        ))).numpy()\n",
    "        \n",
    "        print(\"Train classifier\")\n",
    "        \n",
    "        self.model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=X_id.shape[1:]),\n",
    "                tf.keras.layers.Dense(bn, activation=\"relu\", name=\"io-bn\"),\n",
    "                tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"id-ood-sigmoid\"),\n",
    "            ]\n",
    "        )\n",
    "        self.model.compile(optimizer='adam', loss=\"binary_crossentropy\")\n",
    "        self.model.fit(\n",
    "            np.concatenate([X_id_valid, X_ood], axis=0),\n",
    "            np.concatenate([np.ones(len(X_id_valid)), np.zeros(len(X_ood))], axis=0),\n",
    "            validation_split=0.1,\n",
    "            batch_size=200, epochs=200, verbose=1, callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=10,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bdb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPokeNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, t, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.t = t\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return tf.math.abs(tf.random.normal(\n",
    "            mean=self.t, stddev=0.1, shape=(tf.shape(inputs)[0], 1),\n",
    "        ))\n",
    "\n",
    "class TPokeClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, indim, bn, aa_model):\n",
    "        input = tf.keras.Input(shape=[indim])\n",
    "        \n",
    "        dense_1 = tf.keras.layers.Dense(bn, activation=\"relu\", name=\"dense-1\")(input)\n",
    "        is_id = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"is-id\")(dense_1)\n",
    "        \n",
    "        super().__init__(input, is_id)\n",
    "        \n",
    "        self.aa_model = aa_model\n",
    "        self.noise = tf.keras.layers.GaussianNoise(0.01)\n",
    "        \n",
    "    def compile(self, *args, **kwargs):\n",
    "        self.t = self.add_weight(initializer=\"ones\", trainable=False, name=\"t\")\n",
    "        self.t.assign(self.t * 2.0)\n",
    "        \n",
    "        super().compile(*args, **kwargs)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x_id = data\n",
    "        \n",
    "        x_ood = self.noise(x_id)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_ood)\n",
    "\n",
    "            x_pred_ood = self.aa_model(x_ood, training=False)\n",
    "            x_mse_ood = (x_pred_ood - x_ood) ** 2\n",
    "\n",
    "        adv_grad = tape.gradient(x_mse_ood, x_ood)\n",
    "        x_ood += tf.stop_gradient(tf.math.sign(adv_grad)) * TPokeNoise(self.t)(x_ood)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            c_id = self.call(x_id)\n",
    "            c_ood = self.call(x_ood)\n",
    "            \n",
    "            loss = self.compiled_loss(c_id, c_ood)\n",
    "            \n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        \n",
    "        self.t.assign(self.t * (1.0 - 0.001 * tf.math.minimum(\n",
    "            tf.math.sign(tf.math.reduce_mean(c_id - 0.95)),\n",
    "            tf.math.sign(tf.math.reduce_mean(0.05 - c_ood)),\n",
    "        )))\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x_id = data\n",
    "        \n",
    "        x_ood = self.noise(x_id)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_ood)\n",
    "\n",
    "            x_pred_ood = self.aa_model(x_ood, training=False)\n",
    "            x_mse_ood = (x_pred_ood - x_ood) ** 2\n",
    "\n",
    "        adv_grad = tape.gradient(x_mse_ood, x_ood)\n",
    "        x_ood += tf.stop_gradient(tf.math.sign(adv_grad)) * TPokeNoise(self.t)(x_ood)\n",
    "        \n",
    "        c_id = self.call(x_id)\n",
    "        c_ood = self.call(x_ood)\n",
    "\n",
    "        loss = self.compiled_loss(c_id, c_ood)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    \n",
    "def id_ood_loss(c_id, c_ood):\n",
    "    return -(tf.math.log(tf.maximum(c_id, 1e-6)) + tf.math.log(tf.maximum(1.0 - c_ood, 1e-6)))\n",
    "\n",
    "tf.keras.utils.get_custom_objects()[\"id_ood_loss\"] = id_ood_loss\n",
    "\n",
    "class LogisticTPokeClassifierDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.rng = rng\n",
    "        \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        print(\"Fitting PCA\")\n",
    "        \n",
    "        bn = np.searchsorted(np.cumsum(\n",
    "            PCA(random_state=self.rng).fit(X_id).explained_variance_ratio_\n",
    "        ), 0.95)\n",
    "        \n",
    "        print(f\"Fitting AA with bn={bn}\")\n",
    "        \n",
    "        tf.random.set_seed(int.from_bytes(self.rng.bytes(4), \"little\"))\n",
    "        \n",
    "        self.aa_model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=X_id.shape[1:]),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-enc\"),\n",
    "                tf.keras.layers.Dense(bn, activation=\"relu\", name=\"aa-bn\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-dec\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1], name=\"aa-X\"),\n",
    "            ]\n",
    "        )\n",
    "        self.aa_model.compile(optimizer='adam', loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
    "        self.aa_model.fit(\n",
    "            X_id, X_id,\n",
    "            validation_data=(X_id_valid, X_id_valid),\n",
    "            batch_size=200, epochs=200, verbose=1, callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=10,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        print(\"Train classifier\")\n",
    "        \n",
    "        self.model = TPokeClassifierModel(X_id.shape[1], bn, self.aa_model)\n",
    "        self.model.compile(optimizer='adam', loss=id_ood_loss)\n",
    "        self.model.fit(X_id_valid, validation_split=0.1,\n",
    "            batch_size=200, epochs=200, verbose=1, callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=25,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        print(f\"T-poking resulted in t={self.model.t}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "class GaussianProcessIDWeightDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.gp_model = GaussianProcessRegressor(*args, kernel=(\n",
    "            RBF() + WhiteKernel()\n",
    "        ), random_state=rng, **kwargs)\n",
    "        self.rng = rng\n",
    "        \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        print(\"Fitting Gaussian Process\")\n",
    "        \n",
    "        num_X = len(X_id)\n",
    "        \n",
    "        # Safety hatch to limit computing time\n",
    "        if num_X >= 10_000:\n",
    "            I_train = self.rng.choice(len(X_id), size=num_X//6)\n",
    "            I_valid = np.ones(len(X_id))\n",
    "            I_valid[I_train] = 0\n",
    "            I_valid, = np.nonzero(I_valid)\n",
    "            \n",
    "            X_id_valid = np.concatenate([X_id_valid, X_id[I_valid]], axis=0)\n",
    "            Y_id_valid = np.concatenate([Y_id_valid, Y_id[I_valid]], axis=0)\n",
    "            \n",
    "            X_id = X_id[I_train]\n",
    "            Y_id = Y_id[I_train]\n",
    "        \n",
    "        self.gp_model.fit(X_id, Y_id)\n",
    "        \n",
    "        print(\"Fitting id_err\")\n",
    "        \n",
    "        id_err = self.gp_model.predict(X_id_valid, return_std=True)[1]\n",
    "        self.err_scale = np.mean(id_err)\n",
    "        id_err /= self.err_scale\n",
    "        self.id_err = KernelDensity(bandwidth=0.1, kernel=\"linear\").fit(id_err.reshape(-1, 1))\n",
    "        \n",
    "        print(\"Fitting all_err\")\n",
    "        \n",
    "        X_all = self.rng.normal(loc=0.0, scale=1.0, size=X_id_valid.shape)\n",
    "        all_err = self.gp_model.predict(X_all, return_std=True)[1] / self.err_scale\n",
    "        self.all_err = KernelDensity(bandwidth=0.1, kernel=\"linear\").fit(all_err.reshape(-1, 1))\n",
    "        \n",
    "        print(\"Fitting id_scale\")\n",
    "        \n",
    "        all_as_id = np.exp(self.id_err.score_samples(all_err.reshape(-1, 1)))\n",
    "        all_as_all = np.exp(self.all_err.score_samples(all_err.reshape(-1, 1)))\n",
    "        \n",
    "        self.id_scale = np.nan_to_num(np.nanmin(all_as_all / all_as_id), nan=1.0, posinf=1.0, neginf=1.0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        print(\"Calculating test_err\")\n",
    "        \n",
    "        test_err = self.gp_model.predict(X_test, return_std=True)[1] / self.err_scale\n",
    "        \n",
    "        print(\"Calculating test_as_id and test_as_all\")\n",
    "        \n",
    "        test_as_id = np.exp(self.id_err.score_samples(test_err.reshape(-1, 1)))\n",
    "        test_as_all = np.exp(self.all_err.score_samples(test_err.reshape(-1, 1)))\n",
    "        \n",
    "        return np.nan_to_num(test_as_id / np.maximum(test_as_id, test_as_all), nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "class AutoAssociativeMahalanobisIDWeightDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.rng = rng\n",
    "        \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        print(\"Fitting PCA\")\n",
    "        \n",
    "        bn = np.searchsorted(np.cumsum(\n",
    "            PCA(random_state=self.rng).fit(X_id).explained_variance_ratio_\n",
    "        ), 0.95)\n",
    "        \n",
    "        print(f\"Fitting AA with bn={bn}\")\n",
    "        \n",
    "        self.model = MLPRegressor(\n",
    "            hidden_layer_sizes=[X_id.shape[1]*2, bn, X_id.shape[1]*2],\n",
    "            activation=\"relu\", solver=\"adam\", random_state=self.rng,\n",
    "        ).fit(X_id, X_id)\n",
    "        \n",
    "        print(\"Fitting covariance\")\n",
    "        \n",
    "        self.cov = EmpiricalCovariance().fit((self.model.predict(X_id) - X_id))\n",
    "        \n",
    "        print(\"Fitting id_err\")\n",
    "        \n",
    "        id_err = self.cov.mahalanobis((self.model.predict(X_id_valid) - X_id_valid))\n",
    "        self.err_scale = np.mean(id_err)\n",
    "        id_err /= self.err_scale\n",
    "        self.id_err = KernelDensity(bandwidth=0.1, kernel=\"linear\").fit(id_err.reshape(-1, 1))\n",
    "        \n",
    "        print(\"Fitting all_err\")\n",
    "        \n",
    "        X_all = self.rng.normal(loc=0.0, scale=1.0, size=X_id_valid.shape)\n",
    "        all_err = self.cov.mahalanobis((self.model.predict(X_all) - X_all)) / self.err_scale\n",
    "        self.all_err = KernelDensity(bandwidth=0.1, kernel=\"linear\").fit(all_err.reshape(-1, 1))\n",
    "        \n",
    "        print(\"Fitting id_scale\")\n",
    "        \n",
    "        all_as_id = np.exp(self.id_err.score_samples(all_err.reshape(-1, 1)))\n",
    "        all_as_all = np.exp(self.all_err.score_samples(all_err.reshape(-1, 1)))\n",
    "        \n",
    "        self.id_scale = np.nan_to_num(np.nanmin(all_as_all / all_as_id), nan=1.0, posinf=1.0, neginf=1.0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        print(\"Calculating test_err\")\n",
    "        \n",
    "        test_err = self.cov.mahalanobis((self.model.predict(X_test) - X_test)) / self.err_scale\n",
    "        \n",
    "        print(\"Calculating test_as_id and test_as_all\")\n",
    "        \n",
    "        test_as_id = np.exp(self.id_err.score_samples(test_err.reshape(-1, 1)))\n",
    "        test_as_all = np.exp(self.all_err.score_samples(test_err.reshape(-1, 1)))\n",
    "        \n",
    "        return np.nan_to_num(test_as_id / np.maximum(test_as_id, test_as_all), nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90051e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "class GaussianProcessLogisticDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.gp = GaussianProcessRegressor(*args, kernel=(\n",
    "            RBF() + WhiteKernel()\n",
    "        ), random_state=rng, **kwargs)\n",
    "        self.rng = rng\n",
    "    \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        print(\"Fitting PCA\")\n",
    "        \n",
    "        bn = np.searchsorted(np.cumsum(\n",
    "            PCA(random_state=self.rng).fit(X_id).explained_variance_ratio_\n",
    "        ), 0.95)\n",
    "        \n",
    "        print(f\"Fitting AA with bn={bn}\")\n",
    "        \n",
    "        tf.random.set_seed(int.from_bytes(self.rng.bytes(4), \"little\"))\n",
    "        \n",
    "        self.aa_model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=X_id.shape[1:]),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-enc\"),\n",
    "                tf.keras.layers.Dense(bn, activation=\"relu\", name=\"aa-bn\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-dec\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1], name=\"aa-X\"),\n",
    "            ]\n",
    "        )\n",
    "        self.aa_model.compile(optimizer='adam', loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
    "        self.aa_model.fit(\n",
    "            X_id, X_id,\n",
    "            validation_data=(X_id_valid, X_id_valid),\n",
    "            batch_size=200, epochs=200, verbose=1, callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=10,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        print(\"Generate FGSM inputs\")\n",
    "        \n",
    "        x_id = tf.constant(X_id_valid, dtype=tf.float32)\n",
    "        x_id = tf.random.normal(mean=x_id, stddev=0.01, shape=x_id.shape)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_id)\n",
    "\n",
    "            x_pred_id = self.aa_model(x_id)\n",
    "            x_mse_id = (x_pred_id - x_id) ** 2\n",
    "\n",
    "        adv_grad = tape.gradient(x_mse_id, x_id)\n",
    "\n",
    "        X_ood = (x_id + tf.math.sign(adv_grad) * tf.math.abs(tf.random.normal(\n",
    "            mean=2.0, stddev=0.5, shape=[len(X_id_valid), 1],\n",
    "        ))).numpy()\n",
    "        \n",
    "        print(\"Fitting Gaussian Process\")\n",
    "        \n",
    "        num_X = len(X_id)\n",
    "        \n",
    "        # Safety hatch to limit computing time\n",
    "        if num_X >= 10_000:\n",
    "            I_train = self.rng.choice(len(X_id), size=num_X//6)\n",
    "            I_valid = np.ones(len(X_id))\n",
    "            I_valid[I_train] = 0\n",
    "            I_valid, = np.nonzero(I_valid)\n",
    "            \n",
    "            X_id_valid = np.concatenate([X_id_valid, X_id[I_valid]], axis=0)\n",
    "            Y_id_valid = np.concatenate([Y_id_valid, Y_id[I_valid]], axis=0)\n",
    "            \n",
    "            X_id = X_id[I_train]\n",
    "            Y_id = Y_id[I_train]\n",
    "        \n",
    "        self.gp.fit(X_id, Y_id)\n",
    "        \n",
    "        print(\"Train classifier\")\n",
    "        \n",
    "        U_id = self.gp.predict(X_id_valid, return_std=True)[1]\n",
    "        U_ood = self.gp.predict(X_ood, return_std=True)[1]\n",
    "        \n",
    "        self.scaler = StandardScaler().fit(U_id.reshape(-1, 1))\n",
    "        \n",
    "        self.model = LogisticRegression(\n",
    "            penalty='none', class_weight=\"balanced\", random_state=self.rng,\n",
    "        ).fit(\n",
    "            np.concatenate([\n",
    "                self.scaler.transform(U_id.reshape(-1, 1)),\n",
    "                self.scaler.transform(U_ood.reshape(-1, 1)),\n",
    "            ], axis=0).reshape(-1, 1),\n",
    "            np.concatenate([\n",
    "                np.ones(len(U_id)), np.zeros(len(U_ood)),\n",
    "            ], axis=0),\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        return self.model.predict_proba(self.scaler.transform(\n",
    "            self.gp.predict(X_test, return_std=True)[1].reshape(-1, 1)\n",
    "        ))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class AutoAssociativeMahalanobisLogisticDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.rng = rng\n",
    "        \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        print(\"Fitting PCA\")\n",
    "        \n",
    "        bn = np.searchsorted(np.cumsum(\n",
    "            PCA(random_state=self.rng).fit(X_id).explained_variance_ratio_\n",
    "        ), 0.95)\n",
    "        \n",
    "        print(f\"Fitting AA with bn={bn}\")\n",
    "        \n",
    "        tf.random.set_seed(int.from_bytes(self.rng.bytes(4), \"little\"))\n",
    "        \n",
    "        self.aa_model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=X_id.shape[1:]),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-enc\"),\n",
    "                tf.keras.layers.Dense(bn, activation=\"relu\", name=\"aa-bn\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-dec\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1], name=\"aa-X\"),\n",
    "            ]\n",
    "        )\n",
    "        self.aa_model.compile(optimizer='adam', loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
    "        self.aa_model.fit(\n",
    "            X_id, X_id,\n",
    "            validation_data=(X_id_valid, X_id_valid),\n",
    "            batch_size=200, epochs=200, verbose=1, callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=10,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        print(\"Fitting covariance\")\n",
    "        \n",
    "        self.cov = EmpiricalCovariance().fit((self.aa_model.predict(X_id) - X_id))\n",
    "        \n",
    "        print(\"Generate FGSM inputs\")\n",
    "        \n",
    "        x_id = tf.constant(X_id_valid, dtype=tf.float32)\n",
    "        x_id = tf.random.normal(mean=x_id, stddev=0.01, shape=x_id.shape)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_id)\n",
    "\n",
    "            x_pred_id = self.aa_model(x_id)\n",
    "            x_mse_id = (x_pred_id - x_id) ** 2\n",
    "\n",
    "        adv_grad = tape.gradient(x_mse_id, x_id)\n",
    "\n",
    "        X_ood = (x_id + tf.math.sign(adv_grad) * tf.math.abs(tf.random.normal(\n",
    "            mean=2.0, stddev=0.5, shape=[len(X_id_valid), 1],\n",
    "        ))).numpy()\n",
    "        \n",
    "        print(\"Train classifier\")\n",
    "        \n",
    "        M_id = self.cov.mahalanobis(self.aa_model.predict(X_id_valid) - X_id_valid)\n",
    "        M_ood = self.cov.mahalanobis(self.aa_model.predict(X_ood) - X_ood)\n",
    "        \n",
    "        self.scaler = StandardScaler().fit(M_id.reshape(-1, 1))\n",
    "        \n",
    "        self.model = LogisticRegression(\n",
    "            penalty='none', class_weight=\"balanced\", random_state=self.rng,\n",
    "        ).fit(\n",
    "            np.concatenate([\n",
    "                self.scaler.transform(M_id.reshape(-1, 1)),\n",
    "                self.scaler.transform(M_ood.reshape(-1, 1)),\n",
    "            ], axis=0).reshape(-1, 1),\n",
    "            np.concatenate([\n",
    "                np.ones(len(M_id)), np.zeros(len(M_ood)),\n",
    "            ], axis=0),\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        return self.model.predict_proba(self.scaler.transform(\n",
    "            self.cov.mahalanobis(self.aa_model.predict(X_test) - X_test).reshape(-1, 1)\n",
    "        ))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34333cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class AutoAssociativeErrorLogisticDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.rng = rng\n",
    "        \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        print(\"Fitting PCA\")\n",
    "        \n",
    "        bn = np.searchsorted(np.cumsum(\n",
    "            PCA(random_state=self.rng).fit(X_id).explained_variance_ratio_\n",
    "        ), 0.95)\n",
    "        \n",
    "        print(f\"Fitting AA with bn={bn}\")\n",
    "        \n",
    "        tf.random.set_seed(int.from_bytes(self.rng.bytes(4), \"little\"))\n",
    "        \n",
    "        self.aa_model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=X_id.shape[1:]),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-enc\"),\n",
    "                tf.keras.layers.Dense(bn, activation=\"relu\", name=\"aa-bn\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1]*2, activation=\"relu\", name=\"aa-dec\"),\n",
    "                tf.keras.layers.Dense(X_id.shape[1], name=\"aa-X\"),\n",
    "            ]\n",
    "        )\n",
    "        self.aa_model.compile(optimizer='adam', loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
    "        self.aa_model.fit(\n",
    "            X_id, X_id,\n",
    "            validation_data=(X_id_valid, X_id_valid),\n",
    "            batch_size=200, epochs=200, verbose=1, callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=10,\n",
    "                    restore_best_weights=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        print(\"Generate FGSM inputs\")\n",
    "        \n",
    "        x_id = tf.constant(X_id_valid, dtype=tf.float32)\n",
    "        x_id = tf.random.normal(mean=x_id, stddev=0.01, shape=x_id.shape)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_id)\n",
    "\n",
    "            x_pred_id = self.aa_model(x_id)\n",
    "            x_mse_id = (x_pred_id - x_id) ** 2\n",
    "\n",
    "        adv_grad = tape.gradient(x_mse_id, x_id)\n",
    "\n",
    "        X_ood = (x_id + tf.math.sign(adv_grad) * tf.math.abs(tf.random.normal(\n",
    "            mean=2.0, stddev=0.5, shape=[len(X_id_valid), 1],\n",
    "        ))).numpy()\n",
    "        \n",
    "        print(\"Train classifier\")\n",
    "        \n",
    "        self.model = LogisticRegression(\n",
    "            penalty='none', class_weight=\"balanced\", random_state=self.rng,\n",
    "        ).fit(\n",
    "            np.concatenate([\n",
    "                np.abs(self.aa_model.predict(X_id_valid) - X_id_valid),\n",
    "                np.abs(self.aa_model.predict(X_ood) - X_ood),\n",
    "            ], axis=0),\n",
    "            np.concatenate([\n",
    "                np.ones(len(X_id_valid)), np.zeros(len(X_ood)),\n",
    "            ], axis=0),\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        return self.model.predict_proba(np.abs(self.aa_model.predict(X_test) - X_test))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TruncatedPCAMahalanobisLogisticDetector(OutOfDistributionDetector):\n",
    "    def __init__(self, *args, rng=None, **kwargs):\n",
    "        self.rng = rng\n",
    "        \n",
    "    def _predict_truncated_pca(self, X):\n",
    "        if self.pca.mean_ is not None:\n",
    "            X = X - self.pca.mean_\n",
    "        \n",
    "        X_trans = np.dot(X, self.pca.components_[:self.bn].T)\n",
    "        X = np.dot(X_trans, self.pca.components_[:self.bn])\n",
    "        \n",
    "        if self.pca.mean_ is not None:\n",
    "            X = X + self.pca.mean_\n",
    "        \n",
    "        return X\n",
    "        \n",
    "    def fit(self, X_id, Y_id, X_id_valid, Y_id_valid):\n",
    "        print(\"Fitting PCA\")\n",
    "        \n",
    "        self.pca = PCA(random_state=self.rng).fit(X_id)\n",
    "        self.bn = np.searchsorted(np.cumsum(self.pca.explained_variance_ratio_), 0.95)\n",
    "        \n",
    "        print(\"Fitting covariance\")\n",
    "        \n",
    "        self.cov = EmpiricalCovariance().fit((self._predict_truncated_pca(X_id) - X_id))\n",
    "        \n",
    "        print(\"Generate FGSM inputs\")\n",
    "        \n",
    "        adv_grad = self.pca.components_[self.bn]\n",
    "        \n",
    "        X_ood = self.rng.normal(loc=X_id_valid, scale=0.01) + np.sign(adv_grad) * np.abs(\n",
    "            self.rng.normal(loc=2.0, scale=0.5, size=(len(X_id_valid), 1))\n",
    "        ) * self.rng.choice([-1, 1])\n",
    "        \n",
    "        print(\"Train classifier\")\n",
    "        \n",
    "        M_id = self.cov.mahalanobis(self._predict_truncated_pca(X_id_valid) - X_id_valid)\n",
    "        M_ood = self.cov.mahalanobis(self._predict_truncated_pca(X_ood) - X_ood)\n",
    "        \n",
    "        self.scaler = StandardScaler().fit(M_id.reshape(-1, 1))\n",
    "        \n",
    "        self.model = LogisticRegression(\n",
    "            penalty='none', class_weight=\"balanced\", random_state=self.rng,\n",
    "        ).fit(\n",
    "            np.concatenate([\n",
    "                self.scaler.transform(M_id.reshape(-1, 1)),\n",
    "                self.scaler.transform(M_ood.reshape(-1, 1)),\n",
    "            ], axis=0).reshape(-1, 1),\n",
    "            np.concatenate([\n",
    "                np.ones(len(M_id)), np.zeros(len(M_ood)),\n",
    "            ], axis=0),\n",
    "        )\n",
    "        \n",
    "        print(\"Finished training the OOD scorer\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score(self, X_test):\n",
    "        print(\"Generating confidence scores ...\")\n",
    "        \n",
    "        return self.model.predict_proba(self.scaler.transform(\n",
    "            self.cov.mahalanobis(self._predict_truncated_pca(X_test) - X_test).reshape(-1, 1)\n",
    "        ))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_cache_detector(dt: datetime.datetime, clump: float, datasets: dict, models: dict, cls, *args, **kwargs):\n",
    "    if isinstance(dt, tuple) or isinstance(dt, list):\n",
    "        dt = tuple(sorted(dt))\n",
    "    \n",
    "    model_key = (cls.__name__, dt, clump)\n",
    "    \n",
    "    cached = models.get(model_key)\n",
    "    \n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    \n",
    "    model_path = f\"{cls.__name__.lower()}.score.{hash_for_dt(dt).hexdigest(8)}.{clump}.jl\"\n",
    "    \n",
    "    if Path(model_path).exists():    \n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        models[model_key] = model\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    dataset = load_and_cache_dataset(dt, clump, datasets)\n",
    "    \n",
    "    rng = np.random.RandomState(seed=int.from_bytes(hash_for_dt(dt).digest(4), 'little'))\n",
    "    \n",
    "    model = cls(*args, rng=rng, **kwargs).fit(\n",
    "        X_id=dataset.X_train, Y_id=dataset.Y_train,\n",
    "        X_id_valid=dataset.X_valid, Y_id_valid=dataset.Y_valid,\n",
    "    )\n",
    "    \n",
    "    joblib.dump(model, model_path)\n",
    "    \n",
    "    models[model_key] = model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train = datetime.datetime(year=2018, month=5, day=15, hour=19)\n",
    "dt_test = [\n",
    "    datetime.datetime(year=2018, month=5, day=14, hour=10),\n",
    "    datetime.datetime(year=2018, month=5, day=17, hour=0),\n",
    "    datetime.datetime(year=2018, month=5, day=19, hour=4),\n",
    "    datetime.datetime(year=2018, month=5, day=21, hour=15),\n",
    "    datetime.datetime(year=2018, month=5, day=23, hour=13),\n",
    "]\n",
    "\n",
    "ds_train = load_and_cache_dataset(dt_train, 0.75, DATASETS)\n",
    "ds_test = load_and_cache_dataset(dt_test, 0.75, DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a141e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_temp = [\n",
    "    datetime.datetime(year=2018, month=5, day=15, hour=15),\n",
    "    datetime.datetime(year=2018, month=5, day=15, hour=17),\n",
    "    datetime.datetime(year=2018, month=5, day=15, hour=18),\n",
    "    datetime.datetime(year=2018, month=5, day=15, hour=20),\n",
    "    datetime.datetime(year=2018, month=5, day=15, hour=21),\n",
    "    datetime.datetime(year=2018, month=5, day=15, hour=23),\n",
    "]\n",
    "\n",
    "ds_temp = load_and_cache_dataset(dt_temp, 0.75, DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "myrng = np.random.RandomState(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_id = EmpiricalCovariance().fit(ds_train.X_train)\n",
    "\n",
    "X_id_cov = myrng.multivariate_normal(\n",
    "    mean=cov_id.location_, cov=cov_id.covariance_, size=len(ds_train.X_train),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ff3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ood_cov = []\n",
    "\n",
    "for f in np.linspace(0.05, 0.95, 10):\n",
    "    X_ood_cov.append(\n",
    "        myrng.multivariate_normal(\n",
    "            mean=cov_id.location_ * f,\n",
    "            cov=cov_id.covariance_ * f + np.identity(len(cov_id.location_)) * (1-f),\n",
    "            size=len(ds_train.X_test) // 20,\n",
    "        )\n",
    "    )\n",
    "\n",
    "X_ood_cov = np.concatenate(X_ood_cov, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2eb8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ood_n01 = myrng.normal(loc=0.0, scale=1.0, size=(len(ds_train.X_test) // 2, ds_train.X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307620a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODELS = dict()\n",
    "\n",
    "for cls, args, kwargs in [\n",
    "    (GaussianProcessPercentileDetector, [], dict()),\n",
    "    (AutoAssociativeMahalanobisPercentileDetector, [], dict()),\n",
    "    (LogisticClassifierDetector, [], dict()),\n",
    "    (LogisticTPokeClassifierDetector, [], dict()),\n",
    "    (GaussianProcessIDWeightDetector, [], dict()),\n",
    "    (AutoAssociativeMahalanobisIDWeightDetector, [], dict()),\n",
    "    (GaussianProcessLogisticDetector, [], dict()),\n",
    "    (AutoAssociativeMahalanobisLogisticDetector, [], dict()),\n",
    "    (AutoAssociativeErrorLogisticDetector, [], dict()),\n",
    "    (TruncatedPCAMahalanobisLogisticDetector, [], dict()),\n",
    "]:      \n",
    "    detector = train_and_cache_detector(dt_train, 0.75, DATASETS, MODELS, cls, *args, **kwargs)\n",
    "    \n",
    "    C_id_train = detector.score(ds_train.X_train)\n",
    "    C_id_valid = detector.score(ds_train.X_valid)\n",
    "    C_id_test = detector.score(ds_train.X_test)\n",
    "    C_id = C_id_test\n",
    "\n",
    "    C_id_cov = detector.score(X_id_cov)\n",
    "    C_temp = detector.score(ds_train.X_scaler.transform(\n",
    "        ds_temp.X_scaler.inverse_transform(np.concatenate([\n",
    "            ds_temp.X_train, ds_temp.X_valid, ds_temp.X_test,\n",
    "        ], axis=0))\n",
    "    ))\n",
    "    C_trajs = detector.score(ds_train.X_scaler.transform(\n",
    "        ds_test.X_scaler.inverse_transform(np.concatenate([\n",
    "            ds_test.X_train, ds_test.X_valid, ds_test.X_test,\n",
    "        ], axis=0))\n",
    "    ))\n",
    "    \n",
    "    C_ood_cov = detector.score(X_ood_cov)\n",
    "    C_ood_n01 = detector.score(X_ood_n01)\n",
    "    C_ood = np.concatenate([C_ood_cov, C_ood_n01], axis=0)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "    ax.set_title(f\"{cls.name()}\")\n",
    "    ax.set_xlabel(\"confidence score $c$\")\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel(\"relative frequency\")\n",
    "\n",
    "    ax.set_xlim((-0.05, 1.05))\n",
    "\n",
    "    # Add some minor random noise to the confidence scores to avoid\n",
    "    #  singular matrices without affecting the visuals\n",
    "    v1 = ax.violinplot((\n",
    "        np.concatenate([\n",
    "            C_id_test, C_id_valid,\n",
    "        ], axis=0) + np.random.normal(0.0, 1e-9, size=np.concatenate([\n",
    "            C_id_test, C_id_valid,\n",
    "        ], axis=0).shape)\n",
    "    ).reshape(-1, 1), showextrema=True, vert=False, positions=[7])\n",
    "    v2 = ax.violinplot((\n",
    "        C_id_test + np.random.normal(0.0, 1e-9, size=C_id_test.shape)\n",
    "    ).reshape(-1, 1), showextrema=True, vert=False, positions=[6])\n",
    "    \n",
    "    v3 = ax.violinplot((\n",
    "        C_id_cov + np.random.normal(0.0, 1e-9, size=C_id_cov.shape)\n",
    "    ).reshape(-1, 1), showextrema=True, vert=False, positions=[5])\n",
    "    v4 = ax.violinplot((\n",
    "        C_temp + np.random.normal(0.0, 1e-9, size=C_temp.shape)\n",
    "    ).reshape(-1, 1), showextrema=True, vert=False, positions=[4])\n",
    "    v5 = ax.violinplot((\n",
    "        C_trajs + np.random.normal(0.0, 1e-9, size=C_trajs.shape)\n",
    "    ).reshape(-1, 1), showextrema=True, vert=False, positions=[3])\n",
    "    \n",
    "    v6 = ax.violinplot((\n",
    "        C_ood_cov + np.random.normal(0.0, 1e-9, size=C_ood_cov.shape)\n",
    "    ).reshape(-1, 1), showextrema=True, vert=False, positions=[2])\n",
    "    v7 = ax.violinplot((\n",
    "        C_ood_n01 + np.random.normal(0.0, 1e-9, size=C_ood_n01.shape)\n",
    "    ).reshape(-1, 1), showextrema=True, vert=False, positions=[1])\n",
    "\n",
    "    for v, c in zip([v1, v2, v3, v4, v5, v6, v7], [\n",
    "        plt.cm.tab20b(1), plt.cm.tab20c(0), \n",
    "        plt.cm.tab20c(12), plt.cm.tab20c(8), plt.cm.tab20b(9),\n",
    "        plt.cm.tab20c(5), plt.cm.tab20c(4),\n",
    "    ]):\n",
    "        v[\"cbars\"].set_visible(False)\n",
    "        v[\"cbars\"].set_color(c)\n",
    "\n",
    "        s = v[\"cmins\"].get_segments()\n",
    "        s[0][0,1] -= 0.2\n",
    "        s[0][1,1] += 0.2\n",
    "        v[\"cmins\"].set_segments(s)\n",
    "        v[\"cmins\"].set_linestyle(\":\")\n",
    "        v[\"cmins\"].set_color(c)\n",
    "\n",
    "        s = v[\"cmaxes\"].get_segments()\n",
    "        s[0][0,1] -= 0.2\n",
    "        s[0][1,1] += 0.2\n",
    "        v[\"cmaxes\"].set_segments(s)\n",
    "        v[\"cmaxes\"].set_linestyle(\":\")\n",
    "        v[\"cmaxes\"].set_color(c)\n",
    "\n",
    "        for b in v[\"bodies\"]:\n",
    "            b.set_color(c)\n",
    "\n",
    "    ax.text(\n",
    "        0.5, 7.0, \"ID trajectory training+validation data\", c=v1[\"cbars\"].get_color()[0],\n",
    "        va=\"center\", ha=\"center\", path_effects=outlined,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5, 6.0, \"ID trajectory test data\", c=v2[\"cbars\"].get_color()[0],\n",
    "        va=\"center\", ha=\"center\", path_effects=outlined,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5, 5.0, r\"N$(\\mu($ID$), \\Sigma($ID$))$\", c=v3[\"cbars\"].get_color()[0],\n",
    "        va=\"center\", ha=\"center\", path_effects=outlined,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5, 4.0, r\"$\\pm 1$h, $\\pm 2$h, and $\\pm 4$h trajectories\",\n",
    "        c=v4[\"cbars\"].get_color()[0], va=\"center\", ha=\"center\", path_effects=outlined,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5, 3.0, \"other five trajectories\", c=v5[\"cbars\"].get_color()[0],\n",
    "        va=\"center\", ha=\"center\", path_effects=outlined,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5, 2.0, r\"OOD N$(\\mu($ID$), \\Sigma($ID$))$ $\\rightarrow$ N$(0, 1)$\",\n",
    "        c=v6[\"cbars\"].get_color()[0], va=\"center\", ha=\"center\", path_effects=outlined,\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5, 1.0, \"OOD N$(0, 1)$\", c=v7[\"cbars\"].get_color()[0],\n",
    "        va=\"center\", ha=\"center\", path_effects=outlined,\n",
    "    )\n",
    "    \n",
    "    plt.savefig(f\"ood.{cls.__name__.lower()}-distribution.pdf\", dpi=100, transparent=True, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "    fpr, tpr, roc_thresholds = metrics.roc_curve(np.concatenate([\n",
    "        np.ones(shape=len(C_id)), np.zeros(shape=len(C_ood)),\n",
    "    ]), np.concatenate([\n",
    "        C_id, C_ood,\n",
    "    ]))\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    pr0, rc0, pr_thresholds0 = metrics.precision_recall_curve(np.concatenate([\n",
    "        np.ones(shape=len(C_id)), np.zeros(shape=len(C_ood)),\n",
    "    ]), 1-np.concatenate([\n",
    "        C_id, C_ood,\n",
    "    ]), pos_label=0)\n",
    "    pr_auc0 = metrics.auc(rc0, pr0)\n",
    "    ap0 = metrics.average_precision_score(np.concatenate([\n",
    "        np.ones(shape=len(C_id)), np.zeros(shape=len(C_ood)),\n",
    "    ]), 1-np.concatenate([\n",
    "        C_id, C_ood,\n",
    "    ]), pos_label=0)\n",
    "\n",
    "    pr1, rc1, pr_thresholds1 = metrics.precision_recall_curve(np.concatenate([\n",
    "        np.ones(shape=len(C_id)), np.zeros(shape=len(C_ood)),\n",
    "    ]), np.concatenate([\n",
    "        C_id, C_ood,\n",
    "    ]), pos_label=1)\n",
    "    pr_auc1 = metrics.auc(rc1, pr1)\n",
    "    ap1 = metrics.average_precision_score(np.concatenate([\n",
    "        np.ones(shape=len(C_id)), np.zeros(shape=len(C_ood)),\n",
    "    ]), np.concatenate([\n",
    "        C_id, C_ood,\n",
    "    ]), pos_label=1)\n",
    "\n",
    "    pid, pc = calibration_curve(\n",
    "        np.concatenate([\n",
    "            np.ones(shape=len(C_id)), np.zeros(shape=len(C_ood)),\n",
    "        ], axis=0),\n",
    "        np.concatenate([C_id, C_ood], axis=0),\n",
    "        n_bins=10, strategy=\"uniform\"\n",
    "    )\n",
    "    cal_err = np.sqrt(np.mean((pid-pc)**2))\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(4, 4))\n",
    "\n",
    "    fig.suptitle(\"Confidence Score Calibration\", y=0.935)\n",
    "\n",
    "    for ax in axs.flatten():\n",
    "        ax.set_xlim((-0.05, 1.05))\n",
    "        ax.set_ylim((-0.05, 1.05))\n",
    "\n",
    "        ax.set_xticks([0.0, 1.0])\n",
    "        ax.set_yticks([0.0, 1.0])\n",
    "\n",
    "    axs[0,0].set_xlabel(\"mean $c$\", labelpad=-10.0)\n",
    "    axs[0,0].set_ylabel(\"ID rate\", labelpad=-10.0)\n",
    "\n",
    "    axs[0,1].set_xlabel(\"fp rate\", labelpad=-10.0)\n",
    "    axs[0,1].set_ylabel(\"tp rate\", labelpad=-10.0)\n",
    "\n",
    "    axs[1,0].set_xlabel(\"recall\", labelpad=-10.0)\n",
    "    axs[1,0].set_ylabel(\"precision\", labelpad=-10.0)\n",
    "\n",
    "    axs[1,1].set_xlabel(\"recall\", labelpad=-10.0)\n",
    "    axs[1,1].set_ylabel(\"precision\", labelpad=-10.0)\n",
    "\n",
    "    axs[0,0].plot([0, 1], [0, 1], \"k:\")\n",
    "    axs[0,0].plot(pc, pid, \"s-\", c=\"black\")\n",
    "    axs[0,0].legend(handles=[\n",
    "        mpl.patches.Patch(label=f\"RMSCE = {cal_err:.2}\")\n",
    "    ], loc=\"lower right\", handlelength=0, handletextpad=0)\n",
    "\n",
    "    axs[0,1].plot(fpr, tpr, c=plt.cm.tab20(4))\n",
    "    axs[0,1].legend(handles=[\n",
    "        mpl.patches.Patch(label=f\"ROC-AUC = {roc_auc:.2}\")\n",
    "    ], loc=\"lower right\", handlelength=0, handletextpad=0)\n",
    "\n",
    "    axs[1,0].plot(rc1, pr1, c=plt.cm.tab20(0))\n",
    "    axs[1,0].legend(handles=[\n",
    "        mpl.patches.Patch(label=f\"ID-Detection:\\nPR-AUC = {pr_auc1:.2}\\nAP = {ap1:.2}\")\n",
    "    ], loc=\"lower left\", handlelength=0, handletextpad=0)\n",
    "\n",
    "    axs[1,1].plot(rc0, pr0, c=plt.cm.tab20(6))\n",
    "    axs[1,1].legend(handles=[\n",
    "        mpl.patches.Patch(label=f\"OOD-Detection:\\nPR-AUC = {pr_auc0:.2}\\nAP = {ap0:.2}\")\n",
    "    ], loc=\"lower left\", handlelength=0, handletextpad=0)\n",
    "\n",
    "    plt.savefig(f\"ood.{cls.__name__.lower()}-calibration.pdf\", dpi=100, transparent=True, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b6620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12737.072597,
   "end_time": "2023-01-25T13:33:11.655593",
   "environment_variables": {},
   "exception": null,
   "input_path": "generalisation.ipynb",
   "output_path": "generalisation.ipynb",
   "parameters": {},
   "start_time": "2023-01-25T10:00:54.582996",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "019031af7d8c48e69022c1cb57b2f9d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "19a7f13716ca4daf81a3c2f5568df16c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dfab174198f44685bc8e11b9912259a2",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f8baa2bab0574d989b93569a13c54266",
       "tabbable": null,
       "tooltip": null,
       "value": 100
      }
     },
     "1b37107810414e828546b148557c66a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2651cb61da234172af0b46e69103ba21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28f81896ed5b4f7ba6346541750a27e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e031ff83c380437c9c22bdd9b62903de",
       "placeholder": "​",
       "style": "IPY_MODEL_3a1ff08eb6694ad484a822b55b79690a",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "2bfbccfa44e34a0e9b839940f1f840f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d2e7e129f644397849f21e014563416": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2f1bf43a34ef41babf9771e9dc9a0fdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "31201e786cb0442f9d5d99eec85afe99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f2d8a2f02c5c435f8632d4f03f35748d",
       "placeholder": "​",
       "style": "IPY_MODEL_9e75d52a8e2a44eb82127a4d8a40cb60",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "3a1ff08eb6694ad484a822b55b79690a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "40c7c519d7f74843abbbeaf9c7355f12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7514425f5ee848c79abb969f383b4e46",
       "placeholder": "​",
       "style": "IPY_MODEL_8b3b4f97b0a84a2fb31812dd44317792",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "414f8ca5df0e43df8f4349e41101af28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "41da05f10df242cc93d09f8417ff55b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "425d25c6f3cf446a8fb287f6f844525f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f69effc1db1a45b89beb8ec695ad323d",
       "placeholder": "​",
       "style": "IPY_MODEL_f08a94be5e7d40adaf14baa99f5d7039",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "466dea72255c4c269f0329f626c8c149": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_41da05f10df242cc93d09f8417ff55b1",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2d2e7e129f644397849f21e014563416",
       "tabbable": null,
       "tooltip": null,
       "value": 100
      }
     },
     "48d1fdfb9fde452288455ecbd3ef8c80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_31201e786cb0442f9d5d99eec85afe99",
        "IPY_MODEL_74e87566b9e543e9be174b313392bed6",
        "IPY_MODEL_e8170b5e76ca43079848afc04fb6dfc7"
       ],
       "layout": "IPY_MODEL_2651cb61da234172af0b46e69103ba21",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5e20360724014f9b8413f64d373e98c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5e98b9da1dd643f491fd33a46a0e43ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "63d75d770b674ccaa2049a10e3c45b0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "678c72d46baa47439867d3fec767c5cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "74e302337398458e97602a413293b495": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_40c7c519d7f74843abbbeaf9c7355f12",
        "IPY_MODEL_466dea72255c4c269f0329f626c8c149",
        "IPY_MODEL_c5a1cd58bace49c68461416790dda952"
       ],
       "layout": "IPY_MODEL_414f8ca5df0e43df8f4349e41101af28",
       "tabbable": null,
       "tooltip": null
      }
     },
     "74e87566b9e543e9be174b313392bed6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2bfbccfa44e34a0e9b839940f1f840f2",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5e98b9da1dd643f491fd33a46a0e43ec",
       "tabbable": null,
       "tooltip": null,
       "value": 100
      }
     },
     "7514425f5ee848c79abb969f383b4e46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7bbbedf3e34d4aec98e0e046c109d508": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_425d25c6f3cf446a8fb287f6f844525f",
        "IPY_MODEL_19a7f13716ca4daf81a3c2f5568df16c",
        "IPY_MODEL_f4a034837d1847cf96d61a320106b3cc"
       ],
       "layout": "IPY_MODEL_dbbb78bea9f340f497eafc29b1fb1f0f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8723cebae357498ea0bbd3cb7c8dd979": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fea27a5789d24ada95ccfd4cdb11147a",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2f1bf43a34ef41babf9771e9dc9a0fdf",
       "tabbable": null,
       "tooltip": null,
       "value": 100
      }
     },
     "8b3b4f97b0a84a2fb31812dd44317792": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9e75d52a8e2a44eb82127a4d8a40cb60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9ed0ca273f14498d855bb1641804db13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab9bcf89cf9b4b789f5f1878c1ea6465": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eee31f717c544b10b30da7ed0736803a",
       "placeholder": "​",
       "style": "IPY_MODEL_678c72d46baa47439867d3fec767c5cc",
       "tabbable": null,
       "tooltip": null,
       "value": " 100/100 [00:35&lt;00:00,  2.86it/s]"
      }
     },
     "b9365f68125c4b59ab67132e84334dda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c557ac89352a420085c5816bb05d3f77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_28f81896ed5b4f7ba6346541750a27e8",
        "IPY_MODEL_8723cebae357498ea0bbd3cb7c8dd979",
        "IPY_MODEL_ab9bcf89cf9b4b789f5f1878c1ea6465"
       ],
       "layout": "IPY_MODEL_63d75d770b674ccaa2049a10e3c45b0f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c5a1cd58bace49c68461416790dda952": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b37107810414e828546b148557c66a8",
       "placeholder": "​",
       "style": "IPY_MODEL_df7b83b2aba54c0ebcdb31f4583b02bb",
       "tabbable": null,
       "tooltip": null,
       "value": " 100/100 [00:34&lt;00:00,  2.86it/s]"
      }
     },
     "dbbb78bea9f340f497eafc29b1fb1f0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df7b83b2aba54c0ebcdb31f4583b02bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dfab174198f44685bc8e11b9912259a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e031ff83c380437c9c22bdd9b62903de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8170b5e76ca43079848afc04fb6dfc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9ed0ca273f14498d855bb1641804db13",
       "placeholder": "​",
       "style": "IPY_MODEL_5e20360724014f9b8413f64d373e98c4",
       "tabbable": null,
       "tooltip": null,
       "value": " 100/100 [00:34&lt;00:00,  2.87it/s]"
      }
     },
     "eee31f717c544b10b30da7ed0736803a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f08a94be5e7d40adaf14baa99f5d7039": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f2d8a2f02c5c435f8632d4f03f35748d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4a034837d1847cf96d61a320106b3cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b9365f68125c4b59ab67132e84334dda",
       "placeholder": "​",
       "style": "IPY_MODEL_019031af7d8c48e69022c1cb57b2f9d3",
       "tabbable": null,
       "tooltip": null,
       "value": " 100/100 [00:34&lt;00:00,  2.92it/s]"
      }
     },
     "f69effc1db1a45b89beb8ec695ad323d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f8baa2bab0574d989b93569a13c54266": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fea27a5789d24ada95ccfd4cdb11147a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
