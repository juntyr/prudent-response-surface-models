{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126d792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:00:58.907938Z",
     "iopub.status.busy": "2023-01-25T10:00:58.907453Z",
     "iopub.status.idle": "2023-01-25T10:00:58.913633Z",
     "shell.execute_reply": "2023-01-25T10:00:58.913203Z"
    },
    "papermill": {
     "duration": 0.023998,
     "end_time": "2023-01-25T10:00:58.914605",
     "exception": false,
     "start_time": "2023-01-25T10:00:58.890607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import datetime\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import copy\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafab0a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:00:58.945978Z",
     "iopub.status.busy": "2023-01-25T10:00:58.945394Z",
     "iopub.status.idle": "2023-01-25T10:01:10.516003Z",
     "shell.execute_reply": "2023-01-25T10:01:10.515382Z"
    },
    "papermill": {
     "duration": 11.586791,
     "end_time": "2023-01-25T10:01:10.517703",
     "exception": false,
     "start_time": "2023-01-25T10:00:58.930912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import joblib\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "import sklearn\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8e1a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:10.589026Z",
     "iopub.status.busy": "2023-01-25T10:01:10.588881Z",
     "iopub.status.idle": "2023-01-25T10:01:10.592072Z",
     "shell.execute_reply": "2023-01-25T10:01:10.591651Z"
    },
    "papermill": {
     "duration": 0.017674,
     "end_time": "2023-01-25T10:01:10.592952",
     "exception": false,
     "start_time": "2023-01-25T10:01:10.575278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrajectoryPaths = namedtuple(\"TrajectoryPaths\", [\"date\", \"out\", \"aer\", \"ant\", \"bio\", \"met\"])\n",
    "TrajectoryDatasets = namedtuple(\"TrajectoryDatasets\", [\"date\", \"out\", \"aer\", \"ant\", \"bio\", \"met\"])\n",
    "MLDataset = namedtuple(\"MLDataset\", [\"date\", \"paths\", \"X_raw\", \"Y_raw\", \"X_train\", \"X_valid\", \"X_test\", \"Y_train\", \"Y_valid\", \"Y_test\", \"X_scaler\", \"Y_scaler\"])\n",
    "PerturbedDataset = namedtuple(\"PerturbedDataset\", [\"date\", \"perturbation\", \"paths\", \"X\", \"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b689da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_for_perturbation(dt: datetime.datetime, perturbation: Path) -> TrajectoryPaths:\n",
    "    base = Path.cwd().parent / \"trajectories\"\n",
    "    \n",
    "    out_path = base / \"outputs\" / perturbation / dt.strftime('%Y%m%d_T%H') / \"output.nc\"\n",
    "    aer_path = (\n",
    "        base / \"inputs\" / perturbation / \"HYDE_BASE_Y2018\" /\n",
    "        f\"OUTPUT_bwd_{dt.strftime('%Y%m%d')}\" /\n",
    "        \"EMISSIONS_0422\" /\n",
    "        f\"{dt.strftime('%Y%m%d')}_7daybwd_Hyde_traj_AER_{24-dt.hour:02}_L3.nc\"\n",
    "    )\n",
    "    ant_path = (\n",
    "        base / \"inputs\" / perturbation / \"HYDE_BASE_Y2018\" /\n",
    "        f\"OUTPUT_bwd_{dt.strftime('%Y%m%d')}\" /\n",
    "        \"EMISSIONS_0422\" /\n",
    "        f\"{dt.strftime('%Y%m%d')}_7daybwd_Hyde_traj_ANT_{24-dt.hour:02}_L3.nc\"\n",
    "    )\n",
    "    bio_path = (\n",
    "        base / \"inputs\" / perturbation / \"HYDE_BASE_Y2018\" /\n",
    "        f\"OUTPUT_bwd_{dt.strftime('%Y%m%d')}\" /\n",
    "        \"EMISSIONS_0422\" /\n",
    "        f\"{dt.strftime('%Y%m%d')}_7daybwd_Hyde_traj_BIO_{24-dt.hour:02}_L3.nc\"\n",
    "    )\n",
    "    met_path = (\n",
    "        base / \"inputs\" / perturbation / \"HYDE_BASE_Y2018\" /\n",
    "        f\"OUTPUT_bwd_{dt.strftime('%Y%m%d')}\" /\n",
    "        \"METEO\" /\n",
    "        f\"METEO_{dt.strftime('%Y%m%d')}_R{24-dt.hour:02}.nc\"\n",
    "    )\n",
    "    \n",
    "    if (\n",
    "        (not out_path.exists()) or (not aer_path.exists()) or\n",
    "        (not ant_path.exists()) or (not bio_path.exists()) or\n",
    "        (not met_path.exists())\n",
    "    ):\n",
    "        raise Exception(out_path, aer_path, ant_path, bio_path, met_path)\n",
    "    \n",
    "    return TrajectoryPaths(\n",
    "        date=dt, out=out_path, aer=aer_path, ant=ant_path, bio=bio_path, met=met_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed374e16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.531705Z",
     "iopub.status.busy": "2023-01-25T10:01:13.531475Z",
     "iopub.status.idle": "2023-01-25T10:01:13.535055Z",
     "shell.execute_reply": "2023-01-25T10:01:13.534641Z"
    },
    "papermill": {
     "duration": 0.018564,
     "end_time": "2023-01-25T10:01:13.535960",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.517396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_trajectory_dataset(paths: TrajectoryPaths) -> TrajectoryDatasets:\n",
    "    outds = Dataset(paths.out, \"r\", format=\"NETCDF4\")\n",
    "    aerds = Dataset(paths.aer, \"r\", format=\"NETCDF4\")\n",
    "    antds = Dataset(paths.ant, \"r\", format=\"NETCDF4\")\n",
    "    biods = Dataset(paths.bio, \"r\", format=\"NETCDF4\")\n",
    "    metds = Dataset(paths.met, \"r\", format=\"NETCDF4\")\n",
    "    \n",
    "    return TrajectoryDatasets(\n",
    "        date=paths.date, out=outds, aer=aerds, ant=antds, bio=biods, met=metds,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44943f5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.564286Z",
     "iopub.status.busy": "2023-01-25T10:01:13.564144Z",
     "iopub.status.idle": "2023-01-25T10:01:13.567282Z",
     "shell.execute_reply": "2023-01-25T10:01:13.566824Z"
    },
    "papermill": {
     "duration": 0.017935,
     "end_time": "2023-01-25T10:01:13.568160",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.550225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_proj = ccrs.PlateCarree()\n",
    "projection = ccrs.LambertConformal(\n",
    "    central_latitude=50, central_longitude=20, standard_parallels=(25, 25)\n",
    ")\n",
    "extent = [-60, 60, 40, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e210a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.597497Z",
     "iopub.status.busy": "2023-01-25T10:01:13.597275Z",
     "iopub.status.idle": "2023-01-25T10:01:13.600827Z",
     "shell.execute_reply": "2023-01-25T10:01:13.600422Z"
    },
    "papermill": {
     "duration": 0.018385,
     "end_time": "2023-01-25T10:01:13.601713",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.583328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ccn_concentration(ds: TrajectoryDatasets):\n",
    "    ccn_bin_indices, = np.nonzero(ds.out[\"dp_dry_fs\"][:].data > 80e-9)\n",
    "    ccn_concentration = np.sum(ds.out[\"nconc_par\"][:].data[:,ccn_bin_indices,:], axis=1)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"ccn\": ccn_concentration.flatten(),\n",
    "    }).set_index([\"time\", \"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5ab46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.663400Z",
     "iopub.status.busy": "2023-01-25T10:01:13.663182Z",
     "iopub.status.idle": "2023-01-25T10:01:13.673472Z",
     "shell.execute_reply": "2023-01-25T10:01:13.673062Z"
    },
    "papermill": {
     "duration": 0.025231,
     "end_time": "2023-01-25T10:01:13.674358",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.649127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_output_time(ds: TrajectoryDatasets):\n",
    "    fdom = datetime.datetime.strptime(\n",
    "        ds.out[\"time\"].__dict__[\"first_day_of_month\"], \"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "    dt = (ds.date - fdom).total_seconds()\n",
    "    \n",
    "    out_t = ds.out[\"time\"][:].data\n",
    "    \n",
    "    return out_t - dt\n",
    "\n",
    "def interpolate_meteorology_values(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    met_t = ds.met[\"time\"][:].data\n",
    "    met_h = ds.met[\"lev\"][:].data\n",
    "    \n",
    "    met_t_h = ds.met[key][:]\n",
    "    \n",
    "    met_t_h_int = sp.interpolate.interp2d(\n",
    "        x=met_h, y=met_t, z=met_t_h, kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    return met_t_h_int(x=out_h, y=out_t)\n",
    "\n",
    "def interpolate_meteorology_time_values(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    met_t = ds.met[\"time\"][:].data\n",
    "    \n",
    "    met_t_v = ds.met[key][:]\n",
    "    \n",
    "    met_t_int = sp.interpolate.interp1d(\n",
    "        x=met_t, y=met_t_v, kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    return np.repeat(\n",
    "        met_t_int(x=out_t).reshape(-1, 1),\n",
    "        out_h.shape[0], axis=1,\n",
    "    )\n",
    "\n",
    "def interpolate_biogenic_emissions(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    # depth of each box layer, assuming level heights are midpoints and end points are clamped\n",
    "    out_d = (np.array(list(out_h[1:])+[out_h[-1]]) - np.array([out_h[0]]+list(out_h[:-1]))) / 2.0\n",
    "    \n",
    "    bio_t = ds.bio[\"time\"][:].data\n",
    "    \n",
    "    # Biogenic emissions are limited to boxes at <= 10m height\n",
    "    biogenic_emission_layers = np.nonzero(out_h <= 10.0)\n",
    "    biogenic_emission_layer_height_cumsum = np.cumsum(out_d[biogenic_emission_layers])\n",
    "    biogenic_emission_layer_proportion = biogenic_emission_layer_height_cumsum / biogenic_emission_layer_height_cumsum[-1]\n",
    "    num_biogenic_emission_layers = sum(out_h <= 10.0)\n",
    "    \n",
    "    bio_t_h = np.zeros(shape=(out_t.size, out_h.size))\n",
    "    \n",
    "    bio_t_int = sp.interpolate.interp1d(\n",
    "        x=bio_t, y=ds.bio[key][:], kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    # Split up the biogenic emissions relative to the depth of the boxes\n",
    "    bio_t_h[:,biogenic_emission_layers] = (\n",
    "        np.tile(bio_t_int(x=out_t), (num_biogenic_emission_layers, 1, 1)) * biogenic_emission_layer_proportion.reshape(-1, 1, 1)\n",
    "    ).T\n",
    "    \n",
    "    return bio_t_h\n",
    "\n",
    "def interpolate_aerosol_emissions(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    aer_t = ds.aer[\"time\"][:].data\n",
    "    aer_h = ds.aer[\"mid_layer_height\"][:].data\n",
    "    \n",
    "    aer_t_h = ds.aer[key][:].T\n",
    "    \n",
    "    aer_t_h_int = sp.interpolate.interp2d(\n",
    "        x=aer_h, y=aer_t, z=aer_t_h, kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    return aer_t_h_int(x=out_h, y=out_t)\n",
    "\n",
    "def interpolate_anthropogenic_emissions(ds: TrajectoryDatasets, key: str):\n",
    "    out_t = get_output_time(ds)\n",
    "    out_h = ds.out[\"lev\"][:].data\n",
    "    \n",
    "    ant_t = ds.ant[\"time\"][:].data\n",
    "    ant_h = ds.ant[\"mid_layer_height\"][:].data\n",
    "    \n",
    "    ant_t_h = ds.ant[key][:].T\n",
    "    \n",
    "    ant_t_h_int = sp.interpolate.interp2d(\n",
    "        x=ant_h, y=ant_t, z=ant_t_h, kind=\"linear\", bounds_error=False, fill_value=0.0,\n",
    "    )\n",
    "    \n",
    "    return ant_t_h_int(x=out_h, y=out_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac89f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.703443Z",
     "iopub.status.busy": "2023-01-25T10:01:13.703305Z",
     "iopub.status.idle": "2023-01-25T10:01:13.723956Z",
     "shell.execute_reply": "2023-01-25T10:01:13.723482Z"
    },
    "papermill": {
     "duration": 0.035712,
     "end_time": "2023-01-25T10:01:13.724875",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.689163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_meteorology_features(ds: TrajectoryDatasets):\n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"met_t\": interpolate_meteorology_values(ds, \"t\").flatten(),\n",
    "        # \"met_u\": interpolate_meteorology_values(ds, \"u\").flatten(),\n",
    "        # \"met_v\": interpolate_meteorology_values(ds, \"v\").flatten(),\n",
    "        \"met_q\": interpolate_meteorology_values(ds, \"q\").flatten(),\n",
    "        # \"met_qc\": interpolate_meteorology_values(ds, \"qc\").flatten(),\n",
    "        # \"met_sp\": interpolate_meteorology_time_values(ds, \"sp\").flatten(),\n",
    "        # \"met_cp\": interpolate_meteorology_time_values(ds, \"cp\").flatten(),\n",
    "        # \"met_sshf\": interpolate_meteorology_time_values(ds, \"sshf\").flatten(),\n",
    "        \"met_ssr\": interpolate_meteorology_time_values(ds, \"ssr\").flatten(),\n",
    "        # \"met_lsp\": interpolate_meteorology_time_values(ds, \"lsp\").flatten(),\n",
    "        # \"met_ewss\": interpolate_meteorology_time_values(ds, \"ewss\").flatten(),\n",
    "        # \"met_nsss\": interpolate_meteorology_time_values(ds, \"nsss\").flatten(),\n",
    "        # \"met_tcc\": interpolate_meteorology_time_values(ds, \"tcc\").flatten(),\n",
    "        \"met_lsm\": interpolate_meteorology_time_values(ds, \"lsm\").flatten(),\n",
    "        # \"met_omega\": interpolate_meteorology_values(ds, \"omega\").flatten(),\n",
    "        # \"met_z\": interpolate_meteorology_time_values(ds, \"z\").flatten(),\n",
    "        # \"met_mla\": interpolate_meteorology_values(ds, \"mla\").flatten(),\n",
    "        # NOTE: lp is excluded because it allows the model to overfit\n",
    "        # \"met_lp\": interpolate_meteorology_values(ds, \"lp\").flatten(),\n",
    "        \"met_blh\": interpolate_meteorology_time_values(ds, \"blh\").flatten(),\n",
    "    }).set_index([\"time\", \"level\"])\n",
    "\n",
    "def get_bio_emissions_features(ds: TrajectoryDatasets):\n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"bio_acetaldehyde\": interpolate_biogenic_emissions(ds, \"acetaldehyde\").flatten(),\n",
    "        \"bio_acetone\": interpolate_biogenic_emissions(ds, \"acetone\").flatten(),\n",
    "        \"bio_butanes_and_higher_alkanes\": interpolate_biogenic_emissions(ds, \"butanes-and-higher-alkanes\").flatten(),\n",
    "        \"bio_butanes_and_higher_alkenes\": interpolate_biogenic_emissions(ds, \"butenes-and-higher-alkenes\").flatten(),\n",
    "        \"bio_ch4\": interpolate_biogenic_emissions(ds, \"CH4\").flatten(),\n",
    "        \"bio_co\": interpolate_biogenic_emissions(ds, \"CO\").flatten(),\n",
    "        \"bio_ethane\": interpolate_biogenic_emissions(ds, \"ethane\").flatten(),\n",
    "        \"bio_ethanol\": interpolate_biogenic_emissions(ds, \"ethanol\").flatten(),\n",
    "        \"bio_ethene\": interpolate_biogenic_emissions(ds, \"ethene\").flatten(),\n",
    "        \"bio_formaldehyde\": interpolate_biogenic_emissions(ds, \"formaldehyde\").flatten(),\n",
    "        \"bio_hydrogen_cyanide\": interpolate_biogenic_emissions(ds, \"hydrogen-cyanide\").flatten(),\n",
    "        \"bio_iosprene\": interpolate_biogenic_emissions(ds, \"isoprene\").flatten(),\n",
    "        \"bio_mbo\": interpolate_biogenic_emissions(ds, \"MBO\").flatten(),\n",
    "        \"bio_methanol\": interpolate_biogenic_emissions(ds, \"methanol\").flatten(),\n",
    "        \"bio_methyl_bromide\": interpolate_biogenic_emissions(ds, \"methyl-bromide\").flatten(),\n",
    "        \"bio_methyl_chloride\": interpolate_biogenic_emissions(ds, \"methyl-chloride\").flatten(),\n",
    "        \"bio_methyl_iodide\": interpolate_biogenic_emissions(ds, \"methyl-iodide\").flatten(),\n",
    "        \"bio_other_aldehydes\": interpolate_biogenic_emissions(ds, \"other-aldehydes\").flatten(),\n",
    "        \"bio_other_ketones\": interpolate_biogenic_emissions(ds, \"other-ketones\").flatten(),\n",
    "        \"bio_other_monoterpenes\": interpolate_biogenic_emissions(ds, \"other-monoterpenes\").flatten(),\n",
    "        \"bio_pinene_a\": interpolate_biogenic_emissions(ds, \"pinene-a\").flatten(),\n",
    "        \"bio_pinene_b\": interpolate_biogenic_emissions(ds, \"pinene-b\").flatten(),\n",
    "        \"bio_propane\": interpolate_biogenic_emissions(ds, \"propane\").flatten(),\n",
    "        \"bio_propene\": interpolate_biogenic_emissions(ds, \"propene\").flatten(),\n",
    "        \"bio_sesquiterpenes\": interpolate_biogenic_emissions(ds, \"sesquiterpenes\").flatten(),\n",
    "        \"bio_toluene\": interpolate_biogenic_emissions(ds, \"toluene\").flatten(),\n",
    "        \"bio_ch2br2\": interpolate_biogenic_emissions(ds, \"CH2Br2\").flatten(),\n",
    "        \"bio_ch3i\": interpolate_biogenic_emissions(ds, \"CH3I\").flatten(),\n",
    "        \"bio_chbr3\": interpolate_biogenic_emissions(ds, \"CHBr3\").flatten(),\n",
    "        \"bio_dms\": interpolate_biogenic_emissions(ds, \"DMS\").flatten(),\n",
    "    }).set_index([\"time\", \"level\"])\n",
    "\n",
    "def get_aer_emissions_features(ds: TrajectoryDatasets):\n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"aer_3_10_nm\": interpolate_aerosol_emissions(ds, \"3-10nm\").flatten(),\n",
    "        \"aer_10_20_nm\": interpolate_aerosol_emissions(ds, \"10-20nm\").flatten(),\n",
    "        \"aer_20_30_nm\": interpolate_aerosol_emissions(ds, \"20-30nm\").flatten(),\n",
    "        \"aer_30_50_nm\": interpolate_aerosol_emissions(ds, \"30-50nm\").flatten(),\n",
    "        \"aer_50_70_nm\": interpolate_aerosol_emissions(ds, \"50-70nm\").flatten(),\n",
    "        \"aer_70_100_nm\": interpolate_aerosol_emissions(ds, \"70-100nm\").flatten(),\n",
    "        \"aer_100_200_nm\": interpolate_aerosol_emissions(ds, \"100-200nm\").flatten(),\n",
    "        \"aer_200_400_nm\": interpolate_aerosol_emissions(ds, \"200-400nm\").flatten(),\n",
    "        \"aer_400_1000_nm\": interpolate_aerosol_emissions(ds, \"400-1000nm\").flatten(),\n",
    "    }).set_index([\"time\", \"level\"])\n",
    "\n",
    "def get_ant_emissions_features(ds: TrajectoryDatasets):\n",
    "    return pd.DataFrame({\n",
    "        \"time\": np.repeat(get_output_time(ds), ds.out[\"lev\"].shape[0]),\n",
    "        \"level\": np.tile(ds.out[\"lev\"][:].data, ds.out[\"time\"].shape[0]),\n",
    "        \"ant_co\": interpolate_anthropogenic_emissions(ds, \"co\").flatten(),\n",
    "        \"ant_nox\": interpolate_anthropogenic_emissions(ds, \"nox\").flatten(),\n",
    "        \"ant_co2\": interpolate_anthropogenic_emissions(ds, \"co2\").flatten(),\n",
    "        \"ant_nh3\": interpolate_anthropogenic_emissions(ds, \"nh3\").flatten(),\n",
    "        \"ant_ch4\": interpolate_anthropogenic_emissions(ds, \"ch4\").flatten(),\n",
    "        \"ant_so2\": interpolate_anthropogenic_emissions(ds, \"so2\").flatten(),\n",
    "        \"ant_nmvoc\": interpolate_anthropogenic_emissions(ds, \"nmvoc\").flatten(),\n",
    "        \"ant_alcohols\": interpolate_anthropogenic_emissions(ds, \"alcohols\").flatten(),\n",
    "        \"ant_ethane\": interpolate_anthropogenic_emissions(ds, \"ethane\").flatten(),\n",
    "        \"ant_propane\": interpolate_anthropogenic_emissions(ds, \"propane\").flatten(),\n",
    "        \"ant_butanes\": interpolate_anthropogenic_emissions(ds, \"butanes\").flatten(),\n",
    "        \"ant_pentanes\": interpolate_anthropogenic_emissions(ds, \"pentanes\").flatten(),\n",
    "        \"ant_hexanes\": interpolate_anthropogenic_emissions(ds, \"hexanes\").flatten(),\n",
    "        \"ant_ethene\": interpolate_anthropogenic_emissions(ds, \"ethene\").flatten(),\n",
    "        \"ant_propene\": interpolate_anthropogenic_emissions(ds, \"propene\").flatten(),\n",
    "        \"ant_acetylene\": interpolate_anthropogenic_emissions(ds, \"acetylene\").flatten(),\n",
    "        \"ant_isoprene\": interpolate_anthropogenic_emissions(ds, \"isoprene\").flatten(),\n",
    "        \"ant_monoterpenes\": interpolate_anthropogenic_emissions(ds, \"monoterpenes\").flatten(),\n",
    "        \"ant_other_alkenes_and_alkynes\": interpolate_anthropogenic_emissions(ds, \"other-alkenes-and-alkynes\").flatten(),\n",
    "        \"ant_benzene\": interpolate_anthropogenic_emissions(ds, \"benzene\").flatten(),\n",
    "        \"ant_toluene\": interpolate_anthropogenic_emissions(ds, \"toluene\").flatten(),\n",
    "        \"ant_xylene\": interpolate_anthropogenic_emissions(ds, \"xylene\").flatten(),\n",
    "        \"ant_trimethylbenzene\": interpolate_anthropogenic_emissions(ds, \"trimethylbenzene\").flatten(),\n",
    "        \"ant_other_aromatics\": interpolate_anthropogenic_emissions(ds, \"other-aromatics\").flatten(),\n",
    "        \"ant_esters\": interpolate_anthropogenic_emissions(ds, \"esters\").flatten(),\n",
    "        \"ant_ethers\": interpolate_anthropogenic_emissions(ds, \"ethers\").flatten(),\n",
    "        \"ant_formaldehyde\": interpolate_anthropogenic_emissions(ds, \"formaldehyde\").flatten(),\n",
    "        \"ant_other_aldehydes\": interpolate_anthropogenic_emissions(ds, \"other-aldehydes\").flatten(),\n",
    "        \"ant_total_ketones\": interpolate_anthropogenic_emissions(ds, \"total-ketones\").flatten(),\n",
    "        \"ant_total_acids\": interpolate_anthropogenic_emissions(ds, \"total-acids\").flatten(),\n",
    "        \"ant_other_vocs\": interpolate_anthropogenic_emissions(ds, \"other-VOCs\").flatten(),\n",
    "    }).set_index([\"time\", \"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cdc12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.753717Z",
     "iopub.status.busy": "2023-01-25T10:01:13.753577Z",
     "iopub.status.idle": "2023-01-25T10:01:13.768670Z",
     "shell.execute_reply": "2023-01-25T10:01:13.768220Z"
    },
    "papermill": {
     "duration": 0.029729,
     "end_time": "2023-01-25T10:01:13.769583",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.739854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/67809235\n",
    "def df_to_numpy(df):\n",
    "    try:\n",
    "        shape = [len(level) for level in df.index.levels]\n",
    "    except AttributeError:\n",
    "        shape = [len(df.index)]\n",
    "    ncol = df.shape[-1]\n",
    "    if ncol > 1:\n",
    "        shape.append(ncol)\n",
    "    return df.to_numpy().reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0bc7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.798794Z",
     "iopub.status.busy": "2023-01-25T10:01:13.798574Z",
     "iopub.status.idle": "2023-01-25T10:01:13.803346Z",
     "shell.execute_reply": "2023-01-25T10:01:13.802721Z"
    },
    "papermill": {
     "duration": 0.019778,
     "end_time": "2023-01-25T10:01:13.804285",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.784507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_time_level_windows():\n",
    "    # -0.5h, -1.5h, -3h, -6h, -12h, -24h, -48h\n",
    "    # 0, -2, -5, -11, -23, -47, -95\n",
    "    time_windows = [(0, 0), (-2, -1), (-5, -3), (-11, -6), (-23, -12), (-47, -24), (-95, -48)]\n",
    "    \n",
    "    # +1l, +2l, +4l, +8l, +16l, +32l, +64\n",
    "    top_windows = [(1, 1), (1, 2), (1, 4), (2, 8), (2, 16), (3, 32), (3, 64)]\n",
    "    mid_windows = [(0, 0), (0, 0), (0, 0), (-1, 1), (-1, 1), (-2, 2), (-2, 2)]\n",
    "    bot_windows = [(-1, -1), (-2, -1), (-4, -1), (-8, -2), (-16, -2), (-32, -3), (-64, -3)]\n",
    "    \n",
    "    return list(itertools.chain(\n",
    "        zip(time_windows, top_windows), zip(time_windows, mid_windows), zip(time_windows, bot_windows),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5faff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.833780Z",
     "iopub.status.busy": "2023-01-25T10:01:13.833644Z",
     "iopub.status.idle": "2023-01-25T10:01:13.837261Z",
     "shell.execute_reply": "2023-01-25T10:01:13.836852Z"
    },
    "papermill": {
     "duration": 0.018687,
     "end_time": "2023-01-25T10:01:13.838147",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.819460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_windowed_feature_names(columns):\n",
    "    time_windows = [\"-0.5h\", \"-1.5h\", \"-3h\", \"-6h\", \"-12h\", \"-24h\", \"-48h\"]\n",
    "    \n",
    "    top_windows = [\"+1l\", \"+2l\", \"+4l\", \"+8l\", \"+16l\", \"+32l\", \"+64l\"]\n",
    "    mid_windows = [\"+0l\", \"+0l\", \"+0l\", \"±1l\", \"±1l\", \"±2l\", \"±2l\"]\n",
    "    bot_windows = [\"-1l\", \"-2l\", \"-4l\", \"-8l\", \"-16l\", \"-32l\", \"-64l\"]\n",
    "    \n",
    "    names = []\n",
    "    \n",
    "    for (t, l) in itertools.chain(\n",
    "        zip(time_windows, top_windows), zip(time_windows, mid_windows), zip(time_windows, bot_windows),\n",
    "    ):\n",
    "        for c in columns:\n",
    "            names.append(f\"{c}{t}{l}\")\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676fb84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.867609Z",
     "iopub.status.busy": "2023-01-25T10:01:13.867473Z",
     "iopub.status.idle": "2023-01-25T10:01:13.875553Z",
     "shell.execute_reply": "2023-01-25T10:01:13.875144Z"
    },
    "papermill": {
     "duration": 0.023248,
     "end_time": "2023-01-25T10:01:13.876476",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.853228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_level_window_mean_v1(input, t_range, l_range):\n",
    "    output = np.zeros(shape=input.shape)\n",
    "\n",
    "    for t in range(input.shape[0]):\n",
    "        for l in range(input.shape[1]):\n",
    "            for f in range(input.shape[2]):\n",
    "                window = input[\n",
    "                    min(max(0, t+t_range[0]), input.shape[0]):max(0, min(t+1+t_range[1], input.shape[0])),\n",
    "                    min(max(0, l+l_range[0]), input.shape[1]):max(0, min(l+1+l_range[1], input.shape[1])),\n",
    "                    f\n",
    "                ]\n",
    "\n",
    "                output[t,l,f] = np.mean(window) if window.size > 0 else 0.0\n",
    "    \n",
    "    return output\n",
    "\n",
    "def time_level_window_mean_v2(input, t_range, l_range):\n",
    "    output = np.zeros(shape=input.shape)\n",
    "\n",
    "    for t in range(input.shape[0]):\n",
    "        mint = min(max(0, t+t_range[0]), input.shape[0])\n",
    "        maxt = max(0, min(t+1+t_range[1], input.shape[0]))\n",
    "        \n",
    "        if mint == maxt:\n",
    "            continue\n",
    "        \n",
    "        for l in range(input.shape[1]):\n",
    "            minl = min(max(0, l+l_range[0]), input.shape[1])\n",
    "            maxl = max(0, min(l+1+l_range[1], input.shape[1]))\n",
    "            \n",
    "            if minl == maxl:\n",
    "                continue\n",
    "                \n",
    "            output[t,l,:] = np.mean(input[mint:maxt,minl:maxl,:], axis=(0,1))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def time_level_window_mean_v3(input, t_range, l_range):\n",
    "    min_t = min(t_range[0], 0)\n",
    "    max_t = max(0, t_range[1])\n",
    "    abs_t = max(abs(min_t), abs(max_t))\n",
    "    \n",
    "    min_l = min(l_range[0], 0)\n",
    "    max_l = max(0, l_range[1])\n",
    "    abs_l = max(abs(min_l), abs(max_l))\n",
    "    \n",
    "    kernel = np.zeros(shape=(abs_t*2 + 1, abs_l*2 + 1, 1))\n",
    "    kernel[t_range[0]+abs_t:t_range[1]+abs_t+1,l_range[0]+abs_l:l_range[1]+abs_l+1,:] = 1.0\n",
    "    kernel = kernel[::-1,::-1]\n",
    "    \n",
    "    quot = sp.ndimage.convolve(np.ones_like(input), kernel, mode='constant', cval=0.0)\n",
    "    \n",
    "    result = np.zeros_like(input)\n",
    "    \n",
    "    np.divide(\n",
    "        sp.ndimage.convolve(input, kernel, mode='constant', cval=0.0),\n",
    "        quot, out=result, where=quot > 0,\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128aaf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.905945Z",
     "iopub.status.busy": "2023-01-25T10:01:13.905799Z",
     "iopub.status.idle": "2023-01-25T10:01:13.908745Z",
     "shell.execute_reply": "2023-01-25T10:01:13.908343Z"
    },
    "papermill": {
     "duration": 0.018056,
     "end_time": "2023-01-25T10:01:13.909591",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.891535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_features_for_dataset(ds: TrajectoryDatasets):\n",
    "    bio_features = get_bio_emissions_features(ds)\n",
    "    aer_features = get_aer_emissions_features(ds) * 1e21\n",
    "    ant_features = get_ant_emissions_features(ds)\n",
    "    met_features = get_meteorology_features(ds)\n",
    "    \n",
    "    return pd.concat([\n",
    "        bio_features, aer_features, ant_features, met_features,\n",
    "    ], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03acab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.938815Z",
     "iopub.status.busy": "2023-01-25T10:01:13.938595Z",
     "iopub.status.idle": "2023-01-25T10:01:13.943083Z",
     "shell.execute_reply": "2023-01-25T10:01:13.942621Z"
    },
    "papermill": {
     "duration": 0.019594,
     "end_time": "2023-01-25T10:01:13.943986",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.924392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features_from_raw_features(raw_features):\n",
    "    raw_features_np = df_to_numpy(raw_features)\n",
    "    \n",
    "    features_np = np.concatenate([\n",
    "        raw_features.index.get_level_values(0).to_numpy().reshape(\n",
    "            (raw_features.index.levels[0].size, raw_features.index.levels[1].size, 1)\n",
    "        ),\n",
    "        raw_features.index.get_level_values(1).to_numpy().reshape(\n",
    "            (raw_features.index.levels[0].size, raw_features.index.levels[1].size, 1)\n",
    "        )\n",
    "    ] + joblib.Parallel(n_jobs=-1)([\n",
    "        joblib.delayed(time_level_window_mean_v2)(raw_features_np, t, l) for t, l in generate_time_level_windows()\n",
    "    ]), axis=2)\n",
    "    \n",
    "    # Trim off the first two days, for which the time features are ill-defined\n",
    "    features_np_trimmed = features_np[95:-1,:,:]\n",
    "    \n",
    "    feature_names = [\"time\", \"level\"] + generate_windowed_feature_names(raw_features.columns)\n",
    "    \n",
    "    features = pd.DataFrame(features_np_trimmed.reshape(\n",
    "        features_np_trimmed.shape[0]*features_np_trimmed.shape[1], features_np_trimmed.shape[2],\n",
    "    ), columns=feature_names).set_index([\"time\", \"level\"])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f3825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:13.973408Z",
     "iopub.status.busy": "2023-01-25T10:01:13.973188Z",
     "iopub.status.idle": "2023-01-25T10:01:13.977456Z",
     "shell.execute_reply": "2023-01-25T10:01:13.977046Z"
    },
    "papermill": {
     "duration": 0.01929,
     "end_time": "2023-01-25T10:01:13.978350",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.959060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_labels_for_dataset(ds: TrajectoryDatasets):\n",
    "    ccn_concentration = get_ccn_concentration(ds)\n",
    "    \n",
    "    ccn_concentration_np = df_to_numpy(ccn_concentration)\n",
    "    \n",
    "    labels_np = np.concatenate([\n",
    "        ccn_concentration.index.get_level_values(0).to_numpy().reshape(\n",
    "            (ccn_concentration.index.levels[0].size, ccn_concentration.index.levels[1].size, 1)\n",
    "        ),\n",
    "        ccn_concentration.index.get_level_values(1).to_numpy().reshape(\n",
    "            (ccn_concentration.index.levels[0].size, ccn_concentration.index.levels[1].size, 1)\n",
    "        ),\n",
    "        ccn_concentration_np.reshape(\n",
    "            (ccn_concentration_np.shape[0], ccn_concentration_np.shape[1], 1)\n",
    "        ),\n",
    "    ], axis=2)\n",
    "    \n",
    "    # Trim off the first two days, for which the time features are ill-defined\n",
    "    labels_np_trimmed = labels_np[96:,:,:]\n",
    "    \n",
    "    label_names = [\"time\", \"level\", \"ccn\"]\n",
    "    \n",
    "    labels = pd.DataFrame(labels_np_trimmed.reshape(\n",
    "        labels_np_trimmed.shape[0]*labels_np_trimmed.shape[1], labels_np_trimmed.shape[2],\n",
    "    ), columns=label_names).set_index([\"time\", \"level\"])\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf43e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:14.010144Z",
     "iopub.status.busy": "2023-01-25T10:01:14.010005Z",
     "iopub.status.idle": "2023-01-25T10:01:14.012955Z",
     "shell.execute_reply": "2023-01-25T10:01:14.012546Z"
    },
    "papermill": {
     "duration": 0.018426,
     "end_time": "2023-01-25T10:01:14.013864",
     "exception": false,
     "start_time": "2023-01-25T10:01:13.995438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hash_for_dt(dt):\n",
    "    if not(isinstance(dt, tuple) or isinstance(dt, list)):\n",
    "        dt = [dt]\n",
    "    \n",
    "    dt_str = '.'.join(dtt.strftime('%d.%m.%Y-%H:00%z') for dtt in dt)\n",
    "    \n",
    "    h = hashlib.shake_256()\n",
    "    h.update(dt_str.encode('ascii'))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e04dd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:14.043698Z",
     "iopub.status.busy": "2023-01-25T10:01:14.043250Z",
     "iopub.status.idle": "2023-01-25T10:01:14.047542Z",
     "shell.execute_reply": "2023-01-25T10:01:14.047088Z"
    },
    "papermill": {
     "duration": 0.019423,
     "end_time": "2023-01-25T10:01:14.048460",
     "exception": false,
     "start_time": "2023-01-25T10:01:14.029037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clumped 0/1 sampler using a Markov Process\n",
    "\n",
    "P(0) = p and P(1) = 1-p\n",
    "clump = 0 => IID samples\n",
    "clump -> 1 => highly correlated samples\n",
    "\n",
    "\"\"\"\n",
    "class Clump:\n",
    "    def __init__(self, p=0.5, clump=0.0, rng=None):\n",
    "        a = 1 - (1-p)*(1-clump)\n",
    "        b = (1-a)*p/(1-p)\n",
    "        \n",
    "        self.C = np.array([[a, 1-a],[b, 1-b]])\n",
    "        \n",
    "        self.i = 0 if rng.random() < p else 1\n",
    "    \n",
    "    def sample(self, rng):\n",
    "        p = self.C[self.i,0]\n",
    "        u = rng.random()\n",
    "        \n",
    "        self.i = 0 if u < p else 1\n",
    "        \n",
    "        return self.i\n",
    "    \n",
    "    def steady(self, X):\n",
    "        return np.matmul(X, self.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ce21a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:14.078182Z",
     "iopub.status.busy": "2023-01-25T10:01:14.078045Z",
     "iopub.status.idle": "2023-01-25T10:01:14.084083Z",
     "shell.execute_reply": "2023-01-25T10:01:14.083672Z"
    },
    "papermill": {
     "duration": 0.021247,
     "end_time": "2023-01-25T10:01:14.084943",
     "exception": false,
     "start_time": "2023-01-25T10:01:14.063696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, Y, test_size=0.25, random_state=None, shuffle=True, clump=0.0):\n",
    "    assert len(X) == len(Y)\n",
    "    assert type(X) == type(Y)\n",
    "    assert test_size > 0.0\n",
    "    assert test_size < 1.0\n",
    "    assert random_state is not None\n",
    "    assert clump >= 0.0\n",
    "    assert clump < 1.0\n",
    "    \n",
    "    c = Clump(p=test_size, clump=clump, rng=random_state)\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        assert X.index.values.shape == Y.index.values.shape\n",
    "        \n",
    "        # Split only based on the first-level index instead of flattening\n",
    "        n1 = len(X.index.levels[1])\n",
    "        n0 = len(X) // n1\n",
    "        \n",
    "        C = np.array([c.sample(random_state) for _ in range(n0)])\n",
    "        I_train, = np.nonzero(C)\n",
    "        I_train = np.repeat(I_train, n1) * n1 + np.tile(np.arange(n1), len(I_train))\n",
    "        I_test, = np.nonzero(1-C)\n",
    "        I_test = np.repeat(I_test, n1) * n1 + np.tile(np.arange(n1), len(I_test))\n",
    "    else:\n",
    "        C = np.array([c.sample(random_state) for _ in range(len(X))])\n",
    "        I_train, = np.nonzero(C)\n",
    "        I_test, = np.nonzero(1-C)\n",
    "    \n",
    "    if shuffle:\n",
    "        random_state.shuffle(I_train)\n",
    "        random_state.shuffle(I_test)\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_train = X.iloc[I_train]\n",
    "        X_test = X.iloc[I_test]\n",
    "        \n",
    "        Y_train = Y.iloc[I_train]\n",
    "        Y_test = Y.iloc[I_test]\n",
    "    else:\n",
    "        X_train = X[I_train]\n",
    "        X_test = X[I_test]\n",
    "        \n",
    "        Y_train = Y[I_train]\n",
    "        Y_test = Y[I_test]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa585ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:14.114242Z",
     "iopub.status.busy": "2023-01-25T10:01:14.114104Z",
     "iopub.status.idle": "2023-01-25T10:01:14.123235Z",
     "shell.execute_reply": "2023-01-25T10:01:14.122816Z"
    },
    "papermill": {
     "duration": 0.024469,
     "end_time": "2023-01-25T10:01:14.124122",
     "exception": false,
     "start_time": "2023-01-25T10:01:14.099653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_cache_dataset(dt: datetime.datetime, clump: float, datasets: dict) -> MLDataset:\n",
    "    if isinstance(dt, tuple) or isinstance(dt, list):\n",
    "        dt = tuple(sorted(dt))\n",
    "    \n",
    "    cached = datasets.get((dt, clump))\n",
    "    \n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    \n",
    "    if isinstance(dt, tuple) or isinstance(dt, list):\n",
    "        mls = [load_and_cache_dataset(dtt, clump, datasets) for dtt in dt]\n",
    "\n",
    "        dp = tuple(ml.paths for ml in mls)\n",
    "        X_raw = pd.concat([ml.X_raw for ml in mls], axis='index')\n",
    "        Y = pd.concat([ml.Y_raw for ml in mls], axis='index')\n",
    "\n",
    "        train_features = np.concatenate([ml.X_scaler.inverse_transform(ml.X_train) for ml in mls], axis=0)\n",
    "        train_labels = np.concatenate([ml.Y_scaler.inverse_transform(ml.Y_train) for ml in mls], axis=0)\n",
    "        valid_features = np.concatenate([ml.X_scaler.inverse_transform(ml.X_valid) for ml in mls], axis=0)\n",
    "        valid_labels = np.concatenate([ml.Y_scaler.inverse_transform(ml.Y_valid) for ml in mls], axis=0)\n",
    "        test_features = np.concatenate([ml.X_scaler.inverse_transform(ml.X_test) for ml in mls], axis=0)\n",
    "        test_labels = np.concatenate([ml.Y_scaler.inverse_transform(ml.Y_test) for ml in mls], axis=0)\n",
    "    else:\n",
    "        dp = get_path_for_perturbation(dt, Path(\"baseline\"))\n",
    "        ds = load_trajectory_dataset(dp)\n",
    "\n",
    "        X_raw = get_raw_features_for_dataset(ds)\n",
    "\n",
    "        X = get_features_from_raw_features(X_raw)\n",
    "        Y = np.log10(get_labels_for_dataset(ds) + 1)\n",
    "\n",
    "        rng = np.random.RandomState(seed=int.from_bytes(hash_for_dt(dt).digest(4), 'little'))\n",
    "\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            X, Y, test_size=0.25, random_state=rng, clump=clump,\n",
    "        )\n",
    "        train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "            train_features, train_labels, test_size=1.0/3.0, random_state=rng, clump=clump,\n",
    "        )\n",
    "\n",
    "        # Close the NetCDF datasets\n",
    "        ds.out.close()\n",
    "        ds.aer.close()\n",
    "        ds.ant.close()\n",
    "        ds.bio.close()\n",
    "        ds.met.close()\n",
    "\n",
    "    # Scale features to N(0,1)\n",
    "    # - only fit on training data\n",
    "    # - OOD inputs for constants at training time are blown up\n",
    "    feature_scaler = StandardScaler().fit(train_features)\n",
    "    feature_scaler.scale_[np.nonzero(feature_scaler.var_ == 0.0)] = np.nan_to_num(np.inf)\n",
    "\n",
    "    label_scaler = StandardScaler().fit(train_labels)\n",
    "\n",
    "    train_features = feature_scaler.transform(train_features)\n",
    "    train_labels = label_scaler.transform(train_labels)\n",
    "    valid_features = feature_scaler.transform(valid_features)\n",
    "    valid_labels = label_scaler.transform(valid_labels)\n",
    "    test_features = feature_scaler.transform(test_features)\n",
    "    test_labels = label_scaler.transform(test_labels)\n",
    "\n",
    "    dataset = MLDataset(\n",
    "        date=dt, paths=dp, X_raw=X_raw, Y_raw=Y,\n",
    "        X_train=train_features, X_valid=valid_features, X_test=test_features,\n",
    "        Y_train=train_labels, Y_valid=valid_labels, Y_test=test_labels,\n",
    "        X_scaler=feature_scaler, Y_scaler=label_scaler,\n",
    "    )\n",
    "\n",
    "    datasets[(dt, clump)] = dataset\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ec3e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:14.154372Z",
     "iopub.status.busy": "2023-01-25T10:01:14.153844Z",
     "iopub.status.idle": "2023-01-25T10:01:14.156402Z",
     "shell.execute_reply": "2023-01-25T10:01:14.156004Z"
    },
    "papermill": {
     "duration": 0.017777,
     "end_time": "2023-01-25T10:01:14.157251",
     "exception": false,
     "start_time": "2023-01-25T10:01:14.139474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb70eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERTURBED_DATASETS = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "IcarusPrediction = namedtuple(\"IcarusPrediction\", [\"prediction\", \"uncertainty\", \"confidence\"])\n",
    "\n",
    "class IcarusRSM(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: np.ndarray, Y_train: np.ndarray,\n",
    "        X_valid: np.ndarray, Y_valid: np.ndarray,\n",
    "        rng: np.random.Generator, **kwargs,\n",
    "    ) -> IcarusRSM:\n",
    "        return self\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def predict(self, X_test: np.ndarray, rng: np.random.Generator, **kwargs) -> IcarusPrediction:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ded9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestSosaaRSM(IcarusRSM):\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: np.ndarray, Y_train: np.ndarray,\n",
    "        X_valid: np.ndarray, Y_valid: np.ndarray,\n",
    "        rng: np.random.Generator,\n",
    "        n_trees: int = 16, verbose: int = 1,\n",
    "    ) -> RandomForestSosaaRSM:\n",
    "        assert Y_train.shape[1:] == (1,)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"Training the RandomForestSosaaRSM\")\n",
    "\n",
    "            print(\" - Training the OOD detector\")\n",
    "            print(\"   - Fitting truncated PCA\")\n",
    "        \n",
    "        self.pca = PCA(random_state=rng).fit(X_train)\n",
    "        self.bn = np.searchsorted(np.cumsum(self.pca.explained_variance_ratio_), 0.95)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"   - Fitting truncated PCA reconstruction error covariance\")\n",
    "        \n",
    "        self.cov = EmpiricalCovariance().fit((self._predict_truncated_pca(X_train) - X_train))\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"   - Generating FGSM OOD inputs\")\n",
    "        \n",
    "        adv_grad = self.pca.components_[self.bn]\n",
    "        \n",
    "        X_ood = rng.normal(loc=X_valid, scale=0.01) + np.sign(adv_grad) * np.abs(\n",
    "            rng.normal(loc=2.0, scale=0.5, size=(len(X_valid), 1))\n",
    "        ) * rng.choice([-1, 1])\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"   - Training the OOD classifier\")\n",
    "        \n",
    "        M_id = self.cov.mahalanobis(self._predict_truncated_pca(X_valid) - X_valid)\n",
    "        M_ood = self.cov.mahalanobis(self._predict_truncated_pca(X_ood) - X_ood)\n",
    "        \n",
    "        self.scaler = StandardScaler().fit(M_id.reshape(-1, 1))\n",
    "        \n",
    "        self.ood_detector = LogisticRegression(\n",
    "            penalty='none', class_weight=\"balanced\", random_state=rng,\n",
    "        ).fit(\n",
    "            np.concatenate([\n",
    "                self.scaler.transform(M_id.reshape(-1, 1)),\n",
    "                self.scaler.transform(M_ood.reshape(-1, 1)),\n",
    "            ], axis=0).reshape(-1, 1),\n",
    "            np.concatenate([\n",
    "                np.ones(len(M_id)), np.zeros(len(M_ood)),\n",
    "            ], axis=0),\n",
    "        )\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\" - Training the Prediction Model and Uncertainty Quantifier\")\n",
    "        \n",
    "        self.predictor = RandomForestRegressor(\n",
    "            n_estimators=n_trees, random_state=rng, n_jobs=-1, min_samples_leaf=5,\n",
    "            max_features=1.0/3.0, verbose=verbose,\n",
    "        ).fit(X_train, Y_train.ravel())\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\" - Finished training the RandomForestSosaaRSM\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(\n",
    "        self, X_test: np.ndarray, rng: np.random.Generator, verbose: int = 1,\n",
    "    ) -> IcarusPrediction:\n",
    "        # No extra randomness is needed during prediction\n",
    "        _rng = rng\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(\"Predicting with the RandomForestSosaaRSM\")\n",
    "            \n",
    "            print(\" - Generating confidence scores\")\n",
    "        \n",
    "        confidence = self.ood_detector.predict_proba(self.scaler.transform(\n",
    "            self.cov.mahalanobis(self._predict_truncated_pca(X_test) - X_test).reshape(-1, 1)\n",
    "        ))[:,1]\n",
    "            \n",
    "        if verbose > 1:\n",
    "            print(\" - Generating ensemble predictions\")\n",
    "            \n",
    "        def tree_predict(i: int) -> np.ndarray:\n",
    "            if verbose > 1:\n",
    "                print(f\"   - Predicting tree {i}/{len(self.predictor.estimators_)}\")\n",
    "            \n",
    "            return self.predictor.estimators_[i].predict(X_test)\n",
    "        \n",
    "        predictions = joblib.Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "            joblib.delayed(tree_predict)(i) for i in range(len(self.predictor.estimators_))\n",
    "        )\n",
    "        \n",
    "        prediction = np.mean(np.stack(predictions, axis=0), axis=0).reshape((len(X_test), 1))\n",
    "        uncertainty = np.std(np.stack(predictions, axis=0), axis=0).reshape((len(X_test), 1))\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(\" - Finished predicting with the RandomForestSosaaRSM\")\n",
    "            \n",
    "        return IcarusPrediction(\n",
    "            prediction=prediction, uncertainty=uncertainty, confidence=confidence,\n",
    "        )\n",
    "\n",
    "    def _predict_truncated_pca(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.pca.mean_ is not None:\n",
    "            X = X - self.pca.mean_\n",
    "        \n",
    "        X_trans = np.dot(X, self.pca.components_[:self.bn].T)\n",
    "        X = np.dot(X_trans, self.pca.components_[:self.bn])\n",
    "        \n",
    "        if self.pca.mean_ is not None:\n",
    "            X = X + self.pca.mean_\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseDifferenceRegressionRandomForestSosaaRSM(IcarusRSM):\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: np.ndarray, Y_train: np.ndarray,\n",
    "        X_valid: np.ndarray, Y_valid: np.ndarray,\n",
    "        rng: np.random.Generator,\n",
    "        n_trees: int = 16, n_samples: int = 16, verbose: int = 1,\n",
    "    ) -> PairwiseDifferenceRegressionRandomForestSosaaRSM:\n",
    "        assert Y_train.shape[1:] == (1,)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"Training the PairwiseDifferenceRegressionRandomForestSosaaRSM\")\n",
    "            \n",
    "            print(\" - Resampling the training and validation datasets\")\n",
    "    \n",
    "        Ia_train = rng.choice(len(self.X_train), size=len(X_train)*n_samples, replace=True)\n",
    "        Ib_train = rng.choice(len(X_train), size=len(X_train)*n_samples, replace=True)\n",
    "        Ia_valid = rng.choice(len(self.X_train), size=len(X_valid)*n_samples, replace=True)\n",
    "        Ib_valid = rng.choice(len(X_valid), size=len(X_valid)*n_samples, replace=True)\n",
    "        \n",
    "        # N(0,1)-N(0,1) ~ N(0,2) -> divide by sqrt(2) s.t. all features are N(0,1)\n",
    "        X_train = np.concatenate([\n",
    "            self.X_train[Ia_train], (X_train[Ib_train]-self.X_train[Ia_train]) / np.sqrt(2.0),\n",
    "        ], axis=1)\n",
    "        Y_train = Y_train[Ib_train] - self.Y_train[Ia_train]\n",
    "        \n",
    "        X_valid = np.concatenate([\n",
    "            self.X_train[Ia_valid], (X_valid[Ib_valid]-self.X_train[Ia_valid]) / np.sqrt(2.0),\n",
    "        ], axis=1)\n",
    "        Y_valid = Y_valid[Ib_valid] - self.Y_train[Ia_valid]\n",
    "            \n",
    "        if verbose > 0:\n",
    "            print(\" - Training the OOD detector\")\n",
    "            print(\"   - Fitting truncated PCA\")\n",
    "        \n",
    "        self.pca = PCA(random_state=rng).fit(X_train)\n",
    "        self.bn = np.searchsorted(np.cumsum(self.pca.explained_variance_ratio_), 0.95)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"   - Fitting truncated PCA reconstruction error covariance\")\n",
    "        \n",
    "        self.cov = EmpiricalCovariance().fit((self._predict_truncated_pca(X_train) - X_train))\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"   - Generating FGSM OOD inputs\")\n",
    "        \n",
    "        adv_grad = self.pca.components_[self.bn]\n",
    "        \n",
    "        X_ood = rng.normal(loc=X_valid, scale=0.01) + np.sign(adv_grad) * np.abs(\n",
    "            rng.normal(loc=2.0, scale=0.5, size=(len(X_valid), 1))\n",
    "        ) * rng.choice([-1, 1])\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"   - Training the OOD classifier\")\n",
    "        \n",
    "        M_id = self.cov.mahalanobis(self._predict_truncated_pca(X_valid) - X_valid)\n",
    "        M_ood = self.cov.mahalanobis(self._predict_truncated_pca(X_ood) - X_ood)\n",
    "        \n",
    "        self.scaler = StandardScaler().fit(M_id.reshape(-1, 1))\n",
    "        \n",
    "        self.ood_detector = LogisticRegression(\n",
    "            penalty='none', class_weight=\"balanced\", random_state=rng,\n",
    "        ).fit(\n",
    "            np.concatenate([\n",
    "                self.scaler.transform(M_id.reshape(-1, 1)),\n",
    "                self.scaler.transform(M_ood.reshape(-1, 1)),\n",
    "            ], axis=0).reshape(-1, 1),\n",
    "            np.concatenate([\n",
    "                np.ones(len(M_id)), np.zeros(len(M_ood)),\n",
    "            ], axis=0),\n",
    "        )\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\" - Training the Prediction Model and Uncertainty Quantifier\")\n",
    "        \n",
    "        self.predictor = RandomForestRegressor(\n",
    "            n_estimators=n_trees, random_state=rng, n_jobs=-1, min_samples_leaf=5,\n",
    "            max_features=1.0/3.0, verbose=verbose,\n",
    "        ).fit(X_train, Y_train)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\" - Calibrating the Uncertainty Quantifier\")\n",
    "        \n",
    "        def tree_predict(i: int) -> np.ndarray:\n",
    "            if verbose > 1:\n",
    "                print(f\"   - Predicting tree {i}/{len(self.predictor.estimators_)}\")\n",
    "            \n",
    "            return self.predictor.estimators_[i].predict(X_valid)\n",
    "            \n",
    "        valid_predictions = joblib.Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "            joblib.delayed(tree_predict)(i) for i in range(len(self.predictor.estimators_))\n",
    "        )\n",
    "        \n",
    "        Y_valid_pred = np.mean(np.stack(valid_predictions, axis=0), axis=0)\n",
    "        Y_valid_stdv = np.std(np.stack(valid_predictions, axis=0), axis=0)\n",
    "            \n",
    "        Zc = (Y_valid.flatten() - Y_valid_pred.flatten()) / Y_valid_stdv.flatten()\n",
    "        \n",
    "        self.Zc_mean = np.mean(Zc)\n",
    "        self.Zc_stdv = np.std(Zc)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"- Finished training the PairwiseDifferenceRegressionRandomForestSosaaRSM\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(\n",
    "        self, X_test: np.ndarray, rng: np.random.Generator, verbose: int = 1,\n",
    "        direct_difference: bool = False, X_base: np.ndarray = None, Y_base: np.ndarray = None,\n",
    "    ) -> IcarusPrediction:\n",
    "        if verbose > 1:\n",
    "            print(\"Predicting with the PairwiseDifferenceRegressionRandomForestSosaaRSM\")\n",
    "            \n",
    "            print(\" - Resampling the input dataset\")\n",
    "    \n",
    "        if not direct_difference:\n",
    "            # Only one anchor sample is produced, call predict several times for more\n",
    "            Ia_test = rng.choice(len(self.X_train), size=len(X_test), replace=True)\n",
    "            X_train = self.X_train\n",
    "            Y_train = self.Y_train\n",
    "        else:\n",
    "            if (X_base is not None) and (Y_base is not None):\n",
    "                X_train = X_base\n",
    "                Y_train = Y_base\n",
    "            else:\n",
    "                X_train = self.X_train\n",
    "                Y_train = self.Y_train\n",
    "            \n",
    "            assert len(X_test) == len(X_train)\n",
    "            assert len(X_train) == len(Y_train)\n",
    "            \n",
    "            Ia_test = np.arange(len(X_train))\n",
    "        \n",
    "        # N(0,1)-N(0,1) ~ N(0,2) -> divide by sqrt(2) s.t. all features are N(0,1)\n",
    "        X_test = np.concatenate([\n",
    "            X_train[Ia_test], (X_test-X_train[Ia_test]) / np.sqrt(2.0),\n",
    "        ], axis=1)\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(\" - Generating confidence scores\")\n",
    "        \n",
    "        confidence = self.ood_detector.predict_proba(self.scaler.transform(\n",
    "            self.cov.mahalanobis(self._predict_truncated_pca(X_test) - X_test).reshape(-1, 1)\n",
    "        ))[:,1]\n",
    "            \n",
    "        if verbose > 1:\n",
    "            print(\" - Generating ensemble predictions\")\n",
    "        \n",
    "        def tree_predict(i: int) -> np.ndarray:\n",
    "            if verbose > 1:\n",
    "                print(f\"   - Predicting tree {i}/{len(self.predictor.estimators_)}\")\n",
    "            \n",
    "            return self.predictor.estimators_[i].predict(X_test)\n",
    "        \n",
    "        predictions = joblib.Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "            joblib.delayed(tree_predict)(i) for i in range(len(self.predictor.estimators_))\n",
    "        )\n",
    "        \n",
    "        prediction = np.mean(np.stack(predictions, axis=0), axis=0).reshape((len(X_test), 1))\n",
    "        uncertainty = np.std(np.stack(predictions, axis=0), axis=0).reshape((len(X_test), 1))\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(\" - Recalibrating predictions and uncertainties\")\n",
    "        \n",
    "        prediction = Y_train[Ia_test] + (\n",
    "            prediction.flatten() + self.Zc_mean * uncertainty.flatten()\n",
    "        ).reshape((len(X_test), 1))\n",
    "        uncertainty = (uncertainty.flatten() * self.Zc_stdv).reshape((len(X_test), 1))\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(\" - Finished predicting with the PairwiseDifferenceRegressionRandomForestSosaaRSM\")\n",
    "            \n",
    "        return IcarusPrediction(\n",
    "            prediction=prediction, uncertainty=uncertainty, confidence=confidence,\n",
    "        )\n",
    "\n",
    "    def _predict_truncated_pca(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.pca.mean_ is not None:\n",
    "            X = X - self.pca.mean_\n",
    "        \n",
    "        X_trans = np.dot(X, self.pca.components_[:self.bn].T)\n",
    "        X = np.dot(X_trans, self.pca.components_[:self.bn])\n",
    "        \n",
    "        if self.pca.mean_ is not None:\n",
    "            X = X + self.pca.mean_\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2043ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercentilePairwiseDifferenceRegressionRandomForestSosaaRSM(IcarusRSM):\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: np.ndarray, Y_train: np.ndarray,\n",
    "        X_valid: np.ndarray, Y_valid: np.ndarray,\n",
    "        rng: np.random.Generator,\n",
    "        n_trees: int = 16, n_samples: int = 16, verbose: int = 1,\n",
    "    ) -> PairwiseDifferenceRegressionRandomForestSosaaRSM:\n",
    "        assert Y_train.shape[1:] == (1,)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"Training the PercentilePairwiseDifferenceRegressionRandomForestSosaaRSM\")\n",
    "            \n",
    "            print(\" - Resampling the training and validation datasets\")\n",
    "    \n",
    "        Ia_train = rng.choice(len(self.X_train), size=len(X_train)*n_samples, replace=True)\n",
    "        Ib_train = rng.choice(len(X_train), size=len(X_train)*n_samples, replace=True)\n",
    "        Ia_valid = rng.choice(len(self.X_train), size=len(X_valid)*n_samples, replace=True)\n",
    "        Ib_valid = rng.choice(len(X_valid), size=len(X_valid)*n_samples, replace=True)\n",
    "        \n",
    "        # N(0,1)-N(0,1) ~ N(0,2) -> divide by sqrt(2) s.t. all features are N(0,1)\n",
    "        X_train = np.concatenate([\n",
    "            self.X_train[Ia_train], (X_train[Ib_train]-self.X_train[Ia_train]) / np.sqrt(2.0),\n",
    "        ], axis=1)\n",
    "        Y_train = Y_train[Ib_train] - self.Y_train[Ia_train]\n",
    "        \n",
    "        X_valid = np.concatenate([\n",
    "            self.X_train[Ia_valid], (X_valid[Ib_valid]-self.X_train[Ia_valid]) / np.sqrt(2.0),\n",
    "        ], axis=1)\n",
    "        Y_valid = Y_valid[Ib_valid] - self.Y_train[Ia_valid]\n",
    "            \n",
    "        if verbose > 0:\n",
    "            print(\" - Training the OOD detector\")\n",
    "            print(\"   - Fitting truncated PCA\")\n",
    "        \n",
    "        self.pca = PCA(random_state=rng).fit(X_train)\n",
    "        self.bn = np.searchsorted(np.cumsum(self.pca.explained_variance_ratio_), 0.95)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"   - Fitting truncated PCA reconstruction error covariance\")\n",
    "        \n",
    "        self.cov = EmpiricalCovariance().fit((self._predict_truncated_pca(X_train) - X_train))\n",
    "        \n",
    "        self.err_valid = np.sort(\n",
    "            self.cov.mahalanobis(self._predict_truncated_pca(X_valid) - X_valid)\n",
    "        )\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\" - Training the Prediction Model and Uncertainty Quantifier\")\n",
    "        \n",
    "        self.predictor = RandomForestRegressor(\n",
    "            n_estimators=n_trees, random_state=rng, n_jobs=-1, min_samples_leaf=5,\n",
    "            max_features=1.0/3.0, verbose=verbose,\n",
    "        ).fit(X_train, Y_train)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\" - Calibrating the Uncertainty Quantifier\")\n",
    "        \n",
    "        def tree_predict(i: int) -> np.ndarray:\n",
    "            if verbose > 1:\n",
    "                print(f\"   - Predicting tree {i}/{len(self.predictor.estimators_)}\")\n",
    "            \n",
    "            return self.predictor.estimators_[i].predict(X_valid)\n",
    "            \n",
    "        valid_predictions = joblib.Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "            joblib.delayed(tree_predict)(i) for i in range(len(self.predictor.estimators_))\n",
    "        )\n",
    "        \n",
    "        Y_valid_pred = np.mean(np.stack(valid_predictions, axis=0), axis=0)\n",
    "        Y_valid_stdv = np.std(np.stack(valid_predictions, axis=0), axis=0)\n",
    "            \n",
    "        Zc = (Y_valid.flatten() - Y_valid_pred.flatten()) / Y_valid_stdv.flatten()\n",
    "        \n",
    "        self.Zc_mean = np.mean(Zc)\n",
    "        self.Zc_stdv = np.std(Zc)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"- Finished training the PercentilePairwiseDifferenceRegressionRandomForestSosaaRSM\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(\n",
    "        self, X_test: np.ndarray, rng: np.random.Generator, verbose: int = 1,\n",
    "        direct_difference: bool = False, X_base: np.ndarray = None, Y_base: np.ndarray = None,\n",
    "    ) -> IcarusPrediction:\n",
    "        if verbose > 1:\n",
    "            print(\"Predicting with the PercentilePairwiseDifferenceRegressionRandomForestSosaaRSM\")\n",
    "            \n",
    "            print(\" - Resampling the input dataset\")\n",
    "    \n",
    "        if not direct_difference:\n",
    "            # Only one anchor sample is produced, call predict several times for more\n",
    "            Ia_test = rng.choice(len(self.X_train), size=len(X_test), replace=True)\n",
    "            X_train = self.X_train\n",
    "            Y_train = self.Y_train\n",
    "        else:\n",
    "            if (X_base is not None) and (Y_base is not None):\n",
    "                X_train = X_base\n",
    "                Y_train = Y_base\n",
    "            else:\n",
    "                X_train = self.X_train\n",
    "                Y_train = self.Y_train\n",
    "            \n",
    "            assert len(X_test) == len(X_train)\n",
    "            assert len(X_train) == len(Y_train)\n",
    "            \n",
    "            Ia_test = np.arange(len(X_train))\n",
    "        \n",
    "        # N(0,1)-N(0,1) ~ N(0,2) -> divide by sqrt(2) s.t. all features are N(0,1)\n",
    "        X_test = np.concatenate([\n",
    "            X_train[Ia_test], (X_test-X_train[Ia_test]) / np.sqrt(2.0),\n",
    "        ], axis=1)\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(\" - Generating confidence scores\")\n",
    "            \n",
    "        confidence = 1.0 - np.searchsorted(\n",
    "            self.err_valid,\n",
    "            self.cov.mahalanobis((self._predict_truncated_pca(X_test) - X_test)),\n",
    "        ) / len(self.err_valid)\n",
    "            \n",
    "        if verbose > 1:\n",
    "            print(\" - Generating ensemble predictions\")\n",
    "        \n",
    "        def tree_predict(i: int) -> np.ndarray:\n",
    "            if verbose > 1:\n",
    "                print(f\"   - Predicting tree {i}/{len(self.predictor.estimators_)}\")\n",
    "            \n",
    "            return self.predictor.estimators_[i].predict(X_test)\n",
    "        \n",
    "        predictions = joblib.Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "            joblib.delayed(tree_predict)(i) for i in range(len(self.predictor.estimators_))\n",
    "        )\n",
    "        \n",
    "        prediction = np.mean(np.stack(predictions, axis=0), axis=0).reshape((len(X_test), 1))\n",
    "        uncertainty = np.std(np.stack(predictions, axis=0), axis=0).reshape((len(X_test), 1))\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(\" - Recalibrating predictions and uncertainties\")\n",
    "        \n",
    "        prediction = Y_train[Ia_test] + (\n",
    "            prediction.flatten() + self.Zc_mean * uncertainty.flatten()\n",
    "        ).reshape((len(X_test), 1))\n",
    "        uncertainty = (uncertainty.flatten() * self.Zc_stdv).reshape((len(X_test), 1))\n",
    "        \n",
    "        if verbose > 1:\n",
    "            print(\" - Finished predicting with the PercentilePairwiseDifferenceRegressionRandomForestSosaaRSM\")\n",
    "            \n",
    "        return IcarusPrediction(\n",
    "            prediction=prediction, uncertainty=uncertainty, confidence=confidence,\n",
    "        )\n",
    "\n",
    "    def _predict_truncated_pca(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.pca.mean_ is not None:\n",
    "            X = X - self.pca.mean_\n",
    "        \n",
    "        X_trans = np.dot(X, self.pca.components_[:self.bn].T)\n",
    "        X = np.dot(X_trans, self.pca.components_[:self.bn])\n",
    "        \n",
    "        if self.pca.mean_ is not None:\n",
    "            X = X + self.pca.mean_\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_cache_model(dt: datetime.datetime, clump: float, datasets: dict, models: dict, cls, **kwargs):\n",
    "    if isinstance(dt, tuple) or isinstance(dt, list):\n",
    "        dt = tuple(sorted(dt))\n",
    "    \n",
    "    model_key = (cls.__name__, dt, clump)\n",
    "    \n",
    "    cached = models.get(model_key)\n",
    "    \n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    \n",
    "    model_path = f\"{cls.__name__.lower()}.icarus.{hash_for_dt(dt).hexdigest(8)}.{clump}.jl\"\n",
    "    \n",
    "    if Path(model_path).exists():\n",
    "        try:\n",
    "            model = joblib.load(model_path)\n",
    "            \n",
    "            models[model_key] = model\n",
    "        \n",
    "            return model\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    dataset = load_and_cache_dataset(dt, clump, datasets)\n",
    "    \n",
    "    rng = np.random.RandomState(seed=int.from_bytes(hash_for_dt(dt).digest(4), 'little'))\n",
    "    \n",
    "    model = cls().fit(\n",
    "        X_train=dataset.X_train, Y_train=dataset.Y_train,\n",
    "        X_valid=dataset.X_valid, Y_valid=dataset.Y_valid,\n",
    "        rng=rng, **kwargs,\n",
    "    )\n",
    "    \n",
    "    joblib.dump(model, model_path)\n",
    "    \n",
    "    models[model_key] = model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4d717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:14.286024Z",
     "iopub.status.busy": "2023-01-25T10:01:14.285877Z",
     "iopub.status.idle": "2023-01-25T10:01:20.236226Z",
     "shell.execute_reply": "2023-01-25T10:01:20.235557Z"
    },
    "papermill": {
     "duration": 5.988419,
     "end_time": "2023-01-25T10:01:20.238460",
     "exception": false,
     "start_time": "2023-01-25T10:01:14.250041",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_FOREST_MODELS = dict()\n",
    "\n",
    "for h in tqdm.tqdm([\n",
    "    130, 163, 192, 244, 303, 349,\n",
    "]):\n",
    "    if isinstance(h, tuple) or isinstance(h, list):\n",
    "        dt = tuple(\n",
    "            datetime.datetime(\n",
    "                year=2018, month=5, day=9+hs//24, hour=hs%24,\n",
    "            ) for hs in sorted(h)\n",
    "        )\n",
    "        \n",
    "        train_and_cache_model(dt, 0.0, DATASETS, RANDOM_FOREST_MODELS, RandomForestSosaaRSM)\n",
    "    else:\n",
    "        dt = datetime.datetime(\n",
    "            year=2018, month=5, day=9+h//24, hour=h%24,\n",
    "        )\n",
    "    \n",
    "        for clump in tqdm.tqdm([0.0, 0.5, 0.75, 0.85, 0.9]):\n",
    "            train_and_cache_model(dt, clump, DATASETS, RANDOM_FOREST_MODELS, RandomForestSosaaRSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b975c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:20.286356Z",
     "iopub.status.busy": "2023-01-25T10:01:20.286087Z",
     "iopub.status.idle": "2023-01-25T10:01:43.263976Z",
     "shell.execute_reply": "2023-01-25T10:01:43.263366Z"
    },
    "papermill": {
     "duration": 23.002236,
     "end_time": "2023-01-25T10:01:43.265896",
     "exception": false,
     "start_time": "2023-01-25T10:01:20.263660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PAIRWISE_RANDOM_FOREST_MODELS = dict()\n",
    "\n",
    "for h in tqdm.tqdm([\n",
    "    130, 163, 192, 244, 303, 349,\n",
    "]):\n",
    "    if isinstance(h, tuple) or isinstance(h, list):\n",
    "        dt = tuple(\n",
    "            datetime.datetime(\n",
    "                year=2018, month=5, day=9+hs//24, hour=hs%24,\n",
    "            ) for hs in sorted(h)\n",
    "        )\n",
    "        \n",
    "        train_and_cache_model(\n",
    "            dt, 0.0, DATASETS, PAIRWISE_RANDOM_FOREST_MODELS,\n",
    "            PairwiseDifferenceRegressionRandomForestSosaaRSM,\n",
    "        )\n",
    "    else:\n",
    "        dt = datetime.datetime(\n",
    "            year=2018, month=5, day=9+h//24, hour=h%24,\n",
    "        )\n",
    "    \n",
    "        for clump in tqdm.tqdm([0.0, 0.5, 0.75, 0.85, 0.9]):\n",
    "            train_and_cache_model(\n",
    "                dt, clump, DATASETS, PAIRWISE_RANDOM_FOREST_MODELS,\n",
    "                PairwiseDifferenceRegressionRandomForestSosaaRSM,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a18ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTILE_PAIRWISE_RANDOM_FOREST_MODELS = dict()\n",
    "\n",
    "for h in tqdm.tqdm([\n",
    "    130, 163, 192, 244, 303, 349,\n",
    "]):\n",
    "    if isinstance(h, tuple) or isinstance(h, list):\n",
    "        dt = tuple(\n",
    "            datetime.datetime(\n",
    "                year=2018, month=5, day=9+hs//24, hour=hs%24,\n",
    "            ) for hs in sorted(h)\n",
    "        )\n",
    "        \n",
    "        train_and_cache_model(\n",
    "            dt, 0.0, DATASETS, PERCENTILE_PAIRWISE_RANDOM_FOREST_MODELS,\n",
    "            PercentilePairwiseDifferenceRegressionRandomForestSosaaRSM,\n",
    "        )\n",
    "    else:\n",
    "        dt = datetime.datetime(\n",
    "            year=2018, month=5, day=9+h//24, hour=h%24,\n",
    "        )\n",
    "    \n",
    "        for clump in tqdm.tqdm([0.75]):\n",
    "            train_and_cache_model(\n",
    "                dt, clump, DATASETS, PERCENTILE_PAIRWISE_RANDOM_FOREST_MODELS,\n",
    "                PercentilePairwiseDifferenceRegressionRandomForestSosaaRSM,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ea7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, filepath, keys=[]):\n",
    "        self.filepath = filepath\n",
    "        \n",
    "        self.cols = { k: [] for k in keys }\n",
    "    \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def insert(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            self.cols[k].append(v)\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if exc_val is None:\n",
    "            df = pd.DataFrame(self.cols)\n",
    "            df.to_csv(self.filepath, header=True, index=False)\n",
    "        \n",
    "        return False\n",
    "            \n",
    "    def backup(self):\n",
    "        df = pd.DataFrame(self.cols)\n",
    "        df.to_csv(self.filepath, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98562a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_icarus_predictions(\n",
    "    predictions: List[IcarusPrediction],\n",
    "    analysis: Callable[[List[np.ndarray], np.ndarray, np.random.Generator, dict], np.ndarray],\n",
    "    rng: np.random.Generator,\n",
    "    n_uncertain_samples: int = 1,  # number of samples to draw from expand each prediction per run\n",
    "    n_analysis_runs: int = 100,  # number of repeats of the analysis to gather uncertainty\n",
    "    **kwargs,\n",
    "):\n",
    "    assert len(predictions) > 0\n",
    "    \n",
    "    # predictions that need to coexist multiply their confidence\n",
    "    prod_confidence = np.prod([p.confidence for p in predictions], axis=0)\n",
    "    # independent predictions average their confidence\n",
    "    confidence = np.mean(prod_confidence)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _ in range(n_analysis_runs):\n",
    "        confs = []\n",
    "        preds = [[] for _ in predictions]\n",
    "        for _ in range(n_uncertain_samples):\n",
    "            I_conf = (\n",
    "                rng.random(size=prod_confidence.shape) <= prod_confidence\n",
    "            )\n",
    "            (I_conf,) = np.nonzero(I_conf)\n",
    "\n",
    "            confs.append(I_conf)\n",
    "            \n",
    "            for i, p in enumerate(predictions):\n",
    "                preds[i].append(\n",
    "                    rng.normal(\n",
    "                        loc=p.prediction[I_conf],\n",
    "                        scale=p.uncertainty[I_conf],\n",
    "                    )\n",
    "                )\n",
    "        confs = np.concatenate(confs, axis=0)\n",
    "        preds = [\n",
    "            np.concatenate(p, axis=0) for p in preds\n",
    "        ]\n",
    "\n",
    "        results.append(analysis(preds, confs, rng, **kwargs))\n",
    "\n",
    "    prediction = np.mean(np.stack(results, axis=0), axis=0)\n",
    "    uncertainty = np.std(np.stack(results, axis=0), axis=0)\n",
    "\n",
    "    return IcarusPrediction(\n",
    "        prediction=prediction,\n",
    "        uncertainty=uncertainty,\n",
    "        confidence=confidence,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39196164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_many_icarus_predictions(\n",
    "    model: IcarusRSM,\n",
    "    X_test: np.ndarray,\n",
    "    rng: np.random.Generator,\n",
    "    n_samples: int,\n",
    "    n_uncertain_samples: int, # number of samples to draw from expand each prediction per run\n",
    "    n_analysis_runs: int, # number of repeats of the analysis to gather uncertainty\n",
    "    **kwargs,\n",
    ") -> IcarusPrediction:\n",
    "    model_predictions = []\n",
    "    for i in range(n_samples):\n",
    "        model_predictions.append(model.predict(X_test, rng, **kwargs))\n",
    "\n",
    "    combined_predictions = IcarusPrediction(\n",
    "        prediction=[],\n",
    "        uncertainty=[],\n",
    "        confidence=[],\n",
    "    )\n",
    "\n",
    "    if len(model_predictions) > 0:\n",
    "        for i in range(len(model_predictions[0].prediction)):\n",
    "            predictions = np.array([p.prediction[i] for p in model_predictions])\n",
    "            uncertainties = np.array([p.uncertainty[i] for p in model_predictions])\n",
    "            confidences = np.array([p.confidence[i] for p in model_predictions])\n",
    "\n",
    "            def combine_predictions(Y_pred, I_pred, rng, **kwargs):\n",
    "                Y_pred, = Y_pred\n",
    "                \n",
    "                return np.mean(Y_pred) if len(Y_pred) > 0 else 0.0\n",
    "\n",
    "            cp = analyse_icarus_predictions(\n",
    "                [IcarusPrediction(\n",
    "                    prediction=predictions,\n",
    "                    uncertainty=uncertainties,\n",
    "                    confidence=confidences,\n",
    "                )],\n",
    "                combine_predictions,\n",
    "                rng,\n",
    "                n_uncertain_samples=n_uncertain_samples,\n",
    "                n_analysis_runs=n_analysis_runs,\n",
    "            )\n",
    "\n",
    "            combined_predictions.prediction.append(cp.prediction)\n",
    "            combined_predictions.uncertainty.append(cp.uncertainty)\n",
    "            combined_predictions.confidence.append(cp.confidence)\n",
    "\n",
    "    return IcarusPrediction(\n",
    "        prediction=np.array(combined_predictions.prediction).reshape(-1, 1),\n",
    "        uncertainty=np.array(combined_predictions.uncertainty).reshape(-1, 1),\n",
    "        confidence=np.array(combined_predictions.confidence),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a784544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:43.321732Z",
     "iopub.status.busy": "2023-01-25T10:01:43.321472Z",
     "iopub.status.idle": "2023-01-25T10:01:43.342456Z",
     "shell.execute_reply": "2023-01-25T10:01:43.342003Z"
    },
    "papermill": {
     "duration": 0.048894,
     "end_time": "2023-01-25T10:01:43.343304",
     "exception": false,
     "start_time": "2023-01-25T10:01:43.294410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model_matrix(datasets, models, n_samples, title=None, **kwargs):\n",
    "    rng = np.random.RandomState(seed=42)\n",
    "    \n",
    "    with Table(\n",
    "        filepath=f\"{title.lower().replace(' ', '-')}.csv\",\n",
    "        keys=[\n",
    "            \"model_date\", \"model_clump\", \"data_date\", \"data_clump\", \"mse\", \"mse_stdv\",\n",
    "            \"mse_conf\", \"mae\", \"mae_stdv\", \"mae_conf\", \"r2\", \"r2_stdv\", \"r2_conf\",\n",
    "        ],\n",
    "    ) as table:\n",
    "        for key in models.keys():\n",
    "            cls, dt, clump = key\n",
    "            \n",
    "            model = models[key]\n",
    "            mlm = load_and_cache_dataset(dt, clump, datasets)\n",
    "\n",
    "            for key2 in tqdm.tqdm(models.keys()):\n",
    "                cls2, dt2, clump2 = key2\n",
    "                \n",
    "                mlt = load_and_cache_dataset(dt2, clump2, datasets)\n",
    "                X_test = mlm.X_scaler.transform(\n",
    "                    mlt.X_scaler.inverse_transform(mlt.X_test, copy=True)\n",
    "                )\n",
    "\n",
    "                Y_test = mlt.Y_scaler.inverse_transform(mlt.Y_test)\n",
    "                Y_pred = combine_many_icarus_predictions(\n",
    "                    model,\n",
    "                    X_test,\n",
    "                    rng,\n",
    "                    n_samples,\n",
    "                    10, # n_uncertain_samples\n",
    "                    100, # n_analysis_runs\n",
    "                    **kwargs,\n",
    "                )\n",
    "                \n",
    "                Y_pred = IcarusPrediction(\n",
    "                    prediction=mlm.Y_scaler.inverse_transform(Y_pred.prediction),\n",
    "                    uncertainty=Y_pred.uncertainty * mlm.Y_scaler.scale_,\n",
    "                    confidence=Y_pred.confidence,\n",
    "                )\n",
    "                \n",
    "                def mse_mae_r2_analysis(Y_pred, I_pred, rng, **kwargs):\n",
    "                    Y_pred, = Y_pred\n",
    "                    Y_true = Y_test[I_pred]\n",
    "\n",
    "                    if len(Y_pred) >= 1:\n",
    "                        mse = mean_squared_error(Y_true, Y_pred)\n",
    "                        mae = mean_absolute_error(Y_true, Y_pred)\n",
    "                    else:\n",
    "                        mse = 0.0\n",
    "                        mae = 0.0\n",
    "                    \n",
    "                    if len(Y_pred) >= 2:\n",
    "                        r2 = r2_score(Y_true, Y_pred)\n",
    "                    else:\n",
    "                        r2 = 1.0\n",
    "\n",
    "                    return np.array([mse, mae, r2])\n",
    "                \n",
    "                mse_mae_r2 = analyse_icarus_predictions(\n",
    "                    [Y_pred],\n",
    "                    mse_mae_r2_analysis,\n",
    "                    rng,\n",
    "                    n_uncertain_samples=1,\n",
    "                    n_analysis_runs=10,\n",
    "                )\n",
    "                \n",
    "                print(mse_mae_r2)\n",
    "                \n",
    "                table.insert(\n",
    "                    model_date=dt, model_clump=clump, data_date=dt2, data_clump=clump2,\n",
    "                    mse=mse_mae_r2.prediction[0],\n",
    "                    mse_stdv=mse_mae_r2.uncertainty[0],\n",
    "                    mse_conf=mse_mae_r2.confidence,\n",
    "                    mae=mse_mae_r2.prediction[1],\n",
    "                    mae_stdv=mse_mae_r2.uncertainty[1],\n",
    "                    mae_conf=mse_mae_r2.confidence,\n",
    "                    r2=mse_mae_r2.prediction[2],\n",
    "                    r2_stdv=mse_mae_r2.uncertainty[2],\n",
    "                    r2_conf=mse_mae_r2.confidence,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf8f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:43.585977Z",
     "iopub.status.busy": "2023-01-25T10:01:43.585828Z",
     "iopub.status.idle": "2023-01-25T10:03:14.657443Z",
     "shell.execute_reply": "2023-01-25T10:03:14.656823Z"
    },
    "papermill": {
     "duration": 91.095792,
     "end_time": "2023-01-25T10:03:14.658781",
     "exception": false,
     "start_time": "2023-01-25T10:01:43.562989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_model_matrix(DATASETS, {\n",
    "    k: v for k, v in RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, 1, title=\"Trajectory Generalisation RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d4436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_matrix(DATASETS, {\n",
    "    k: v for k, v in PAIRWISE_RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, 4, title=\"Trajectory Generalisation PADRE-RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a59e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:43.390614Z",
     "iopub.status.busy": "2023-01-25T10:01:43.390388Z",
     "iopub.status.idle": "2023-01-25T10:01:43.407718Z",
     "shell.execute_reply": "2023-01-25T10:01:43.407253Z"
    },
    "papermill": {
     "duration": 0.041451,
     "end_time": "2023-01-25T10:01:43.408625",
     "exception": false,
     "start_time": "2023-01-25T10:01:43.367174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model_neighbourhood_matrix(datasets, models, all_models, n_samples, title=None, **kwargs):\n",
    "    offsets = [-4, -2, -1, 0, 1, 2, 4]\n",
    "    \n",
    "    rng = np.random.RandomState(seed=42)\n",
    "    \n",
    "    with Table(\n",
    "        filepath=f\"{title.lower().replace(' ', '-')}.csv\",\n",
    "        keys=[\n",
    "            \"model_date\", \"model_clump\", \"data_hour_offset\", \"data_clump\",\n",
    "            \"mse\", \"mse_stdv\", \"mse_conf\", \"mae\", \"mae_stdv\", \"mae_conf\",\n",
    "            \"r2\", \"r2_stdv\", \"r2_conf\",\n",
    "        ],\n",
    "    ) as table:\n",
    "        for key in models.keys():\n",
    "            cls, dt, clump = key\n",
    "\n",
    "            model = all_models[key]\n",
    "            mlm = load_and_cache_dataset(dt, clump, datasets)\n",
    "\n",
    "            for i in offsets:\n",
    "                mlt = load_and_cache_dataset(dt + datetime.timedelta(hours=i), clump, datasets)\n",
    "                \n",
    "                X_test = mlm.X_scaler.transform(\n",
    "                    mlt.X_scaler.inverse_transform(mlt.X_test, copy=True)\n",
    "                )\n",
    "\n",
    "                Y_test = mlt.Y_scaler.inverse_transform(mlt.Y_test)\n",
    "                Y_pred = combine_many_icarus_predictions(\n",
    "                    model,\n",
    "                    X_test,\n",
    "                    rng,\n",
    "                    n_samples,\n",
    "                    10, # n_uncertain_samples\n",
    "                    100, # n_analysis_runs\n",
    "                    **kwargs,\n",
    "                )\n",
    "                \n",
    "                Y_pred = IcarusPrediction(\n",
    "                    prediction=mlm.Y_scaler.inverse_transform(Y_pred.prediction),\n",
    "                    uncertainty=Y_pred.uncertainty * mlm.Y_scaler.scale_,\n",
    "                    confidence=Y_pred.confidence,\n",
    "                )\n",
    "                \n",
    "                def mse_mae_r2_analysis(Y_pred, I_pred, rng, **kwargs):\n",
    "                    Y_pred, = Y_pred\n",
    "                    Y_true = Y_test[I_pred]\n",
    "\n",
    "                    if len(Y_pred) >= 1:\n",
    "                        mse = mean_squared_error(Y_true, Y_pred)\n",
    "                        mae = mean_absolute_error(Y_true, Y_pred)\n",
    "                    else:\n",
    "                        mse = 0.0\n",
    "                        mae = 0.0\n",
    "                    \n",
    "                    if len(Y_pred) >= 2:\n",
    "                        r2 = r2_score(Y_true, Y_pred)\n",
    "                    else:\n",
    "                        r2 = 1.0\n",
    "\n",
    "                    return np.array([mse, mae, r2])\n",
    "                \n",
    "                mse_mae_r2 = analyse_icarus_predictions(\n",
    "                    [Y_pred],\n",
    "                    mse_mae_r2_analysis,\n",
    "                    rng,\n",
    "                    n_uncertain_samples=1,\n",
    "                    n_analysis_runs=10,\n",
    "                )\n",
    "                \n",
    "                print(mse_mae_r2)\n",
    "                \n",
    "                table.insert(\n",
    "                    model_date=dt, model_clump=clump, data_hour_offset=i, data_clump=clump,\n",
    "                    mse=mse_mae_r2.prediction[0],\n",
    "                    mse_stdv=mse_mae_r2.uncertainty[0],\n",
    "                    mse_conf=mse_mae_r2.confidence,\n",
    "                    mae=mse_mae_r2.prediction[1],\n",
    "                    mae_stdv=mse_mae_r2.uncertainty[1],\n",
    "                    mae_conf=mse_mae_r2.confidence,\n",
    "                    r2=mse_mae_r2.prediction[2],\n",
    "                    r2_stdv=mse_mae_r2.uncertainty[2],\n",
    "                    r2_conf=mse_mae_r2.confidence,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725eeca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:20:15.130780Z",
     "iopub.status.busy": "2023-01-25T10:20:15.130482Z",
     "iopub.status.idle": "2023-01-25T10:23:27.500378Z",
     "shell.execute_reply": "2023-01-25T10:23:27.499750Z"
    },
    "papermill": {
     "duration": 192.442166,
     "end_time": "2023-01-25T10:23:27.502195",
     "exception": false,
     "start_time": "2023-01-25T10:20:15.060029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_model_neighbourhood_matrix(DATASETS, {\n",
    "    k: v for k, v in RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, RANDOM_FOREST_MODELS, 1, title=\"Temporal Generalisation RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51c2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_neighbourhood_matrix(DATASETS, {\n",
    "    k: v for k, v in PAIRWISE_RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, PAIRWISE_RANDOM_FOREST_MODELS, 4, title=\"Temporal Generalisation PADRE-RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d01f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:43.455343Z",
     "iopub.status.busy": "2023-01-25T10:01:43.455202Z",
     "iopub.status.idle": "2023-01-25T10:01:43.471040Z",
     "shell.execute_reply": "2023-01-25T10:01:43.470581Z"
    },
    "papermill": {
     "duration": 0.03957,
     "end_time": "2023-01-25T10:01:43.471952",
     "exception": false,
     "start_time": "2023-01-25T10:01:43.432382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model_clump_matrix(datasets, models, all_models, n_samples, title=None, **kwargs):\n",
    "    clumps = [0.0, 0.5, 0.75, 0.85, 0.9]\n",
    "    \n",
    "    rng = np.random.RandomState(seed=42)\n",
    "    \n",
    "    with Table(\n",
    "        filepath=f\"{title.lower().replace(' ', '-')}.csv\",\n",
    "        keys=[\n",
    "            \"model_date\", \"model_clump\", \"mse\", \"mse_stdv\", \"mse_conf\",\n",
    "            \"mae\", \"mae_stdv\", \"mae_conf\", \"r2\", \"r2_stdv\", \"r2_conf\",\n",
    "        ],\n",
    "    ) as table:\n",
    "        for key in models.keys():\n",
    "            cls, dt, _ = key\n",
    "\n",
    "            for c in clumps:\n",
    "                model = all_models[(cls, dt, c)]\n",
    "                mlm = load_and_cache_dataset(dt, c, datasets)\n",
    "                \n",
    "                X_test = np.copy(mlm.X_test)\n",
    "\n",
    "                Y_test = mlm.Y_scaler.inverse_transform(mlm.Y_test)\n",
    "                Y_pred = combine_many_icarus_predictions(\n",
    "                    model,\n",
    "                    X_test,\n",
    "                    rng,\n",
    "                    n_samples,\n",
    "                    10, # n_uncertain_samples\n",
    "                    100, # n_analysis_runs\n",
    "                    **kwargs,\n",
    "                )\n",
    "                \n",
    "                Y_pred = IcarusPrediction(\n",
    "                    prediction=mlm.Y_scaler.inverse_transform(Y_pred.prediction),\n",
    "                    uncertainty=Y_pred.uncertainty * mlm.Y_scaler.scale_,\n",
    "                    confidence=Y_pred.confidence,\n",
    "                )\n",
    "                \n",
    "                def mse_mae_r2_analysis(Y_pred, I_pred, rng, **kwargs):\n",
    "                    Y_pred, = Y_pred\n",
    "                    Y_true = Y_test[I_pred]\n",
    "\n",
    "                    if len(Y_pred) >= 1:\n",
    "                        mse = mean_squared_error(Y_true, Y_pred)\n",
    "                        mae = mean_absolute_error(Y_true, Y_pred)\n",
    "                    else:\n",
    "                        mse = 0.0\n",
    "                        mae = 0.0\n",
    "                    \n",
    "                    if len(Y_pred) >= 2:\n",
    "                        r2 = r2_score(Y_true, Y_pred)\n",
    "                    else:\n",
    "                        r2 = 1.0\n",
    "\n",
    "                    return np.array([mse, mae, r2])\n",
    "                \n",
    "                mse_mae_r2 = analyse_icarus_predictions(\n",
    "                    [Y_pred],\n",
    "                    mse_mae_r2_analysis,\n",
    "                    rng,\n",
    "                    n_uncertain_samples=1,\n",
    "                    n_analysis_runs=10,\n",
    "                )\n",
    "                \n",
    "                print(mse_mae_r2)\n",
    "                \n",
    "                table.insert(\n",
    "                    model_date=dt, model_clump=c,\n",
    "                    mse=mse_mae_r2.prediction[0],\n",
    "                    mse_stdv=mse_mae_r2.uncertainty[0],\n",
    "                    mse_conf=mse_mae_r2.confidence,\n",
    "                    mae=mse_mae_r2.prediction[1],\n",
    "                    mae_stdv=mse_mae_r2.uncertainty[1],\n",
    "                    mae_conf=mse_mae_r2.confidence,\n",
    "                    r2=mse_mae_r2.prediction[2],\n",
    "                    r2_stdv=mse_mae_r2.uncertainty[2],\n",
    "                    r2_conf=mse_mae_r2.confidence,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c180b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:44:20.584084Z",
     "iopub.status.busy": "2023-01-25T10:44:20.583796Z",
     "iopub.status.idle": "2023-01-25T10:46:09.166543Z",
     "shell.execute_reply": "2023-01-25T10:46:09.165886Z"
    },
    "papermill": {
     "duration": 108.729572,
     "end_time": "2023-01-25T10:46:09.167809",
     "exception": false,
     "start_time": "2023-01-25T10:44:20.438237",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_model_clump_matrix(DATASETS, {\n",
    "    k: v for k, v in RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, RANDOM_FOREST_MODELS, 1, title=\"Clumped Generalisation RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24711ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_clump_matrix(DATASETS, {\n",
    "    k: v for k, v in PAIRWISE_RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, PAIRWISE_RANDOM_FOREST_MODELS, 4, title=\"Clumped Generalisation PADRE-RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb8a87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T10:01:43.518851Z",
     "iopub.status.busy": "2023-01-25T10:01:43.518710Z",
     "iopub.status.idle": "2023-01-25T10:01:43.538230Z",
     "shell.execute_reply": "2023-01-25T10:01:43.537774Z"
    },
    "papermill": {
     "duration": 0.043438,
     "end_time": "2023-01-25T10:01:43.539064",
     "exception": false,
     "start_time": "2023-01-25T10:01:43.495626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model_perturbation_matrix(\n",
    "    datasets, perturbed_datasets, models, n_samples, title=None, direct_difference=False, **kwargs,\n",
    "):\n",
    "    perturbations = [\n",
    "        Path(\"anthropogenic\")/ \"mul_1.5\",\n",
    "        Path(\"anthropogenic\")/ \"div_1.5\",\n",
    "        \n",
    "        Path(\"biogenic\")/ \"mul_1.5\",\n",
    "        Path(\"biogenic\")/ \"div_1.5\",\n",
    "        \n",
    "        Path(\"aerosols\")/ \"mul_1.5\",\n",
    "        Path(\"aerosols\")/ \"div_1.5\",\n",
    "        \n",
    "        Path(\"monoterpenes\")/ \"mul_1.5\",\n",
    "        Path(\"monoterpenes\")/ \"div_1.5\",\n",
    "        \n",
    "        Path(\"sesquiterpenes\")/ \"mul_1.5\",\n",
    "        Path(\"sesquiterpenes\")/ \"div_1.5\",\n",
    "        \n",
    "        Path(\"so2\")/ \"mul_1.5\",\n",
    "        Path(\"so2\")/ \"div_1.5\",\n",
    "        \n",
    "        Path(\"nox\")/ \"mul_1.5\",\n",
    "        Path(\"nox\")/ \"div_1.5\",\n",
    "        \n",
    "        Path(\"temperature\")/ \"add_2K\",\n",
    "        Path(\"temperature\")/ \"sub_2K\",\n",
    "        \n",
    "        Path(\"anthropogenic\")/ \"mul_1.01\",\n",
    "        Path(\"anthropogenic\")/ \"div_1.01\",\n",
    "        \n",
    "        Path(\"biogenic\")/ \"mul_1.01\",\n",
    "        Path(\"biogenic\")/ \"div_1.01\",\n",
    "        \n",
    "        Path(\"aerosols\")/ \"mul_1.01\",\n",
    "        Path(\"aerosols\")/ \"div_1.01\",\n",
    "        \n",
    "        Path(\"monoterpenes\")/ \"mul_1.01\",\n",
    "        Path(\"monoterpenes\")/ \"div_1.01\",\n",
    "        \n",
    "        Path(\"sesquiterpenes\")/ \"mul_1.01\",\n",
    "        Path(\"sesquiterpenes\")/ \"div_1.01\",\n",
    "        \n",
    "        Path(\"so2\")/ \"mul_1.01\",\n",
    "        Path(\"so2\")/ \"div_1.01\",\n",
    "        \n",
    "        Path(\"nox\")/ \"mul_1.01\",\n",
    "        Path(\"nox\")/ \"div_1.01\",\n",
    "        \n",
    "        Path(\"temperature\")/ \"add_0.04K\",\n",
    "        Path(\"temperature\")/ \"sub_0.04K\",\n",
    "    ]\n",
    "    \n",
    "    rng = np.random.RandomState(seed=42)\n",
    "    \n",
    "    for (cls, dt, clump) in models.keys():\n",
    "        for perturbation in tqdm.tqdm(perturbations):\n",
    "            if perturbed_datasets.get((dt, perturbation)) is not None:\n",
    "                continue\n",
    "            \n",
    "            dp = get_path_for_perturbation(dt, Path(\"perturbation\") / perturbation)\n",
    "            ds = load_trajectory_dataset(dp)\n",
    "\n",
    "            X_raw = get_raw_features_for_dataset(ds)\n",
    "\n",
    "            X = get_features_from_raw_features(X_raw)\n",
    "            Y = np.log10(get_labels_for_dataset(ds) + 1)\n",
    "\n",
    "            # Close the NetCDF datasets\n",
    "            ds.out.close()\n",
    "            ds.aer.close()\n",
    "            ds.ant.close()\n",
    "            ds.bio.close()\n",
    "            ds.met.close()\n",
    "            \n",
    "            perturbed_datasets[(dt, perturbation)] = PerturbedDataset(\n",
    "                dt, perturbation, dp, X, Y,\n",
    "            )\n",
    "    \n",
    "    with Table(\n",
    "        filepath=f\"{title.lower().replace(' ', '-')}.csv\",\n",
    "        keys=[\n",
    "            \"model_date\", \"model_clump\", \"perturbation\", \"mse\", \"mse_stdv\",\n",
    "            \"mse_conf\", \"mae\", \"mae_stdv\", \"mae_conf\", \"r2\", \"r2_stdv\", \"r2_conf\",\n",
    "        ],\n",
    "    ) as table:\n",
    "        for key in models.keys():\n",
    "            cls, dt, clump = key\n",
    "            \n",
    "            model = models[key]\n",
    "            mlm = load_and_cache_dataset(dt, clump, datasets)\n",
    "\n",
    "            X_raw = get_features_from_raw_features(mlm.X_raw.copy())\n",
    "            X_base = np.nan_to_num(mlm.X_scaler.transform(X_raw))\n",
    "            Y_base = mlm.Y_raw.to_numpy()\n",
    "            \n",
    "            if not direct_difference:\n",
    "                Y_unpt = combine_many_icarus_predictions(\n",
    "                    model,\n",
    "                    np.copy(X_base),\n",
    "                    rng,\n",
    "                    n_samples,\n",
    "                    10, # n_uncertain_samples\n",
    "                    100, # n_analysis_runs\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "                Y_unpt = IcarusPrediction(\n",
    "                    prediction=mlm.Y_scaler.inverse_transform(Y_unpt.prediction),\n",
    "                    uncertainty=Y_unpt.uncertainty * mlm.Y_scaler.scale_,\n",
    "                    confidence=Y_unpt.confidence,\n",
    "                )\n",
    "\n",
    "            for p in tqdm.tqdm(perturbations):\n",
    "                dsp = perturbed_datasets[(dt, p)]\n",
    "\n",
    "                X_prtb = np.nan_to_num(mlm.X_scaler.transform(dsp.X, copy=True))\n",
    "                Y_test = dsp.Y.to_numpy()\n",
    "                \n",
    "                if direct_difference:\n",
    "                    kwargs[\"X_base\"] = X_base\n",
    "                    kwargs[\"Y_base\"] = mlm.Y_scaler.transform(mlm.Y_raw)\n",
    "                \n",
    "                Y_prtb = combine_many_icarus_predictions(\n",
    "                    model,\n",
    "                    X_prtb,\n",
    "                    rng,\n",
    "                    n_samples,\n",
    "                    10, # n_uncertain_samples\n",
    "                    100, # n_analysis_runs\n",
    "                    **kwargs,\n",
    "                )\n",
    "                \n",
    "                Y_prtb = IcarusPrediction(\n",
    "                    prediction=mlm.Y_scaler.inverse_transform(Y_prtb.prediction),\n",
    "                    uncertainty=Y_prtb.uncertainty * mlm.Y_scaler.scale_,\n",
    "                    confidence=Y_prtb.confidence,\n",
    "                )\n",
    "                \n",
    "                def diff_mse_mae_r2_analysis(Y_true, Y_preds, I_pred, rng, **kwargs):\n",
    "                    Y_unpt, Y_prtb = Y_preds\n",
    "                    Y_base, Y_test = Y_true\n",
    "                    Y_base = Y_base[I_pred]\n",
    "                    Y_test = Y_test[I_pred]\n",
    "\n",
    "                    if len(Y_prtb) >= 1:\n",
    "                        mse = mean_squared_error(Y_test-Y_base, Y_prtb-Y_unpt)\n",
    "                        mae = mean_absolute_error(Y_test-Y_base, Y_prtb-Y_unpt)\n",
    "                    else:\n",
    "                        mse = 0.0\n",
    "                        mae = 0.0\n",
    "                    \n",
    "                    if len(Y_prtb) >= 2:\n",
    "                        r2 = r2_score(Y_test-Y_base, Y_prtb-Y_unpt)\n",
    "                    else:\n",
    "                        r2 = 1.0\n",
    "                    \n",
    "                    # TODO: remove\n",
    "                    bins = np.linspace(0.0, 15.0, 150)\n",
    "                    binids = np.searchsorted(bins, Y_base.flatten())\n",
    "                    bincnt = np.bincount(binids, minlength=150)\n",
    "                    binsum_true = np.bincount(binids, weights=(Y_test-Y_base).flatten(), minlength=150)\n",
    "                    binsum_pred = np.bincount(binids, weights=(Y_prtb-Y_unpt).flatten(), minlength=150)\n",
    "                    nonzero = bincnt != 0\n",
    "                    binmean_true = binsum_true[nonzero] / bincnt[nonzero]\n",
    "                    binmean_pred = binsum_pred[nonzero] / bincnt[nonzero]\n",
    "                    \n",
    "                    if len(binmean_true) >= 2:\n",
    "                        r2b = r2_score(binmean_true, binmean_pred)\n",
    "                    else:\n",
    "                        r2b = 1.0\n",
    "\n",
    "                    return np.array([mse, mae, r2, r2b])\n",
    "                \n",
    "                def direct_mse_mae_r2_analysis(Y_true, Y_preds, I_pred, rng, **kwargs):\n",
    "                    Y_prtb, = Y_preds\n",
    "                    Y_base, Y_test = Y_true\n",
    "                    Y_base = Y_base[I_pred]\n",
    "                    Y_test = Y_test[I_pred]\n",
    "\n",
    "                    if len(Y_prtb) >= 1:\n",
    "                        mse = mean_squared_error(Y_test-Y_base, Y_prtb-Y_base)\n",
    "                        mae = mean_absolute_error(Y_test-Y_base, Y_prtb-Y_base)\n",
    "                    else:\n",
    "                        mse = 0.0\n",
    "                        mae = 0.0\n",
    "                    \n",
    "                    if len(Y_prtb) >= 2:\n",
    "                        r2 = r2_score(Y_test-Y_base, Y_prtb-Y_base)\n",
    "                    else:\n",
    "                        r2 = 1.0\n",
    "                    \n",
    "                    # TODO: remove\n",
    "                    bins = np.linspace(0.0, 15.0, 150)\n",
    "                    binids = np.searchsorted(bins, Y_base.flatten())\n",
    "                    bincnt = np.bincount(binids, minlength=150)\n",
    "                    binsum_true = np.bincount(binids, weights=(Y_test-Y_base).flatten(), minlength=150)\n",
    "                    binsum_pred = np.bincount(binids, weights=(Y_prtb-Y_base).flatten(), minlength=150)\n",
    "                    nonzero = bincnt != 0\n",
    "                    binmean_true = binsum_true[nonzero] / bincnt[nonzero]\n",
    "                    binmean_pred = binsum_pred[nonzero] / bincnt[nonzero]\n",
    "                    \n",
    "                    if len(binmean_true) >= 2:\n",
    "                        r2b = r2_score(binmean_true, binmean_pred)\n",
    "                    else:\n",
    "                        r2b = 1.0\n",
    "\n",
    "                    return np.array([mse, mae, r2, r2b])\n",
    "                \n",
    "                mse_mae_r2 = analyse_icarus_predictions(\n",
    "                    [Y_prtb] if direct_difference else [Y_unpt, Y_prtb],\n",
    "                    partial(\n",
    "                        direct_mse_mae_r2_analysis if direct_difference else diff_mse_mae_r2_analysis,\n",
    "                        [Y_base, Y_test],\n",
    "                    ),\n",
    "                    rng,\n",
    "                    n_uncertain_samples=1,\n",
    "                    n_analysis_runs=10,\n",
    "                )\n",
    "                \n",
    "                print(direct_difference, mse_mae_r2)\n",
    "                \n",
    "                table.insert(\n",
    "                    model_date=dt, model_clump=clump, perturbation=p,\n",
    "                    mse=mse_mae_r2.prediction[0],\n",
    "                    mse_stdv=mse_mae_r2.uncertainty[0],\n",
    "                    mse_conf=mse_mae_r2.confidence,\n",
    "                    mae=mse_mae_r2.prediction[1],\n",
    "                    mae_stdv=mse_mae_r2.uncertainty[1],\n",
    "                    mae_conf=mse_mae_r2.confidence,\n",
    "                    r2=mse_mae_r2.prediction[2],\n",
    "                    r2_stdv=mse_mae_r2.uncertainty[2],\n",
    "                    r2_conf=mse_mae_r2.confidence,\n",
    "                )\n",
    "            \n",
    "            table.backup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d3772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-25T11:01:03.210643Z",
     "iopub.status.busy": "2023-01-25T11:01:03.210483Z",
     "iopub.status.idle": "2023-01-25T11:09:38.897815Z",
     "shell.execute_reply": "2023-01-25T11:09:38.897297Z"
    },
    "papermill": {
     "duration": 515.851447,
     "end_time": "2023-01-25T11:09:38.899219",
     "exception": false,
     "start_time": "2023-01-25T11:01:03.047772",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_model_perturbation_matrix(DATASETS, PERTURBED_DATASETS, {\n",
    "    k: v for k, v in RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, 1, title=\"Perturbation Generalisation RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_perturbation_matrix(DATASETS, PERTURBED_DATASETS, {\n",
    "    k: v for k, v in PAIRWISE_RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, 4, title=\"Perturbation Generalisation PADRE-RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_perturbation_matrix(DATASETS, PERTURBED_DATASETS, {\n",
    "    k: v for k, v in PERCENTILE_PAIRWISE_RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, 4, title=\"Perturbation Generalisation Percentile PADRE-RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectDifferencePairwiseDifferenceRegressionRandomForestSosaaRSM(IcarusRSM):\n",
    "    def __init__(self, padre_rf: PairwiseDifferenceRegressionRandomForestSosaaRSM):\n",
    "        self.padre_rf = padre_rf\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: np.ndarray, Y_train: np.ndarray,\n",
    "        X_valid: np.ndarray, Y_valid: np.ndarray,\n",
    "        rng: np.random.Generator,\n",
    "        **kwargs,\n",
    "    ) -> DirectDifferencePairwiseDifferenceRegressionRandomForestSosaaRSM:\n",
    "        self.padre_rf.fit(X_train, Y_train, X_valid, Y_valid, rng, **kwargs)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(\n",
    "        self, X_test: np.ndarray, rng: np.random.Generator, **kwargs\n",
    "    ) -> IcarusPrediction:\n",
    "        return self.padre_rf.predict(X_test, rng, direct_difference=True, **kwargs)\n",
    "\n",
    "evaluate_model_perturbation_matrix(DATASETS, PERTURBED_DATASETS, {\n",
    "    k: DirectDifferencePairwiseDifferenceRegressionRandomForestSosaaRSM(\n",
    "        v,\n",
    "    ) for k, v in PAIRWISE_RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, 1, title=\"Perturbation Generalisation PADRE-RF Direct\", direct_difference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_perturbation_matrix(DATASETS, PERTURBED_DATASETS, {\n",
    "    k: DirectDifferencePairwiseDifferenceRegressionRandomForestSosaaRSM(\n",
    "        v,\n",
    "    ) for k, v in PERCENTILE_PAIRWISE_RANDOM_FOREST_MODELS.items() if not isinstance(k[1], tuple) and k[2] == 0.75\n",
    "}, 1, title=\"Perturbation Generalisation Percentile PADRE-RF Direct\", direct_difference=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12737.072597,
   "end_time": "2023-01-25T13:33:11.655593",
   "environment_variables": {},
   "exception": null,
   "input_path": "generalisation.ipynb",
   "output_path": "generalisation.ipynb",
   "parameters": {},
   "start_time": "2023-01-25T10:00:54.582996",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "019031af7d8c48e69022c1cb57b2f9d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "19a7f13716ca4daf81a3c2f5568df16c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dfab174198f44685bc8e11b9912259a2",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f8baa2bab0574d989b93569a13c54266",
       "tabbable": null,
       "tooltip": null,
       "value": 100
      }
     },
     "1b37107810414e828546b148557c66a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2651cb61da234172af0b46e69103ba21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28f81896ed5b4f7ba6346541750a27e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e031ff83c380437c9c22bdd9b62903de",
       "placeholder": "​",
       "style": "IPY_MODEL_3a1ff08eb6694ad484a822b55b79690a",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "2bfbccfa44e34a0e9b839940f1f840f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d2e7e129f644397849f21e014563416": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2f1bf43a34ef41babf9771e9dc9a0fdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "31201e786cb0442f9d5d99eec85afe99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f2d8a2f02c5c435f8632d4f03f35748d",
       "placeholder": "​",
       "style": "IPY_MODEL_9e75d52a8e2a44eb82127a4d8a40cb60",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "3a1ff08eb6694ad484a822b55b79690a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "40c7c519d7f74843abbbeaf9c7355f12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7514425f5ee848c79abb969f383b4e46",
       "placeholder": "​",
       "style": "IPY_MODEL_8b3b4f97b0a84a2fb31812dd44317792",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "414f8ca5df0e43df8f4349e41101af28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "41da05f10df242cc93d09f8417ff55b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "425d25c6f3cf446a8fb287f6f844525f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f69effc1db1a45b89beb8ec695ad323d",
       "placeholder": "​",
       "style": "IPY_MODEL_f08a94be5e7d40adaf14baa99f5d7039",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "466dea72255c4c269f0329f626c8c149": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_41da05f10df242cc93d09f8417ff55b1",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2d2e7e129f644397849f21e014563416",
       "tabbable": null,
       "tooltip": null,
       "value": 100
      }
     },
     "48d1fdfb9fde452288455ecbd3ef8c80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_31201e786cb0442f9d5d99eec85afe99",
        "IPY_MODEL_74e87566b9e543e9be174b313392bed6",
        "IPY_MODEL_e8170b5e76ca43079848afc04fb6dfc7"
       ],
       "layout": "IPY_MODEL_2651cb61da234172af0b46e69103ba21",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5e20360724014f9b8413f64d373e98c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5e98b9da1dd643f491fd33a46a0e43ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "63d75d770b674ccaa2049a10e3c45b0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "678c72d46baa47439867d3fec767c5cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "74e302337398458e97602a413293b495": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_40c7c519d7f74843abbbeaf9c7355f12",
        "IPY_MODEL_466dea72255c4c269f0329f626c8c149",
        "IPY_MODEL_c5a1cd58bace49c68461416790dda952"
       ],
       "layout": "IPY_MODEL_414f8ca5df0e43df8f4349e41101af28",
       "tabbable": null,
       "tooltip": null
      }
     },
     "74e87566b9e543e9be174b313392bed6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2bfbccfa44e34a0e9b839940f1f840f2",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5e98b9da1dd643f491fd33a46a0e43ec",
       "tabbable": null,
       "tooltip": null,
       "value": 100
      }
     },
     "7514425f5ee848c79abb969f383b4e46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7bbbedf3e34d4aec98e0e046c109d508": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_425d25c6f3cf446a8fb287f6f844525f",
        "IPY_MODEL_19a7f13716ca4daf81a3c2f5568df16c",
        "IPY_MODEL_f4a034837d1847cf96d61a320106b3cc"
       ],
       "layout": "IPY_MODEL_dbbb78bea9f340f497eafc29b1fb1f0f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8723cebae357498ea0bbd3cb7c8dd979": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fea27a5789d24ada95ccfd4cdb11147a",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2f1bf43a34ef41babf9771e9dc9a0fdf",
       "tabbable": null,
       "tooltip": null,
       "value": 100
      }
     },
     "8b3b4f97b0a84a2fb31812dd44317792": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9e75d52a8e2a44eb82127a4d8a40cb60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9ed0ca273f14498d855bb1641804db13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab9bcf89cf9b4b789f5f1878c1ea6465": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eee31f717c544b10b30da7ed0736803a",
       "placeholder": "​",
       "style": "IPY_MODEL_678c72d46baa47439867d3fec767c5cc",
       "tabbable": null,
       "tooltip": null,
       "value": " 100/100 [00:35&lt;00:00,  2.86it/s]"
      }
     },
     "b9365f68125c4b59ab67132e84334dda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c557ac89352a420085c5816bb05d3f77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_28f81896ed5b4f7ba6346541750a27e8",
        "IPY_MODEL_8723cebae357498ea0bbd3cb7c8dd979",
        "IPY_MODEL_ab9bcf89cf9b4b789f5f1878c1ea6465"
       ],
       "layout": "IPY_MODEL_63d75d770b674ccaa2049a10e3c45b0f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c5a1cd58bace49c68461416790dda952": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b37107810414e828546b148557c66a8",
       "placeholder": "​",
       "style": "IPY_MODEL_df7b83b2aba54c0ebcdb31f4583b02bb",
       "tabbable": null,
       "tooltip": null,
       "value": " 100/100 [00:34&lt;00:00,  2.86it/s]"
      }
     },
     "dbbb78bea9f340f497eafc29b1fb1f0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df7b83b2aba54c0ebcdb31f4583b02bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dfab174198f44685bc8e11b9912259a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e031ff83c380437c9c22bdd9b62903de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8170b5e76ca43079848afc04fb6dfc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9ed0ca273f14498d855bb1641804db13",
       "placeholder": "​",
       "style": "IPY_MODEL_5e20360724014f9b8413f64d373e98c4",
       "tabbable": null,
       "tooltip": null,
       "value": " 100/100 [00:34&lt;00:00,  2.87it/s]"
      }
     },
     "eee31f717c544b10b30da7ed0736803a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f08a94be5e7d40adaf14baa99f5d7039": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f2d8a2f02c5c435f8632d4f03f35748d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4a034837d1847cf96d61a320106b3cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b9365f68125c4b59ab67132e84334dda",
       "placeholder": "​",
       "style": "IPY_MODEL_019031af7d8c48e69022c1cb57b2f9d3",
       "tabbable": null,
       "tooltip": null,
       "value": " 100/100 [00:34&lt;00:00,  2.92it/s]"
      }
     },
     "f69effc1db1a45b89beb8ec695ad323d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f8baa2bab0574d989b93569a13c54266": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fea27a5789d24ada95ccfd4cdb11147a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
