@Article{sosaa-trends-2021,
    author = "Chen, Dean and Xavier, Carlton and Clusius, Petri and Nieminen, Tuomo and Roldin, Pontus and Qi, Ximeng and Pichelstorfer, Lukas and Kulmala, Markku and Rantala, Pekka and Aalto, Juho and Sarnela, Nina and Kolari, Pasi and Keronen, Petri and Rissanen, Matti P. and Taipale, Ditte and Foreback, Benjamin and Baykara, Metin and Zhou, Putian and Boy, Michael",
    title = "A modelling study of OH{,} NO3 and H2SO4 in 2007--2018 at SMEAR II{,} Finland: analysis of long-term trends",
    journal = "Environ. Sci.: Atmos.",
    year = "2021",
    volume = "1",
    issue = "6",
    pages = "449--472",
    publisher = "RSC",
    doi = "10.1039/D1EA00020A",
    url = "http://dx.doi.org/10.1039/D1EA00020A",
    abstract = "Major atmospheric oxidants (OH{,} O3 and NO3) dominate the atmospheric oxidation capacity{,} while H2SO4 is considered as a main driver for new particle formation. Although numerous studies have investigated the long-term trend of ozone in Europe{,} the trends of OH{,} NO3 and H2SO4 at specific sites are to a large extent unknown. The one-dimensional model SOSAA has been applied in several studies at the SMEAR II station and has been validated by measurements in several projects. Here{,} we applied the SOSAA model for the years 2007--2018 to simulate the atmospheric chemical components{,} especially the atmospheric oxidants OH and NO3{,} as well as H2SO4 at SMEAR II. The simulations were evaluated with observations from several shorter and longer campaigns at SMEAR II. Our results show that daily OH increased by 2.39\% per year and NO3 decreased by 3.41\% per year{,} with different trends of these oxidants during day and night. On the contrary{,} daytime sulfuric acid concentrations decreased by 2.78\% per year{,} which correlated with the observed decreasing concentration of newly formed particles in the size range of 3--25 nm with 1.4\% per year at SMEAR II during the years 1997--2012. Additionally{,} we compared our simulated OH{,} NO3 and H2SO4 concentrations with proxies{,} which are commonly applied in case a limited number of parameters are measured and no detailed model simulations are available."
}

@Article{sosaa-bvoc-2017,
    AUTHOR = {Zhou, P. and Ganzeveld, L. and Taipale, D. and Rannik, \"U. and Rantala, P. and Rissanen, M. P. and Chen, D. and Boy, M.},
    TITLE = {Boreal forest BVOC exchange: emissions versus in-canopy sinks},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {17},
    YEAR = {2017},
    NUMBER = {23},
    PAGES = {14309--14332},
    URL = {https://acp.copernicus.org/articles/17/14309/2017/},
    DOI = {10.5194/acp-17-14309-2017}
}

@Article{sosa-description-2011,
    AUTHOR = {Boy, M. and Sogachev, A. and Lauros, J. and Zhou, L. and Guenther, A. and Smolander, S.},
    TITLE = {SOSA -- a new model to simulate the concentrations of organic vapours and sulphuric acid inside the ABL -- Part 1: Model description and initial evaluation},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {11},
    YEAR = {2011},
    NUMBER = {1},
    PAGES = {43--51},
    URL = {https://acp.copernicus.org/articles/11/43/2011/},
    DOI = {10.5194/acp-11-43-2011}
}

@Article{arca-box-2022,
    AUTHOR = {Clusius, P. and Xavier, C. and Pichelstorfer, L. and Zhou, P. and Olenius, T. and Roldin, P. and Boy, M.},
    TITLE = {Atmospherically Relevant Chemistry and Aerosol box model -- ARCA box (version 1.2)},
    JOURNAL = {Geoscientific Model Development},
    VOLUME = {15},
    YEAR = {2022},
    NUMBER = {18},
    PAGES = {7257--7286},
    URL = {https://gmd.copernicus.org/articles/15/7257/2022/},
    DOI = {10.5194/gmd-15-7257-2022}
}

@article{sosaa-description-2014,
    title={SOSAA—A new model to simulate the concentrations of organic vapours, sulphuric acid and aerosols inside the ABL—Part 2: Aerosol dynamics and one case study at a boreal forest site},
    author={Zhou, Luxi and Nieminen, Tuomo and Mogensen, Ditte and Smolander, Sampo and Rusanen, Anton and Kulmala, Markku and Boy, Michael},
    year={2014},
    journal={Boreal Environment Research},
    volume={19 (suppl. B)},
    pages={237--256},
    url={http://hdl.handle.net/10138/228763},
    urldate={2023-01-05},
}

@INPROCEEDINGS{novelty-detection-2010,
    author={Miljković, Dubravko},
    booktitle={The 33rd International Convention MIPRO},
    title={Review of novelty detection methods},
    year={2010},
    volume={},
    number={},
    pages={593--598},
    abstract={Novelty detection is the identification of new or unknown data or signals that a machine learning system is not aware of during training. Novelty detection methods try to identify outliers that differ from the distribution of ordinary data. This paper is a short review of novelty detection and its methods.},
    url={https://ieeexplore.ieee.org/document/5533467},
    urldate={2023-01-10},
    month={05},
}

@proceedings{auto-associative-2001,
    author = {Sohn, Hoon and Worden, Keith and Farrar, Charles R.},
    title = {Novelty Detection Using Auto-Associative Neural Network},
    volume = {Dynamic Systems and Control},
    series = {ASME International Mechanical Engineering Congress and Exposition},
    pages = {573--580},
    year = {2001},
    month = {11},
    abstract = {The primary objective of novelty detection is to examine if a system significantly deviates from the initial baseline condition of the system. In reality, the system is often subject to changing environmental and operation conditions affecting its dynamic characteristics. Such variations include changes in loading, boundary conditions, temperature, and humidity. Most damage diagnosis techniques, however, generally neglect the effects of these changing ambient conditions. Here, a novelty detection technique is developed explicitly taking into account these natural variations of the system in order to minimize false positive indications of true system changes. Auto-associative neural networks are employed to discriminate system changes of interest such as structural deterioration and damage from the natural variations of the system.},
    doi = {10.1115/IMECE2001/DSC-24571},
    url = {https://doi.org/10.1115/IMECE2001/DSC-24571},
    eprint = {https://asmedigitalcollection.asme.org/IMECE/proceedings-pdf/IMECE2001/35609/573/6795488/573\\_1\\_imece2001-dsc-24571.pdf},
}

@misc{distance-confidence-2017,
    doi = {10.48550/ARXIV.1709.09844},
    url = {https://arxiv.org/abs/1709.09844},
    author = {Mandelbaum, Amit and Weinshall, Daphna},
    keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Distance-based Confidence Score for Neural Network Classifiers},
    publisher = {arXiv},
    year = {2017},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{learning-ood-confidence-2018,
    doi = {10.48550/ARXIV.1802.04865},
    url = {https://arxiv.org/abs/1802.04865},
    author = {DeVries, Terrance and Taylor, Graham W.},
    keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Learning Confidence for Out-of-Distribution Detection in Neural Networks},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{noise-contrastive-uq-2020,
    title = {Noise Contrastive Priors for Functional Uncertainty},
    author = {Hafner, Danijar and Tran, Dustin and Lillicrap, Timothy and Irpan, Alex and Davidson, James},
    booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
    pages = {905--914},
    year = {2020},
    editor = {Adams, Ryan P. and Gogate, Vibhav},
    volume = {115},
    series = {Proceedings of Machine Learning Research},
    month = {07},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v115/hafner20a/hafner20a.pdf},
    url = {https://proceedings.mlr.press/v115/hafner20a.html},
    abstract = {Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge. Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior. In particular, the common practice of an independent normal prior in weight space imposes relatively weak constraints on the function posterior, allowing it to generalize in unforeseen ways on inputs outside of the training distribution. We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates. The key idea is to train the model to output high uncertainty for data points outside of the training distribution. NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs. NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training. Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning. We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.}
}

@article{ood-boundary-2021,
    doi = {10.48550/ARXIV.2112.11648},
    url = {https://arxiv.org/abs/2112.11648},
    author = {Pei, Sen and Zhang, Xin and Fan, Bin and Meng, Gaofeng},
    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Out-of-distribution Detection with Boundary Aware Learning},
    publisher = {arXiv},
    year = {2021},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{atmospheric-chemistry-1999,
    title={Introduction to atmospheric chemistry},
    author={Jacob, DJ},
    publisher={Princeton, NJ Princeton University Press},
    year={1999},
    url={https://acmg.seas.harvard.edu/education/introduction-atmospheric-chemistry},
    urldate={2023-01-14},
    isbn={978-0691001852},
    edition={1}
}

@article{methane-hydrates-2017,
    author = {Ruppel, Carolyn D. and Kessler, John D.},
    title = {The interaction of climate change and methane hydrates},
    journal = {Reviews of Geophysics},
    volume = {55},
    number = {1},
    pages = {126--168},
    keywords = {methane hydrate, climate, global warming, greenhouse gas},
    doi = {10.1002/2016RG000534},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016RG000534},
    eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2016RG000534},
    abstract = {Abstract Gas hydrate, a frozen, naturally-occurring, and highly-concentrated form of methane, sequesters significant carbon in the global system and is stable only over a range of low-temperature and moderate-pressure conditions. Gas hydrate is widespread in the sediments of marine continental margins and permafrost areas, locations where ocean and atmospheric warming may perturb the hydrate stability field and lead to release of the sequestered methane into the overlying sediments and soils. Methane and methane-derived carbon that escape from sediments and soils and reach the atmosphere could exacerbate greenhouse warming. The synergy between warming climate and gas hydrate dissociation feeds a popular perception that global warming could drive catastrophic methane releases from the contemporary gas hydrate reservoir. Appropriate evaluation of the two sides of the climate-methane hydrate synergy requires assessing direct and indirect observational data related to gas hydrate dissociation phenomena and numerical models that track the interaction of gas hydrates/methane with the ocean and/or atmosphere. Methane hydrate is likely undergoing dissociation now on global upper continental slopes and on continental shelves that ring the Arctic Ocean. Many factors—the depth of the gas hydrates in sediments, strong sediment and water column sinks, and the inability of bubbles emitted at the seafloor to deliver methane to the sea-air interface in most cases—mitigate the impact of gas hydrate dissociation on atmospheric greenhouse gas concentrations though. There is no conclusive proof that hydrate-derived methane is reaching the atmosphere now, but more observational data and improved numerical models will better characterize the climate-hydrate synergy in the future.},
    year = {2017}
}

@incollection{cloud-aerosol-1996,
    title = {Ervations of cloud-induced aerosol growth},
    editor = {Markku Kulmala and Paul E. Wagner},
    booktitle = {Nucleation and Atmospheric Aerosols 1996},
    publisher = {Pergamon},
    address = {Amsterdam},
    pages = {937--940},
    year = {1996},
    isbn = {978-0-08-042030-1},
    doi = {10.1016/B978-008042030-1/50231-6},
    url = {https://www.sciencedirect.com/science/article/pii/B9780080420301502316},
    author = {Michael H. Smith and Colin D. O'dowd and Jason A. Lowe},
    abstract = {Publisher Summary
    Observations of the growth of aerosol in clouds in this chapter are presented for rural and marine environments. Simple calculations suggest that the only feasible growth mechanism capable of explaining the observed growth is aqueous phase oxidation in cloud droplets, or, in other words, chemical processing in clouds. The accumulation mode aerosol distribution observed under cloudy air possess a mode radius of 0.1 gm and is approximately twice that observed under cloud-free conditions (0.05 gm). The progressive growth of aerosol from 0.05 μm to 0.1 μm was observed after repeated passage through cloud. Analysis of possible growth processes suggest that the aqueous phase oxidation of aerosol precursors, such as SO2, is the only growth mechanism capable of occurring over a sufficiently rapid time scale.}
}

@article{aerosol-airways-2005,
    author = {G\"unter Oberd\"orster and Eva Oberd\"orster and Jan Oberd\"orster},
    title = {Nanotoxicology: An Emerging Discipline Evolving from Studies of Ultrafine Particles},
    journal = {Environmental Health Perspectives},
    volume = {113},
    number = {7},
    pages = {823--839},
    year = {2005},
    doi = {10.1289/ehp.7339},
    URL = {https://ehp.niehs.nih.gov/doi/abs/10.1289/ehp.7339},
    eprint = {https://ehp.niehs.nih.gov/doi/pdf/10.1289/ehp.7339},
    abstract = { Although humans have been exposed to airborne nanosized particles (NSPs; \&lt; 100 nm) throughout their evolutionary stages, such exposure has increased dramatically over the last century due to anthropogenic sources. The rapidly developing field of nanotechnology is likely to become yet another source through inhalation, ingestion, skin uptake, and injection of engineered nanomaterials. Information about safety and potential hazards is urgently needed. Results of older bio-kinetic studies with NSPs and newer epidemiologic and toxicologic studies with airborne ultrafine particles can be viewed as the basis for the expanding field of nanotoxicology, which can be defined as safety evaluation of engineered nanostructures and nanodevices. Collectively, some emerging concepts of nanotoxicology can be identified from the results of these studies. When inhaled, specific sizes of NSPs are efficiently deposited by diffusional mechanisms in all regions of the respiratory tract. The small size facilitates uptake into cells and transcytosis across epithelial and endothelial cells into the blood and lymph circulation to reach potentially sensitive target sites such as bone marrow, lymph nodes, spleen, and heart. Access to the central nervous system and ganglia via translocation along axons and dendrites of neurons has also been observed. NSPs penetrating the skin distribute via uptake into lymphatic channels. Endocytosis and biokinetics are largely dependent on NSP surface chemistry (coating) and in vivo surface modifications. The greater surface area per mass compared with larger-sized particles of the same chemistry renders NSPs more active biologically. This activity includes a potential for inflammatory and pro-oxidant, but also antioxidant, activity, which can explain early findings showing mixed results in terms of toxicity of NSPs to environmentally relevant species. Evidence of mitochondrial distribution and oxidative stress response after NSP endocytosis points to a need for basic research on their interactions with subcellular structures. Additional considerations for assessing safety of engineered NSPs include careful selections of appropriate and relevant doses/concentrations, the likelihood of increased effects in a compromised organism, and also the benefits of possible desirable effects. An interdisciplinary team approach (e.g., toxicology, materials science, medicine, molecular biology, and bioinformatics, to name a few) is mandatory for nanotoxicology research to arrive at an appropriate risk assessment.}
}

@incollection{eu-health-2021,
    title = {Health impacts of air pollution in Europe, 2021},
    booktitle = {Air quality in Europe 2021},
    publisher = {European Environment Agency},
    number = {15/2021},
    year = {2021},
    isbn = {978-92-9480-404-4},
    url = {https://www.eea.europa.eu/ds_resolveuid/02a56e1794234a458cf9005fb41dcbe2},
    urldate = {2023-01-14}
} % doi = {10.2800/08097}

@incollection{ccn-1999,
    title = {Chapter 5 - Nucleation and Diffusional Growth},
    editor = {Judith A. Curry and Peter J. Webster},
    series = {International Geophysics},
    publisher = {Academic Press},
    volume = {65},
    pages = {129--158},
    year = {1999},
    booktitle = {Thermodynamics of Atmospheres and Oceans},
    issn = {0074-6142},
    doi = {10.1016/S0074-6142(99)80027-8},
    url = {https://www.sciencedirect.com/science/article/pii/S0074614299800278}
}

@inbook{ipcc-6-summary-2021,
    author = {IPCC},
    title = {Summary for Policymakers},
    booktitle = {Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change},
    editor = {Masson-Delmotte, V. and Zhai, P. and Pirani, A. and Connors, S.L. and P\'ean, C. and Berger, S. and Caud, N. and Chen, Y. and Goldfarb, L. and Gomis, M.I. and Huang, M. and Leitzell, K. and Lonnoy, E. and Matthews, J.B.R. and Maycock, T.K. and Waterfield, T. and Yelek\c{c}i, O. and Yu, R. and Zhou, B.},
    publisher = {Cambridge University Press},
    address = {Cambridge, United Kingdom and New York, NY, USA},
    pages = {3--32},
    doi = {10.1017/9781009157896.001},
    year = {2021},
    type = {Book Section}
}

@Inbook{smear-station-2013,
    author = {Hari, Pertti and Nikinmaa, Eero and Pohja, Toivo and Siivola, Erkki and B{\"a}ck, Jaana and Vesala, Timo and Kulmala, Markku},
    editor = {Hari, Pertti and Heli{\"o}vaara, Kari and Kulmala, Liisa},
    title = {Station for Measuring Ecosystem-Atmosphere Relations: SMEAR},
    bookTitle = {Physical and Physiological Forest Ecology},
    year = {2013},
    publisher="Springer Netherlands",
    address="Dordrecht",
    pages="471--487",
    abstract="We planned and implemented the SMEAR II and I measuring systems to measure material and energy fluxes between forest ecosystem and its surroundings and within the ecosystem. In addition, we measured the processes generating the fluxes and environmental factors affecting the processes. We constructed SMEAR I in 1991 and SMEAR II in 1994--1996. The mass and energy fluxes play an important role in our physical and physiological theory of forest ecology. The construction principle of SMEAR measuring stations is coherent with our theory, and this is why the data obtained at SMEAR stations have been useful in the development and testing of our theory.",
    isbn="978-94-007-5603-8",
    doi="10.1007/978-94-007-5603-8_9",
    url="https://doi.org/10.1007/978-94-007-5603-8_9"
}

@Article{flexpart-10.4-2019,
    AUTHOR = {Pisso, I. and Sollum, E. and Grythe, H. and Kristiansen, N. I. and Cassiani, M. and Eckhardt, S. and Arnold, D. and Morton, D. and Thompson, R. L. and Groot Zwaaftink, C. D. and Evangeliou, N. and Sodemann, H. and Haimberger, L. and Henne, S. and Brunner, D. and Burkhart, J. F. and Fouilloux, A. and Brioude, J. and Philipp, A. and Seibert, P. and Stohl, A.},
    TITLE = {The Lagrangian particle dispersion model FLEXPART version 10.4},
    JOURNAL = {Geoscientific Model Development},
    VOLUME = {12},
    YEAR = {2019},
    NUMBER = {12},
    PAGES = {4955--4997},
    URL = {https://gmd.copernicus.org/articles/12/4955/2019/},
    DOI = {10.5194/gmd-12-4955-2019}
}

@Article{flexpart-6.2-2005,
    AUTHOR = {Stohl, A. and Forster, C. and Frank, A. and Seibert, P. and Wotawa, G.},
    TITLE = {Technical note: The Lagrangian particle dispersion model FLEXPART version 6.2},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {5},
    YEAR = {2005},
    NUMBER = {9},
    PAGES = {2461--2474},
    URL = {https://acp.copernicus.org/articles/5/2461/2005/},
    DOI = {10.5194/acp-5-2461-2005}
}

@article{flexpart-validation-1998,
    title = {Validation of the lagrangian particle dispersion model FLEXPART against large-scale tracer experiment data},
    journal = {Atmospheric Environment},
    volume = {32},
    number = {24},
    pages = {4245--4264},
    year = {1998},
    issn = {1352-2310},
    doi = {10.1016/S1352-2310(98)00184-8},
    url = {https://www.sciencedirect.com/science/article/pii/S1352231098001848},
    author = {A. Stohl and M. Hittenberger and G. Wotawa},
    keywords = {CAPTEX, ANATEX, ETEX, tracer, Lagrangian particle dispersion model, trajectories},
    abstract = {A comprehensive validation of FLEXPART, a recently developed Lagrangian particle dispersion model based on meteorological data from the European Centre for Medium-Range Weather Forecasts, is described in this paper. Measurement data from three large-scale tracer experiments, the Cross-Appalachian Tracer Experiment (CAPTEX), the Across North America Tracer Experiment (ANATEX) and the European Tracer Experiment (ETEX) are used for this purpose. The evaluation is based entirely on comparisons of model results and measurements paired in space and time. It is found that some of the statistical parameters often used for model validation are extremely sensitive to small measurement errors and should not be used in future studies. 40 cases of tracer dispersion are studied, allowing a validation of the model performance under a variety of different meteorological conditions. The model usually performs very well under undisturbed meteorological conditions, but it is less skilful in the presence of fronts. The two ETEX cases reveal the full range of the model’s skill, with the first one being among the best cases studied, and the second one being, by far, the worst. The model performance in terms of the statistical parameters used stays rather constant with time over the periods (up to 117h) studied here. It is shown that the method used to estimate the concentrations at the receptor locations has a significant effect on the evaluation results. The vertical wind component sometimes has a large influence on the model results, but on the average only a slight improvement over simulations which neglect the vertical wind can be demonstrated. Subgrid variability of mixing heights is important and must be accounted for.}
}

@Article{flexpart-correction-1999,
    author={Stohl, Andreas and Thomson, David J.},
    title={A Density Correction for Lagrangian Particle Dispersion Models},
    journal={Boundary-Layer Meteorology},
    year={1999},
    month={01},
    day={01},
    volume={90},
    number={1},
    pages={155--167},
    abstract={Current Lagrangian particle dispersion models, used to simulate the dispersion of passive tracers in the turbulent planetary boundary layer (PBL), assume that the density is constant within the PBL. In deep PBLs, where the density at the boundary-layer top may be lower by more than 20{\%} than at the surface, this assumption leads to errors in the tracer concentrations on the order of 10{\%}. In the presence of a vertical wind shear, this also leads to inaccurate calculations of the horizontal tracer transport. To remove this deficiency, a Langevin equation is presented that contains a density correction term. The effect of the density correction is studied using data from a large-scale tracer experiment. It is found that for this experiment, the main effect of the density correction is an increase in the surface tracer concentrations, whereas the horizontal tracer transport patterns remain largely unaffected.},
    issn={1573-1472},
    doi={10.1023/A:1001741110696},
    url={https://doi.org/10.1023/A:1001741110696}
}

@article{scandis-model-2002,
    author = {SOGACHEV, ANDREJ and MENZHULIN, GENNADY V. and HEIMANN, MARTIN and LLOYD, JON},
    title = {A simple three-dimensional canopy -- planetary boundary layer simulation model for scalar concentrations and fluxes},
    journal = {Tellus B},
    volume = {54},
    number = {5},
    pages = {784--819},
    doi = {10.1034/j.1600-0889.2002.201353.x},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1034/j.1600-0889.2002.201353.x},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1034/j.1600-0889.2002.201353.x},
    abstract = {abstract We present a numerical model capable of computing the physical processes within both plant canopy and planetary boundary layer (PBL), offering the potential benefit of wide applicability due to reduced computational requirements. The model, named SCADIS (scalar distribution), synthesizes existing knowledge of boundary and surface layer turbulence and surface layer vegetative processes and was tested using several data sets from the European part of Russia and Siberia obtained as part of the EUROSIBERIAN CARBONFLUX project. Despite simplifications which were necessary in order to simulate the natural processes, the first version of the model presented here demonstrated a satisfactory agreement between modelled and observed data for different surface features and weather conditions. For example, the model successfully predicted the diurnal patterns of concentration profiles of CO2, water vapour and potential temperature profiles both within the summer atmospheric boundary layer and within the plant canopy itself. The very different effects of the surface energy characteristics of bog versus forest on convective boundary layer (CBL) structure and development are also illustrated. The model was applied to evaluate the effective footprints for eddy covariance measurements above non-uniform plant canopies, the case study here being a mixed spruce forest in European Russia. The model also demonstrates the likely variations in the above-canopy turbulence and surface layer fluxes as dependent on the presence of patches of deciduous broadleaf forest within a predominantly evergreen coniferous stand.},
    year = {2002}
}

@Article{scandis-analysis-2005,
    author={Sogachev, A. and Panferov, O. and Gravenhorst, G. and Vesala, T.},
    title={Numerical analysis of flux footprints for different landscapes},
    journal={Theoretical and Applied Climatology},
    year={2005},
    month={04},
    day={01},
    volume={80},
    number={2},
    pages={169--185},
    abstract={A model for the canopy -- planetary boundary layer flow and scalar transport based on E-ɛ closure was applied to estimate footprint for CO2 fluxes over different inhomogeneous landscapes. Hypothetical heterogeneous vegetation patterns -- forest with clear-cuts as well as hypothetical heterogeneous relief -- a bell-shaped valley and a ridge covered by forest were considered. The distortions of airflow caused by these heterogeneities are shown -- the upwind deceleration of the flow at the ridge foot and above valley, acceleration at the crest and the flow separation with the reversed flow pattern at lee slopes of ridge and valley. The disturbances induce changes in scalar flux fields within the atmospheric surface layer comparing to fluxes for homogeneous conditions: at a fixed height the fluxes vary as a function of distance to disturbance. Correspondingly, the flux footprint estimated from model data depends on the location of the point of interest (flux measurement point) and may significantly deviate from that for a flat terrain. It is shown that proposed method could be used for the choice of optimal sensor position for flux measurements over complex terrain as well as for the interpretation of data for existing measurement sites. To illustrate the latter the method was applied for experimental site in Solling, Germany, taking into account the complex topography and vegetation heterogeneities. Results show that in certain situations (summer, neutral stratification, south or north wind) and for a certain sensor location the assumptions of idealized air flow structure could be used for measurement interpretation at this site, though in general, extreme caution should be applied when analytical footprint models are used in the interpretation of flux measurements over complex sites.},
    issn={1434-4483},
    doi={10.1007/s00704-004-0098-8},
    url={https://doi.org/10.1007/s00704-004-0098-8}
}

@Article{scandis-drag-2006,
    author={Sogachev, Andrey and Panferov, Oleg},
    title={Modification of Two-Equation Models to Account for Plant Drag},
    journal={Boundary-Layer Meteorology},
    year={2006},
    month={11},
    day={01},
    volume={121},
    number={2},
    pages={229--266},
    abstract={A modification of the most popular two-equation (E--$\phi$) models, taking into account the plant drag, is proposed. Here E is the turbulent kinetic energy (TKE) and $\phi$ is any of the following variables: El (product of E and the mixing length l),{\$}{\$}{\backslash}varepsilon{\$}{\$}(dissipation rate of TKE), and $\omega$ (specific dissipation of TKE,{\$}{\$}{\backslash}omega = {\backslash}varepsilon{\backslash}!/{\backslash}!E{\$}{\$}). The proposed modification is due to the fact that the model constants estimated experimentally for `free-air' flow do not allow for adequate reconstruction of the ratio between the production and dissipation rates of TKE in the vegetation canopy and have to be adjusted. The modification is universal, i.e. of the same type for all E--$\phi$ models considered. The numerical experiments carried out for both homogeneous and heterogeneous plant canopies with E--$\phi$ models (and with the E--l model taken as a kind of reference) show that the modification performs well. They also suggest that E--{\$}{\$}{\backslash}varepsilon{\$}{\$}and E--$\omega$ schemes are more promising than the E--El scheme for canopy flow simulation since they are not limited by the need to use a wall function.},
    issn={1573-1472},
    doi={10.1007/s10546-006-9073-5},
    url={https://doi.org/10.1007/s10546-006-9073-5}
}

@Article{megan-model-2006,
    AUTHOR = {Guenther, A. and Karl, T. and Harley, P. and Wiedinmyer, C. and Palmer, P. I. and Geron, C.},
    TITLE = {Estimates of global terrestrial isoprene emissions using MEGAN (Model of Emissions of Gases and Aerosols from Nature)},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {6},
    YEAR = {2006},
    NUMBER = {11},
    PAGES = {3181--3210},
    URL = {https://acp.copernicus.org/articles/6/3181/2006/},
    DOI = {10.5194/acp-6-3181-2006}
}

@article{kpp-preprocessor-2002,
    title = {The kinetic preprocessor KPP-a software environment for solving chemical kinetics},
    journal = {Computers \& Chemical Engineering},
    volume = {26},
    number = {11},
    pages = {1567--1579},
    year = {2002},
    issn = {0098-1354},
    doi = {10.1016/S0098-1354(02)00128-X},
    url = {https://www.sciencedirect.com/science/article/pii/S009813540200128X},
    author = {Valeriu Damian and Adrian Sandu and Mirela Damian and Florian Potra and Gregory R. Carmichael},
    keywords = {Chemical kinetics, Automatic code generation, Sparsity, Numerical integration},
    abstract = {The kinetic preprocessor (KPP) is a software tool that assists the computer simulation of chemical kinetic systems. The concentrations of a chemical system evolve in time according to the differential law of mass action kinetics. A computer simulation requires the implementation of the differential system and its numerical integration in time. KPP translates a specification of the chemical mechanism into fortran or c simulation code that implement the concentration time derivative function and its Jacobian, together with a suitable numerical integration scheme. Sparsity in Jacobian is carefully exploited in order to obtain computational efficiency. KPP incorporates a library with several widely used atmospheric chemistry mechanisms and users can add their own chemical mechanisms to the library. KPP also includes a comprehensive suite of stiff numerical integrators. The KPP development environment is designed in a modular fashion and allows for rapid prototyping of new chemical kinetic schemes as well as new numerical integration methods.}
}

@article{mcm-protocol-1997,
    title = {The tropospheric degradation of volatile organic compounds: a protocol for mechanism development},
    journal = {Atmospheric Environment},
    volume = {31},
    number = {1},
    pages = {81--104},
    year = {1997},
    issn = {1352-2310},
    doi = {10.1016/S1352-2310(96)00105-7},
    url = {https://www.sciencedirect.com/science/article/pii/S1352231096001057},
    author = {Michael E. Jenkin and Sandra M. Saunders and Michael J. Pilling},
    keywords = {VOC oxidation, tropospheric chemistry, secondary pollutants, oxidants, ozone, free radical reactions, mechanisms, modelling},
    abstract = {Kinetic and mechanistic data relevant to the tropospheric oxidation of volatile organic compounds (VOCs) are used to define a series of rules for the construction of detailed degradation schemes for use in numerical models. These rules are intended to apply to the treatment of a wide range of non-aromatic hydrocarbons and oxygenated and chlorinated VOCs, and are currently being used to provide an up-to-date mechanism describing the degradation of a range of VOCs, and the production of secondary oxidants, for use in a model of the boundary layer over Europe. The schemes constructed using this protocol are applicable, however, to a wide range of ambient conditions, and may be employed in models of urban, rural or remote tropospheric environments, or for the simulation of secondary pollutant formation for a range of NOx or VOC emission scenarios. These schemes are believed to be particularly appropriate for comparative assessments of the formation of oxidants, such as ozone, from the degradation of organic compounds. The protocol is divided into a series of subsections dealing with initiation reactions, the reactions of the radical intermediates and the further degradation of first and subsequent generation products. The present work draws heavily on previous reviews and evaluations of data relevant to tropospheric chemistry. Where necessary, however, existing recommendations are adapted, or new rules are defined, to reflect recent improvements in the database, particularly with regard to the treatment of peroxy radical (RO2) reactions for which there have been major advances, even since comparatively recent reviews. The present protocol aims to take into consideration work available in the open literature up to the end of 1994, and some further studies known by the authors, which were under review at that time. A major disadvantage of explicit chemical mechanisms is the very large number of reactions potentially generated, if a series of rules is rigorously applied. The protocol aims to limit the number of reactions in a degradation scheme by applying a degree of strategic simplification, whilst maintaining the essential features of the chemistry. These simplification measures are described, and their influence is demonstrated and discussed. The resultant mechanisms are believed to provide a suitable starting point for the generation of reduced chemical mechanisms.}
}

@Article{mcm-v3-2003,
    AUTHOR = {Saunders, S. M. and Jenkin, M. E. and Derwent, R. G. and Pilling, M. J.},
    TITLE = {Protocol for the development of the Master Chemical Mechanism, MCM v3 (Part A): tropospheric degradation of non-aromatic volatile organic compounds},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {3},
    YEAR = {2003},
    NUMBER = {1},
    PAGES = {161--180},
    URL = {https://acp.copernicus.org/articles/3/161/2003/},
    DOI = {10.5194/acp-3-161-2003}
}

@Article{mcm-beta-2012,
    AUTHOR = {Jenkin, M. E. and Wyche, K. P. and Evans, C. J. and Carr, T. and Monks, P. S. and Alfarra, M. R. and Barley, M. H. and McFiggans, G. B. and Young, J. C. and Rickard, A. R.},
    TITLE = {Development and chamber evaluation of the MCM v3.2 degradation scheme for $\beta$-caryophyllene},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {12},
    YEAR = {2012},
    NUMBER = {11},
    PAGES = {5275--5308},
    URL = {https://acp.copernicus.org/articles/12/5275/2012/},
    DOI = {10.5194/acp-12-5275-2012}
}

@Article{sosaa-ozone-2017,
    AUTHOR = {Zhou, P. and Ganzeveld, L. and Rannik, \"U. and Zhou, L. and Gierens, R. and Taipale, D. and Mammarella, I. and Boy, M.},
    TITLE = {Simulating ozone dry deposition at a boreal forest with a multi-layer canopy deposition model},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {17},
    YEAR = {2017},
    NUMBER = {2},
    PAGES = {1361--1379},
    URL = {https://acp.copernicus.org/articles/17/1361/2017/},
    DOI = {10.5194/acp-17-1361-2017}
}

@article{gas-exchange-2002,
    author = {Ganzeveld, L. N. and Lelieveld, J. and Dentener, F. J. and Krol, M. C. and Roelofs, G.-J.},
    title = {Atmosphere-biosphere trace gas exchanges simulated with a single-column model},
    journal = {Journal of Geophysical Research: Atmospheres},
    volume = {107},
    number = {D16},
    pages = {ACH 8-1 -- ACH 8-21},
    keywords = {Atmosphere-biosphere trace gas exchanges, micrometeorology, dry deposition, biogenic emissions, turbulence, canopy reduction factor},
    doi = {10.1029/2001JD000684},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2001JD000684},
    eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2001JD000684},
    abstract = {The exchange of oxidized nitrogen species (NOx) between the biosphere and atmosphere is controlled by complex interactions between emissions, dry deposition, (photochemical) chemical transformations, and turbulent exchanges, all varying with height within the canopy. We have developed a multilayer atmosphere-biosphere trace gas exchange model to study the role of canopy interactions in the net atmosphere-biosphere NOx exchange flux on a global scale. We evaluate this model, implemented in a single-column chemistry and meteorological model, for a selection of ecosystems by comparison with observations. The modeled and observed ozone and oxidized nitrogen concentrations and fluxes are generally in reasonable agreement if we constrain our model with site-specific surface and meteorological parameters. The sensitivity of atmosphere-biosphere trace gas exchange to nocturnal turbulent exchange appears to be large. A comparison of the NOx fluxes calculated by the traditional big leaf approach and the atmosphere-biosphere model is presented. For sites that are exposed to relatively large anthropogenic emission fluxes, the big-leaf approach and biosphere model calculate similar NOx fluxes, which confirms the applicability of the big-leaf approach for polluted regions. However, for relatively pristine sites, differences between the NOx fluxes of the biosphere model and the big leaf approach are significant. This underscores the importance of an explicit representation of the biosphere processes for those locations where the NO soil emissions flux is comparable to or exceeds the anthropogenic emissions.},
    year = {2002}
}

@Article{uhma-model-2004,
    AUTHOR = {Korhonen, H. and Lehtinen, K. E. J. and Kulmala, M.},
    TITLE = {Multicomponent aerosol dynamics model UHMA: model development and validation},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {4},
    YEAR = {2004},
    NUMBER = {3},
    PAGES = {757--771},
    URL = {https://acp.copernicus.org/articles/4/757/2004/},
    DOI = {10.5194/acp-4-757-2004}
}

@inproceedings{gan-2014,
    author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
    publisher = {Curran Associates, Inc.},
    title = {Generative Adversarial Nets},
    url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
    volume = {27},
    year = {2014},
    doi = {10.48550/arXiv.1406.2661}
}

@misc{fast-gradient-2014,
    doi = {10.48550/ARXIV.1412.6572},
    url = {https://arxiv.org/abs/1412.6572},
    author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
    keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Explaining and Harnessing Adversarial Examples},
    publisher = {arXiv},
    year = {2014},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{akaike-fpe-1969,
    author={Akaike, Hirotugu},
    title={Fitting autoregressive models for prediction},
    journal={Annals of the Institute of Statistical Mathematics},
    year={1969},
    month={12},
    day={01},
    volume={21},
    number={1},
    pages={243--247},
    issn={1572-9052},
    doi={10.1007/BF02532251},
    url={https://doi.org/10.1007/BF02532251}
}

@Article{akaike-fpe-1970,
    author={Akaike, Hirotugu},
    title={Statistical predictor identification},
    journal={Annals of the Institute of Statistical Mathematics},
    year={1970},
    month={12},
    day={01},
    volume={22},
    number={1},
    pages={203--217},
    issn={1572-9052},
    doi={10.1007/BF02506337},
    url={https://doi.org/10.1007/BF02506337}
}

@Article{akaike-fpe-1971,
    author={Akaike, Hirotugu},
    title={Autoregressive model fitting for control},
    journal={Annals of the Institute of Statistical Mathematics},
    year={1971},
    month={12},
    day={01},
    volume={23},
    number={1},
    pages={163--180},
    abstract={The use of a multidimensional extension of the minimum final prediction error (FPE) criterion which was originally developed for the decision of the order of one-dimensional autoregressive process [1] is discussed from the standpoint of controller design. It is shown by numerical examples that the criterion will also be useful for the decision of inclusion or exclusion of a variable into the model. Practical utility of the procedure was verified in the real controller design process of cement rotary kilns.},
    issn={1572-9052},
    doi={10.1007/BF02479221},
    url={https://doi.org/10.1007/BF02479221}
}

@ARTICLE{akaike-aic-1974,
    author={Akaike, H.},
    journal={IEEE Transactions on Automatic Control}, 
    title={A new look at the statistical model identification}, 
    year={1974},
    volume={19},
    number={6},
    pages={716--723},
    doi={10.1109/TAC.1974.1100705}
}

@Article{mahalanobis-distance-1936,
    title={On the generalised distance in statistics [reprint]},
    author={Mahalanobis, Prasanta Chandra},
    journal={Proceedings of the National Institute of Sciences of India},
    year={1936},
    volume={2},
    number={1},
    pages={49--55},
    doi={10.1007/s13171-019-00164-5},
    url={https://doi.org/10.1007/s13171-019-00164-5}
}

@article{mahalanobis-outlier-2000,
    author = {Myung Geun Kim},
    title = {Multivariate outliers and decompositions of mahalanobis distance},
    journal = {Communications in Statistics - Theory and Methods},
    volume = {29},
    number = {7},
    pages = {1511--1526},
    year  = {2000},
    publisher = {Taylor & Francis},
    doi = {10.1080/03610920008832559},
    URL = {https://doi.org/10.1080/03610920008832559},
    eprint = {https://doi.org/10.1080/03610920008832559},
    abstract = { Two decompositions of the Mahalanobis distance are considered. These decompositions help to explain some reasons for the outlyingness of multivari- ate observations. They also provide a graphical tool for identifying outliers including those that have a large influence on the multiple correlation coefficient, Illustrative examples are given. }
}

@inproceedings{lof-outlier-2000,
    author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J\"{o}rg},
    title = {LOF: Identifying Density-Based Local Outliers},
    year = {2000},
    isbn = {1581132174},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/342009.335388},
    doi = {10.1145/342009.335388},
    abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
    booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
    pages = {93--104},
    numpages = {12},
    keywords = {outlier detection, database mining},
    location = {Dallas, Texas, USA},
    series = {SIGMOD '00}
}

@INPROCEEDINGS{knn-outlier-2004,
    author={Hautamaki, V. and Karkkainen, I. and Franti, P.},
    booktitle={Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.}, 
    title={Outlier detection using k-nearest neighbour graph}, 
    year={2004},
    volume={3},
    number={},
    pages={430-433 Vol.3},
    abstract={We present an outlier detection using indegree number (ODIN) algorithm that utilizes k-nearest neighbour graph. Improvements to existing kNN distance-based method are also proposed. We compare the methods with real and synthetic datasets. The results show that the proposed method achieves reasonable results with synthetic data and outperforms compared methods with real data sets with small number of observations.},
    keywords={},
    doi={10.1109/ICPR.2004.1334558},
    ISSN={1051-4651},
    month={8},
}

@inproceedings{cluster-novelty-2008,
    author = {Miljkovi{\'c}, Dubravko},
    year = {2008},
    month = {05},
    title = {Novelty Detection In Machine Vibration Data Based On Cluster Intraset Distance},
    pages = {59--66},
    journal = {CTS, MIPRO},
    url = {https://www.researchgate.net/publication/304082527_Novelty_Detection_In_Machine_Vibration_Data_Based_On_Cluster_Intraset_Distance},
    urldate = {2023-01-15}
}

@Article{non-linear-pca-1989,
    author={Cybenko, G.},
    title={Approximation by superpositions of a sigmoidal function},
    journal={Mathematics of Control, Signals and Systems},
    year={1989},
    month={12},
    day={01},
    volume={2},
    number={4},
    pages={303--314},
    abstract={In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
    issn={1435-568X},
    doi={10.1007/BF02551274},
    url={https://doi.org/10.1007/BF02551274}
}

@article{regression-collinearity-1984,
    author = {Wold, S. and Ruhe, A. and Wold, H. and Dunn, III, W. J.},
    title = {The Collinearity Problem in Linear Regression. The Partial Least Squares (PLS) Approach to Generalized Inverses},
    journal = {SIAM Journal on Scientific and Statistical Computing},
    volume = {5},
    number = {3},
    pages = {735--743},
    year = {1984},
    doi = {10.1137/0905052},
    URL = {https://doi.org/10.1137/0905052},
    eprint = {https://doi.org/10.1137/0905052},
    abstract = { The use of partial least squares (PLS) for handling collinearities among the independent variables X in multiple regression is discussed. Consecutive estimates \$({\text{rank }}1,2,\cdots )\$ are obtained using the residuals from previous rank as a new dependent variable y. The PLS method is equivalent to the conjugate gradient method used in Numerical Analysis for related problems.To estimate the “optimal” rank, cross validation is used. Jackknife estimates of the standard errors are thereby obtained with no extra computation.The PLS method is compared with ridge regression and principal components regression on a chemical example of modelling the relation between the measured biological activity and variables describing the chemical structure of a set of substituted phenethylamines. }
}

@INPROCEEDINGS{ood-class-2022,
    author={Tadros, Antoine and Drouyer, Sébastien and von Gioi, Rafael Grompone},
    booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
    title={Out-Of-Distribution As A Target Class in Semi-Supervised Learning},
    year={2022},
    volume={},
    number={},
    pages={3249--3252},
    abstract={A key limitation of supervised learning is the ability to handle data from unknown distributions. Often, such methods fail when presented with samples from a source not represented in the training data. This work proposes an effective way of controlling the behavior of a neural network in the presence of out-of-distribution examples. For this, the training dataset is supplemented with extraneous data assigned to an additional out-of-distribution class. The extraneous data may come from a different dataset or be even noise. By applying a Gaussian mixture model on the latent representation, and by taking advantage of the ability of these models to generalize well, the method described thereafter performs well. Training the model on a segregated dataset helps the model to distinguish out-of-distribution data, including the ones the model were never confronted to during training.},
    keywords={},
    doi={10.1109/ICASSP43922.2022.9746936},
    ISSN={2379-190X},
    month={05},
}

@inproceedings{ood-adversarial-detection-2018,
    author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
    title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
    year = {2018},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.},
    booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
    pages = {7167--7177},
    numpages = {11},
    location = {Montr\'{e}al, Canada},
    series = {NIPS'18},
    url = {https://dl.acm.org/doi/10.5555/3327757.3327819},
    urldate = {2023-01-16}
}

@article{ood-baseline-2016,
    doi = {10.48550/ARXIV.1610.02136},
    url = {https://arxiv.org/abs/1610.02136},
    author = {Hendrycks, Dan and Gimpel, Kevin},
    keywords = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
    publisher = {arXiv},
    year = {2016},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{odin-detector-2017,
    doi = {10.48550/ARXIV.1706.02690},
    url = {https://arxiv.org/abs/1706.02690},
    author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
    publisher = {arXiv},
    year = {2017},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{mc-dropout-2016,
    title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
    author = {Gal, Yarin and Ghahramani, Zoubin},
    booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
    pages = {1050--1059},
    year = {2016},
    editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
    volume = {48},
    series = {Proceedings of Machine Learning Research},
    address = {New York, New York, USA},
    month = {06},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v48/gal16.pdf},
    url = {https://proceedings.mlr.press/v48/gal16.html},
    abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@misc{batch-normalized-uq-2018,
    doi = {10.48550/ARXIV.1802.06455},
    url = {https://arxiv.org/abs/1802.06455},
    author = {Teye, Mattias and Azizpour, Hossein and Smith, Kevin},
    keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Bayesian Uncertainty Estimation for Batch Normalized Deep Networks},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dime-detector-2021,
    doi = {10.48550/ARXIV.2108.10673},
    url = {https://arxiv.org/abs/2108.10673},
    author = {Sjögren, Rickard and Trygg, Johan},
    keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Out-of-Distribution Example Detection in Deep Neural Networks using Distance to Modelled Embedding},
    publisher = {arXiv},
    year = {2021},
    copyright = {Creative Commons Attribution 4.0 International}
}

@article{ood-exposure-confidence-2021,
    title = {Outlier exposure with confidence control for out-of-distribution detection},
    journal = {Neurocomputing},
    volume = {441},
    pages = {138--150},
    year = {2021},
    issn = {0925-2312},
    doi = {10.1016/j.neucom.2021.02.007},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231221002393},
    author = {Aristotelis-Angelos Papadopoulos and Mohammad Reza Rajati and Nazim Shaikh and Jiamian Wang},
    keywords = {Out-of-distribution detection, Regularization, Anomaly detection, Deep neural networks, Outlier exposure, Calibration},
    abstract = {Deep neural networks have achieved great success in classification tasks during the last years. However, one major problem to the path towards artificial intelligence is the inability of neural networks to accurately detect samples from novel class distributions and therefore, most of the existent classification algorithms assume that all classes are known prior to the training stage. In this work, we propose a methodology for training a neural network that allows it to efficiently detect out-of-distribution (OOD) examples without compromising much of its classification accuracy on the test examples from known classes. We propose a novel loss function that gives rise to a novel method, Outlier Exposure with Confidence Control (OECC), which achieves superior results in OOD detection with OE both on image and text classification tasks without requiring access to OOD samples. Additionally, we experimentally show that the combination of OECC with state-of-the-art post-training OOD detection methods, like the Mahalanobis Detector (MD) and the Gramian Matrices (GM) methods, further improves their performance in the OOD detection task, demonstrating the potential of combining training and post-training methods for OOD detection.}
}

@misc{ood-exposure-2018,
    doi = {10.48550/ARXIV.1812.04606},
    url = {https://arxiv.org/abs/1812.04606},
    author = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
    keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Deep Anomaly Detection with Outlier Exposure},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ood-training-2017,
    doi = {10.48550/ARXIV.1711.09325},
    url = {https://arxiv.org/abs/1711.09325},
    author = {Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
    keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples},
    publisher = {arXiv},
    year = {2017},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{concept-drift-detectors-2022,
    title = {From concept drift to model degradation: An overview on performance-aware drift detectors},
    journal = {Knowledge-Based Systems},
    volume = {245},
    pages = {108632},
    year = {2022},
    issn = {0950-7051},
    doi = {10.1016/j.knosys.2022.108632},
    url = {https://www.sciencedirect.com/science/article/pii/S0950705122002854},
    author = {Firas Bayram and Bestoun S. Ahmed and Andreas Kassler},
    keywords = {Concept drift, Model degradation, Data stream, Machine learning},
    abstract = {The dynamicity of real-world systems poses a significant challenge to deployed predictive machine learning (ML) models. Changes in the system on which the ML model has been trained may lead to performance degradation during the system’s life cycle. Recent advances that study non-stationary environments have mainly focused on identifying and addressing such changes caused by a phenomenon called concept drift. Different terms have been used in the literature to refer to the same type of concept drift and the same term for various types. This lack of unified terminology is set out to create confusion on distinguishing between different concept drift variants. In this paper, we start by grouping concept drift types by their mathematical definitions and survey the different terms used in the literature to build a consolidated taxonomy of the field. We also review and classify performance-based concept drift detection methods proposed in the last decade. These methods utilize the predictive model’s performance degradation to signal substantial changes in the systems. The classification is outlined in a hierarchical diagram to provide an orderly navigation between the methods. We present a comprehensive analysis of the main attributes and strategies for tracking and evaluating the model’s performance in the predictive system. The paper concludes by discussing open research challenges and possible research directions.}
}

@article{concept-drift-rates-2014,
    author = {Gama, Jo\~{a}o and \v{Z}liobait$\dot{\text{e}}$, Indr$\dot{\text{e}}$ and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
    title = {A Survey on Concept Drift Adaptation},
    year = {2014},
    issue_date = {April 2014},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {46},
    number = {4},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/2523813},
    doi = {10.1145/2523813},
    abstract = {Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this article, we characterize adaptive learning processes; categorize existing strategies for handling concept drift; overview the most representative, distinct, and popular techniques and algorithms; discuss evaluation methodology of adaptive algorithms; and present a set of illustrative applications. The survey covers the different facets of concept drift in an integrated way to reflect on the existing scattered state of the art. Thus, it aims at providing a comprehensive introduction to the concept drift adaptation for researchers, industry analysts, and practitioners.},
    journal = {ACM Comput. Surv.},
    month = {03},
    articleno = {44},
    numpages = {37},
    keywords = {Concept drift, data streams, change detection, adaptive learning}
}

@ARTICLE{concept-drift-definition-2019,
    author={Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, João and Zhang, Guangquan},
    journal={IEEE Transactions on Knowledge and Data Engineering}, 
    title={Learning under Concept Drift: A Review}, 
    year={2019},
    volume={31},
    number={12},
    pages={2346--2363},
    abstract={Concept drift describes unforeseeable changes in the underlying distribution of streaming data overtime. Concept drift research involves the development of methodologies and techniques for drift detection, understanding, and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift.},
    keywords={},
    doi={10.1109/TKDE.2018.2876857},
    ISSN={1558-2191},
    month={12},
}

@article{padre-rf-2021,
    author = {Tynes, Michael and Gao, Wenhao and Burrill, Daniel J. and Batista, Enrique R. and Perez, Danny and Yang, Ping and Lubbers, Nicholas},
    title = {Pairwise Difference Regression: A Machine Learning Meta-algorithm for Improved Prediction and Uncertainty Quantification in Chemical Search},
    journal = {Journal of Chemical Information and Modeling},
    volume = {61},
    number = {8},
    pages = {3846--3857},
    year = {2021},
    doi = {10.1021/acs.jcim.1c00670},
    note ={PMID: 34347460},
    URL = {https://doi.org/10.1021/acs.jcim.1c00670},
    eprint = {https://doi.org/10.1021/acs.jcim.1c00670},
    abstract = {Machine learning (ML) plays a growing role in the design and discovery of chemicals, aiming to reduce the need to perform expensive experiments and simulations. ML for such applications is promising but difficult, as models must generalize to vast chemical spaces from small training sets and must have reliable uncertainty quantification metrics to identify and prioritize unexplored regions. Ab initio computational chemistry and chemical intuition alike often take advantage of differences between chemical conditions, rather than their absolute structure or state, to generate more reliable results. We have developed an analogous comparison-based approach for ML regression, called pairwise difference regression (PADRE), which is applicable to arbitrary underlying learning models and operates on pairs of input data points. During training, the model learns to predict differences between all possible pairs of input points. During prediction, the test points are paired with all training set points, giving rise to a set of predictions that can be treated as a distribution of which the mean is treated as a final prediction and the dispersion is treated as an uncertainty measure. Pairwise difference regression was shown to reliably improve the performance of the random forest algorithm across five chemical ML tasks. Additionally, the pair-derived dispersion is both well correlated with model error and performs well in active learning. We also show that this method is competitive with state-of-the-art neural network techniques. Thus, pairwise difference regression is a promising tool for candidate selection algorithms used in chemical discovery.}
}

@misc{pairwise-images-2018,
    doi = {10.48550/ARXIV.1801.02929},
    url = {https://arxiv.org/abs/1801.02929},
    author = {Inoue, Hiroshi},
    keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Data Augmentation by Pairing Samples for Images Classification},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{ml-hyperparameters-2021,
    author = {Probst, Philipp and Boulesteix, Anne-Laure and Bischl, Bernd},
    title = {Tunability: Importance of Hyperparameters of Machine Learning Algorithms},
    year = {2021},
    issue_date = {January 2019},
    publisher = {JMLR.org},
    volume = {20},
    number = {1},
    issn = {1532-4435},
    abstract = {Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define databased defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning.},
    journal = {J. Mach. Learn. Res.},
    month = {03},
    pages = {1934--1965},
    numpages = {32},
    keywords = {hyperparameters, supervised learning, machine learning, tuning, meta-learning, classification},
    url = {https://dl.acm.org/doi/10.5555/3322706.3361994},
    urldate = {2023-01-22}
}

@techreport{gp-dimensionality-2005,
    author = {Bengio, Y. and Delalleau, Olivier and Roux, Nicolas},
    year = {2005},
    month = {01},
    institution = {Universit \'e de Montr \'eal},
    number = {1258},
    title = {The Curse of Dimensionality for Local Kernel Machines},
    url = {https://www.researchgate.net/publication/249772682_The_Curse_of_Dimensionality_for_Local_Kernel_Machines},
    urldate = {2023-01-22}
}

@book{aucpr-informative-1999,
    title = {Foundations of statistical natural language processing},
    author = {Manning, Christopher and Sch\"utze, Hinrich},
    year = {1999},
    publisher = {MIT press},
    isbn = {9780262133609}
}

@article{kl-divergence-1951,
    author = {S. Kullback and R. A. Leibler},
    title = {{On Information and Sufficiency}},
    volume = {22},
    journal = {The Annals of Mathematical Statistics},
    number = {1},
    publisher = {Institute of Mathematical Statistics},
    pages = {79--86},
    year = {1951},
    doi = {10.1214/aoms/1177729694},
    URL = {https://doi.org/10.1214/aoms/1177729694}
}

@software{mpl-label-lines-2022,
    author       = {Cadiou, Corentin},
    title        = {Matplotlib label lines},
    month        = {12},
    year         = {2022},
    publisher    = {Zenodo},
    version      = {v0.5.1},
    doi          = {10.5281/zenodo.7428071},
    url          = {https://doi.org/10.5281/zenodo.7428071}
}

@Inbook{gmm-encyclopedia-2009,
    author = {Reynolds, Douglas},
    editor = {Li, Stan Z. and Jain, Anil},
    title = {Gaussian Mixture Models},
    bookTitle = {Encyclopedia of Biometrics},
    year = {2009},
    publisher = {Springer US},
    address = {Boston, MA},
    pages = {659--663},
    isbn = {978-0-387-73003-5},
    doi = {10.1007/978-0-387-73003-5_196},
    url = {https://doi.org/10.1007/978-0-387-73003-5_196}
}

@Article{lifelong-learning-2017,
    author={Liu, Bing},
    title={Lifelong machine learning: a paradigm for continuous learning},
    journal={Frontiers of Computer Science},
    year={2017},
    month={06},
    day={01},
    volume={11},
    number={3},
    pages={359--361},
    issn={2095-2236},
    doi={10.1007/s11704-016-6903-6},
    url={https://doi.org/10.1007/s11704-016-6903-6}
}

@article{kde-1956,
    author = {Murray Rosenblatt},
    title = {{Remarks on Some Nonparametric Estimates of a Density Function}},
    volume = {27},
    journal = {The Annals of Mathematical Statistics},
    number = {3},
    publisher = {Institute of Mathematical Statistics},
    pages = {832--837},
    year = {1956},
    doi = {10.1214/aoms/1177728190},
    URL = {https://doi.org/10.1214/aoms/1177728190}
}

@article{kde-1962,
    author = {Emanuel Parzen},
    title = {{On Estimation of a Probability Density Function and Mode}},
    volume = {33},
    journal = {The Annals of Mathematical Statistics},
    number = {3},
    publisher = {Institute of Mathematical Statistics},
    pages = {1065--1076},
    year = {1962},
    doi = {10.1214/aoms/1177704472},
    URL = {https://doi.org/10.1214/aoms/1177704472}
}

@MISC{pdf-quantiles-2018,
    TITLE = {Find the PDF from quantiles},
    AUTHOR = {Hans Dembinski},
    year = {2018},
    HOWPUBLISHED = {Cross Validated},
    EPRINT = {https://stats.stackexchange.com/q/349088},
    URL = {https://stats.stackexchange.com/q/349088},
    urldate = {2023-01-27}
}

@INPROCEEDINGS{quantile-outlier-2020,
    author={Bahrami, Sajjad and Tuncel, Ertem},
    booktitle={2020 IEEE International Symposium on Information Theory (ISIT)}, 
    title={An Efficient Running Quantile Estimation Technique alongside Correntropy for Outlier Rejection in Online Regression}, 
    year={2020},
    volume={},
    number={},
    pages={2813--2818},
    abstract={In this paper, online linear regression in the presence of non-Gaussian noise is addressed. In such environments, there are outliers in error samples (error between system output and labels) and/or the error does not follow a Gaussian distribution. Information theoretic measures such as error entropy criterion (EEC) and error correntropy criterion (ECC) are known for their superior performance compared to mean square error (MSE) in these cases. In this paper, an efficient technique of running quantile estimation based on quantization of error samples is introduced alongside which correntropy leads to lower steady state misalignment compared to previous algorithms.},
    keywords={},
    doi={10.1109/ISIT44484.2020.9174111},
    ISSN={2157-8117},
    month={06},
}

@article{p2-quantile-1985,
    author = {Jain, Raj and Chlamtac, Imrich},
    title = {The P2 Algorithm for Dynamic Calculation of Quantiles and Histograms without Storing Observations},
    year = {1985},
    issue_date = {Oct. 1985},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {28},
    number = {10},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/4372.4378},
    doi = {10.1145/4372.4378},
    abstract = {A heuristic algorithm is proposed for dynamic calculation of the median and other quantiles. The estimates are produced dynamically as the observations are generated. The observations are not stored; therefore, the algorithm has a very small and fixed storage requirement regardless of the number of observations. This makes it ideal for implementing in a quantile chip that can be used in industrial controllers and recorders. The algorithm is further extended to histogram plotting. The accuracy of the algorithm is analyzed.},
    journal = {Commun. ACM},
    month = {10},
    pages = {1076--1085},
    numpages = {10}
}

@misc{frugal-quantiles-2014,
    doi = {10.48550/ARXIV.1407.1121},
    url = {https://arxiv.org/abs/1407.1121},
    author = {Ma, Qiang and Muthukrishnan, S. and Sandler, Mark},
    keywords = {Databases (cs.DB), Data Structures and Algorithms (cs.DS), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Frugal Streaming for Estimating Quantiles:One (or two) memory suffices},
    publisher = {arXiv},
    year = {2014},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@MISC{online-median-2021,
    TITLE = {"On-line" (iterator) algorithms for estimating statistical median, mode, skewness, kurtosis?},
    AUTHOR = {Tyler Streeter},
    year = {2021},
    HOWPUBLISHED = {Stack Overflow},
    EPRINT = {https://stackoverflow.com/a/2144754},
    URL = {https://stackoverflow.com/a/2144754},
    urldate = {2023-01-27}
}

@INPROCEEDINGS{median-perceptron-1997,
    author={Bylander, T. and Rosen, B.},
    booktitle={Proceedings of International Conference on Neural Networks (ICNN'97)}, 
    title={A perceptron-like online algorithm for tracking the median}, 
    year={1997},
    volume={4},
    number={},
    pages={2219--2224},
    abstract={We present an online algorithm for tracking the median of a series of values. The algorithm updates its current estimate of the median by incrementing or decrementing a fixed value, which is analogous to perceptron updating. The median value of a sequence minimizes the absolute loss, i.e., the sum of absolute deviations. The analysis shows that the worst-case absolute loss of our algorithm is comparable to the absolute loss of any sequence of target medians, given restrictions on how much the target can change per trial.},
    keywords={},
    doi={10.1109/ICNN.1997.614292},
    ISSN={},
    month={06},
}

@article{emission-rsm-2022,
    title = {Accurate prediction of air quality response to emissions for effective control policy design},
    journal = {Journal of Environmental Sciences},
    year = {2022},
    issn = {1001-0742},
    doi = {10.1016/j.jes.2022.02.009},
    url = {https://www.sciencedirect.com/science/article/pii/S1001074222000705},
    author = {Min Cao and Jia Xing and Shovan Kumar Sahu and Lei Duan and Junhua Li},
    keywords = {Air pollution, Nonlinear response, Air quality management, Reduced form models (RFMs), Precursor emissions},
    abstract = {Designing effective control policy requires accurate quantification of the relationship between the ambient concentrations of O3 and PM2.5 and the emissions of their precursors. However, the challenge is that precursor reduction does not necessarily lead to decreases in the concentrations of O3 and PM2.5, which are formed by multiple precursors under complex physical and chemical processes; this calls for the development of advanced model technologies to provide accurate predictions of the nonlinear responses of air quality to emissions. Different from the traditional sensitivity analysis and source apportionment methods, the reduced form models (RFMs) based on chemical transport models (CTMs) are able to quantify air quality responses to emissions more accurately and efficiently with lower computational cost. Here we review recent approaches used in RFMs and compare their structures, advantages and disadvantages, performance and applications. In general, RFMs are classified into three types including (1) sensitivity-based models, (2) models with simplified chemistry and physical processes, and (3) statistical models, with considerable differences in principles, characteristics and application ranges. The prediction of nonlinear responses by RFMs enables more in-depth analysis, not only in terms of real-time prediction of concentrations and quantification of human exposure, health impacts and economic damage, but also in optimizing control policies. Notably, data assimilation and emission inventory inversion based on the nonlinear response of concentrations to emissions can also be greatly beneficial to air pollution control management. In future studies, improvement in the performance of CTMs is exceedingly crucial to obtain a more reliable baseline for the prediction of air quality responses. Development of models to determine the air quality response to emissions under varying meteorological conditions is also necessary in the context of future climate changes, which pose great challenges to the quantification of response relationships. Additionally, with rising requirements for fine-scale air quality management, improving the performance of urban-scale simulations is worth considering. In short, accurate predictions of the response of air quality to emissions, though challenging, holds great promise for the present as well as for future scenarios.}
}

@INPROCEEDINGS{srsm-2004,
    author = {Isukapalli, S.S. and Balakrishnan, S. and Georgopoulos, P.G.},
    booktitle = {2004 43rd IEEE Conference on Decision and Control (CDC) (IEEE Cat. No.04CH37601)},
    title = {Computationally efficient uncertainty propagation and reduction using the stochastic response surface method},
    year = {2004},
    volume = {2},
    number = {},
    pages = {2237--2243},
    doi = {10.1109/CDC.2004.1430381}
}

@article{deep-rsm-2020,
    author = {Xing, Jia and Zheng, Shuxin and Ding, Dian and Kelly, James T. and Wang, Shuxiao and Li, Siwei and Qin, Tao and Ma, Mingyuan and Dong, Zhaoxin and Jang, Carey and Zhu, Yun and Zheng, Haotian and Ren, Lu and Liu, Tie-Yan and Hao, Jiming},
    title = {Deep Learning for Prediction of the Air Quality Response to Emission Changes},
    journal = {Environmental Science \& Technology},
    volume = {54},
    number = {14},
    pages = {8589--8600},
    year = {2020},
    doi = {10.1021/acs.est.0c02923},
    note = {PMID: 32551547},
    URL = {https://doi.org/10.1021/acs.est.0c02923},
    eprint = {https://doi.org/10.1021/acs.est.0c02923}
}

@article{self-adaptive-rsm-2022,
    title = {Improvements of response surface modeling with self-adaptive machine learning method for PM2.5 and O3 predictions},
    journal = {Journal of Environmental Management},
    volume = {303},
    pages = {114210},
    year = {2022},
    issn = {0301-4797},
    doi = {10.1016/j.jenvman.2021.114210},
    url = {https://www.sciencedirect.com/science/article/pii/S0301479721022726},
    author = {Jinying Li and Youzhi Dai and Yun Zhu and Xiangbo Tang and Shuxiao Wang and Jia Xing and Bin Zhao and Shaojia Fan and Shicheng Long and Tingting Fang},
    keywords = {Air quality model, Response surface model, Emissions control, Machine learning, Fine particles, Ozone},
    abstract = {Quickly quantifying the PM2.5 or O3 response to their precursor emission changes is a key point for developing effective control policies. The polynomial function-based response surface model (pf-RSM) can rapidly predict the nonlinear response of PM2.5 and O3 to precursors, but has drawbacks of overload computation and marginal effects (relatively larger prediction errors under strict control scenarios). To improve the performance of pf-RSM, a novel self-adaptive RSM (SA-RSM) was proposed by integrating the machine learning-based stepwise regression for establishing robust models to increase the computational efficiency and the collinearity diagnosis for reducing marginal effects caused by overfitting. The pilot study case demonstrated that compared with pf-RSM, SA-RSM can effectively reduce the training number by 70\% and 40\% and the fitting time by 40\% and 52\%, and decrease the prediction error by 49\% and 74\% for PM2.5 and O3 predictions respectively; moreover, the isopleths of PM2.5 or O3 as a function of their precursors generated by SA-RSM were more similar to those derived by chemical transport model (CTM), after successfully addressing the marginal effect issue. With the improved computation efficiency and prediction performance, SA-RSM is expected as a better scientific tool for decision-makers to make sound PM2.5 and O3 control policies.}
}

@phdthesis{srsm-phd-1999,
    author = {S. Isukapalli and Dr. Panos and G. Georgopoulos and Sastry S. Isukapalli and Sastry S. Isukapalli and Dissertation Director and Dr. Panos and G. Georgopoulos},
    title = {Uncertainty Analysis of Transport-Transformation Models},
    year = {1999},
    school = {The State University of New Jersey},
    url = {http://www.ccl.rutgers.edu/~ssi/thesis-screen.pdf},
    urldate = {2023-01-29}
}

@book{rv-generation-1986,
    title = {\textit{Non-Uniform Random Variate Generation}},
    author = {Devroye, Luc},
    publisher = {Springer, New York, NY},
    year = {1986},
    doi = {10.1007/978-1-4613-8643-8},
    url = {http://www.nrbook.com/devroye/Devroye_files/chapter_two.pdf},
    address = {New York, NY},
    isbn = {978-1-4613-8645-2},
    urldate = {2023-01-31}
}

@techreport{rsm-epa-2006,
    author      = {{U.S. EPA}},
    title       = {Technical Support Document for the Proposed PM NAAQS Rule: Response Surface Modeling},
    institution = {U.S. Environmental Protection Agency, Office of Air Quality Planning and Standards},
    year        = {2006},
    address     = {Research Triangle Park, NC, U.S},
    month       = {02},
    url         = {https://www.epa.gov/sites/default/files/2021-01/documents/pmnaaqs_tsd_rsm_all_021606.pdf},
    urldate     = {2023-02-02}
}

@Article{ersm-v1-2015,
    AUTHOR = {Zhao, B. and Wang, S. X. and Xing, J. and Fu, K. and Fu, J. S. and Jang, C. and Zhu, Y. and Dong, X. Y. and Gao, Y. and Wu, W. J. and Wang, J. D. and Hao, J. M.},
    TITLE = {Assessing the nonlinear response of fine particles to precursor emissions: development and application of an extended response surface modeling technique v1.0},
    JOURNAL = {Geoscientific Model Development},
    VOLUME = {8},
    YEAR = {2015},
    NUMBER = {1},
    PAGES = {115--128},
    URL = {https://gmd.copernicus.org/articles/8/115/2015/},
    DOI = {10.5194/gmd-8-115-2015}
}

@article{ersm-v2-2017,
    author = {Xing, Jia and Wang, Shuxiao and Zhao, Bin and Wu, Wenjing and Ding, Dian and Jang, Carey and Zhu, Yun and Chang, Xing and Wang, Jiandong and Zhang, Fenfen and Hao, Jiming},
    title = {Quantifying Nonlinear Multiregional Contributions to Ozone and Fine Particles Using an Updated Response Surface Modeling Technique},
    journal = {Environmental Science \& Technology},
    volume = {51},
    number = {20},
    pages = {11788--11798},
    year = {2017},
    doi = {10.1021/acs.est.7b01975},
    note = {PMID: 28891287},
    URL = {https://doi.org/10.1021/acs.est.7b01975},
    eprint = {https://doi.org/10.1021/acs.est.7b01975},
    abstract = { Tropospheric ozone (O3) and fine particles (PM2.5) come from both local and regional emissions sources. Due to the nonlinearity in the response of O3 and PM2.5 to their precursors, contributions from multiregional sources are challenging to quantify. Here we developed an updated extended response surface modeling technique (ERSMv2.0) to address this challenge. Multiregional contributions were estimated as the sum of three components: (1) the impacts of local chemistry on the formation of the pollutant associated with the change in its precursor levels at the receptor region; (2) regional transport of the pollutant from the source region to the receptor region; and (3) interregional effects among multiple regions, representing the impacts on the contribution from one source region by other source regions. Three components were quantified individually in the case study of Beijing-Tianjin-Hebei using the ERSMv2.0 model. For PM2.5 in most cases, the contribution from local chemistry (i.e., component 1) is greater than the contribution from regional transport (i.e., component 2). However, regional transport is more important for O3. For both O3 and PM2.5, the contribution from regional sources increases during high-pollution episodes, suggesting the importance of joint controls on regional sources for reducing the heavy air pollution. }
}

@Article{pf-rsm-2018,
    AUTHOR = {Xing, J. and Ding, D. and Wang, S. and Zhao, B. and Jang, C. and Wu, W. and Zhang, F. and Zhu, Y. and Hao, J.},
    TITLE = {Quantification of the enhanced effectiveness of NO$_{x}$ control from
    simultaneous reductions of VOC and $\text{NH}_3$ for reducing air pollution in
    the Beijing--Tianjin--Hebei region, China},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {18},
    YEAR = {2018},
    NUMBER = {11},
    PAGES = {7799--7814},
    URL = {https://acp.copernicus.org/articles/18/7799/2018/},
    DOI = {10.5194/acp-18-7799-2018}
}

@Article{epf-rsm-2020,
    author={Jin, Jiangbo and Zhu, Yun and Jang, Jicheng and Wang, Shuxiao and Xing, Jia and Chiang, Pen-Chi and Fan, Shaojia and Long, Shicheng},
    title={Enhancement of the polynomial functions response surface model for real-time analyzing ozone sensitivity},
    journal={Frontiers of Environmental Science {\&} Engineering},
    year={2020},
    month={09},
    day={10},
    volume={15},
    number={2},
    pages={31},
    abstract={Quantification of the nonlinearities between ambient ozone (O3) and the emissions of nitrogen oxides (NOx) and volatile organic compound (VOC) is a prerequisite for an effective O3 control strategy. An Enhanced polynomial functions Response Surface Model (Epf-RSM) with the capability to analyze O3-NOx-VOC sensitivities in real time was developed by integrating the hill-climbing adaptive method into the optimized Extended Response Surface Model (ERSM) system. The Epf-RSM could single out the best suited polynomial function for each grid cell to quantify the responses of O3 concentrations to precursor emission changes. Several comparisons between Epf-RSM and pf-ERSM (polynomial functions based ERSM) were performed using out-of-sample validation, together with comparisons of the spatial distribution and the Empirical Kinetic Modeling Approach diagrams. The comparison results showed that Epf-RSM effectively addressed the drawbacks of pf-ERSM with respect to over-fitting in the margin areas and high biases in the transition areas. The O3 concentrations predicted by Epf-RSM agreed well with Community Multi-scale Air Quality simulation results. The case study results in the Pearl River Delta and the north-western area of the Shandong province indicated that the O3 formations in the central areas of both the regions were more sensitive to anthropogenic VOC in January, April, and October, while more NOx-sensitive in July.},
    issn={2095-221X},
    doi={10.1007/s11783-020-1323-0},
    url={https://doi.org/10.1007/s11783-020-1323-0}
}

@article{pf-ersm-sl-2020,
    title = {Real-time source contribution analysis of ambient ozone using an enhanced meta-modeling approach over the Pearl River Delta Region of China},
    journal = {Journal of Environmental Management},
    volume = {268},
    pages = {110650},
    year = {2020},
    issn = {0301-4797},
    doi = {10.1016/j.jenvman.2020.110650},
    url = {https://www.sciencedirect.com/science/article/pii/S030147972030582X},
    author = {Tingting Fang and Yun Zhu and Jicheng Jang and Shuxiao Wang and Jia Xing and Pen-Chi Chiang and Shaojia Fan and Zhiqiang You and Jinying Li},
    keywords = {O, Nonlinear response, O-NO-VOC relationships, Source contribution, Response surface modeling},
    abstract = {The nonlinear response of O3 to nitrogen oxides (NOx) and volatile organic compounds (VOC) is not conducive to accurately identify the various source contributions and O3-NOx-VOC relationships. An enhanced meta-modeling approach, polynomial functions based response surface modeling coupled with the sectoral linear fitting technique (pf-ERSM-SL), integrating a new differential method (DM), was proposed to break through the limitation. The pf-ERSM-SL with DM was applied for analysis of O3 formation regime and real-time source contributions in July and October 2015 over the Pearl River Delta Region (PRD) of Mainland China. According to evaluations, the pf-ERSM-SL with DM was proven to be effective in source apportionment when the traditional sensitivity analysis was unsuitable for deriving the source contributions in the nonlinear system. After diagnosing the O3-NOx-VOC relationships, O3 formation in most regions of the PRD was identified as a distinctive NOx-limited regime in July; in October, the initial VOC-limited regime was found at small emission reductions (less than 22--44\%), but it will transit to NOx-limited when further reductions were implemented. Investigation of the source contributions suggested that NOx emissions were the dominated contributor when turning-off the anthropogenic emissions, occupying 85.41--94.90\% and 52.60--75.37\% of the peak O3 responses in July and October respectively in the receptor regions of the PRD; NOx emissions from the on-road mobile source (NOx_ORM) in Guangzhou (GZ), Dongguan&Shenzhen (DG&SZ) and Zhongshan (ZS) were identified as the main contributors. Consequently, the reinforced control of NOx_ORM is highly recommended to lower the ambient O3 in the PRD effectively.}
}

@ARTICLE{hill-climbing-2004,
    author={Lozano, Manuel and Herrera, Francisco and Krasnogor, Natalio and Molina, Daniel},
    journal={Evolutionary Computation}, 
    title={Real-Coded Memetic Algorithms with Crossover Hill-Climbing}, 
    year={2004},
    volume={12},
    number={3},
    pages={273--302},
    abstract={This paper presents a real-coded memetic algorithm that applies a crossover hill-climbing to solutions produced by the genetic operators. On the one hand, the memetic algorithm provides global search (reliability) by means of the promotion of high levels of population diversity. On the other, the crossover hill-climbing exploits the self-adaptive capacity of real-parameter crossover operators with the aim of producing an effective local tuning on the solutions (accuracy). An important aspect of the memetic algorithm proposed is that it adaptively assigns different local search probabilities to individuals. It was observed that the algorithm adjusts the global/local search balance according to the particularities of each problem instance. Experimental results show that, for a wide range of problems, the method we propose here consistently outperforms other real-coded memetic algorithms which appeared in the literature.},
    keywords={},
    doi={10.1162/1063656041774983},
    ISSN={1063-6560},
    month={09},
}

@article{cmaq-2006,
    author = {Byun, Daewon and Schere, Kenneth L.},
    title = {Review of the Governing Equations, Computational Algorithms, and Other Components of the Models-3 Community Multiscale Air Quality (CMAQ) Modeling System},
    journal = {Applied Mechanics Reviews},
    volume = {59},
    number = {2},
    pages = {51--77},
    year = {2006},
    month = {03},
    issn = {0003-6900},
    doi = {10.1115/1.2128636},
    url = {https://doi.org/10.1115/1.2128636},
    eprint = {https://asmedigitalcollection.asme.org/appliedmechanicsreviews/article-pdf/59/2/51/6900902/51\_1.pdf},
}


@inbook{stepwise-regression-2010,
    author = {Franke, George R.},
    publisher = {John Wiley \& Sons, Ltd},
    isbn = {9781444316568},
    title = {Stepwise Regression},
    booktitle = {Wiley International Encyclopedia of Marketing},
    chapter = {},
    pages = {},
    doi = {10.1002/9781444316568.wiem02071},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781444316568.wiem02071},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781444316568.wiem02071},
    year = {2010},
    keywords = {regression, correlation, collinearity, variable selection, forward selection, backward elimination},
    abstract = {Abstract Stepwise regression is both a general term and a specific method for choosing predictor variables from a larger pool of possible predictors in multiple regression. Related approaches include forward selection and backward elimination. All three procedures may identify different and suboptimal sets of predictors for a given sample of data. Some regression packages can identify the best possible one-predictor model, two-predictor model, and so on, according to the criteria specified by the user. This approach is computer intensive but is feasible with sets of at least 50 or 60 predictors. However they are selected, the best set of predictors in one sample will often not be the best in another sample or in the underlying population. Significance tests and R2 values may be far more liberal than their nominal values. Some of the estimated coefficients are also likely to be substantially overstated relative to their population values. Researchers using stepwise regression procedures will generally get better results when they have large samples of observations and relatively small sets of potential predictors; when they consider multiple procedures and criteria for selecting predictors; when they cross-validate findings with new data or with random subsamples from the available data; and when they avoid self-deception in interpreting the findings.}
}

@article{polynomial-chaos-1938,
    ISSN = {00029327, 10806377},
    URL = {http://www.jstor.org/stable/2371268},
    author = {Norbert Wiener},
    journal = {American Journal of Mathematics},
    number = {4},
    pages = {897--936},
    publisher = {Johns Hopkins University Press},
    title = {The Homogeneous Chaos},
    urldate = {2023-02-05},
    volume = {60},
    year = {1938},
    doi = {10.2307/2371268}
}

@Inbook{polynomial-chaos-1991,
    author="Ghanem, Roger G.
    and Spanos, Pol D.",
    title="Stochastic Finite Element Method: Response Statistics",
    bookTitle="Stochastic Finite Elements: A Spectral Approach",
    year="1991",
    publisher="Springer New York",
    address="New York, NY",
    pages="101--119",
    abstract="An important objective of a stochastic finite element analysis of an engineering system should be the determination of a set of design criteria which can be implemented in a probabilistic context. In other words, it is important that any method that is used for the numerical treatment of stochastic systems be compatible with some rationale that permits a reliability analysis. Traditionally, this problem has been addressed by relying on the second order statistical moments of the response process. These moments offer useful statistical information regarding the response of the system and equations will be developed in an ensuing section for their determination. However, second order moments do not suffice for a complete reliability analysis. For this, higher order moments and related information are required. In this context, the response of any system to a certain excitation may be viewed as a point in the space defined by the parameters describing the system. In other words, for every set of such parameters, the response is uniquely determined as a function of the independent variables. When these parameters are regarded as random variables, the response function may be described as a point in the space spanned by these variables, defining a surface that corresponds to all possible realizations of the random parameters.",
    isbn="978-1-4612-3094-6",
    doi="10.1007/978-1-4612-3094-6_4",
    url="https://doi.org/10.1007/978-1-4612-3094-6_4"
}

@article{ccn-size-2006,
    author = {U. Dusek  and G. P. Frank  and L. Hildebrandt  and J. Curtius  and J. Schneider  and S. Walter  and D. Chand  and F. Drewnick  and S. Hings  and D. Jung  and S. Borrmann  and M. O. Andreae },
    title = {Size Matters More Than Chemistry for Cloud-Nucleating Ability of Aerosol Particles},
    journal = {Science},
    volume = {312},
    number = {5778},
    pages = {1375--1378},
    year = {2006},
    doi = {10.1126/science.1125261},
    URL = {https://www.science.org/doi/abs/10.1126/science.1125261},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.1125261},
    abstract = {Size-resolved cloud condensation nuclei (CCN) spectra measured for various aerosol types at a non-urban site in Germany showed that CCN concentrations are mainly determined by the aerosol number size distribution. Distinct variations of CCN activation with particle chemical composition were observed but played a secondary role. When the temporal variation of chemical effects on CCN activation is neglected, variation in the size distribution alone explains 84 to 96\% of the variation in CCN concentrations. Understanding that particles' ability to act as CCN is largely controlled by aerosol size rather than composition greatly facilitates the treatment of aerosol effects on cloud physics in regional and global models.}
}

@inproceedings{ood-svm-1999,
    author = {Sch\"{o}lkopf, Bernhard and Williamson, Robert and Smola, Alex and Shawe-Taylor, John and Platt, John},
    title = {Support Vector Method for Novelty Detection},
    year = {1999},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    abstract = {Suppose you are given some dataset drawn from an underlying probability distribution P and you want to estimate a "simple" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified ν between 0 and 1.We propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm.The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.},
    booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
    pages = {582--588},
    numpages = {7},
    location = {Denver, CO},
    series = {NIPS'99},
    url = {https://dl.acm.org/doi/10.5555/3009657.3009740},
    urldate = {2023-02-10},
    eprint = {http://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf}
}

@article{scikit-learn-2011,
    author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
    title = {Scikit-Learn: Machine Learning in Python},
    year = {2011},
    issue_date = {2/1/2011},
    publisher = {JMLR.org},
    volume = {12},
    issn = {1532-4435},
    abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
    journal = {J. Mach. Learn. Res.},
    month = {11},
    pages = {2825--2830},
    numpages = {6},
    url = {https://dl.acm.org/doi/10.5555/1953048.2078195},
    urldate = {2023-02-10}
}

@inproceedings{sklearn-api-2013,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
  doi = {10.48550/arXiv.1309.0238},
}

@INPROCEEDINGS{isolation-forest-2008,
    author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
    booktitle={2008 Eighth IEEE International Conference on Data Mining}, 
    title={Isolation Forest}, 
    year={2008},
    volume={},
    number={},
    pages={413--422},
    abstract={Most existing model-based approaches to anomaly detection construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and random forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.},
    keywords={},
    doi={10.1109/ICDM.2008.17},
    ISSN={2374-8486},
    month={12},
}

@article{isolation-forest-2012,
    author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
    title = {Isolation-Based Anomaly Detection},
    year = {2012},
    issue_date = {March 2012},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {6},
    number = {1},
    issn = {1556-4681},
    url = {https://doi.org/10.1145/2133360.2133363},
    doi = {10.1145/2133360.2133363},
    abstract = {Anomalies are data points that are few and different. As a result of these properties, we show that, anomalies are susceptible to a mechanism called isolation. This article proposes a method called Isolation Forest (iForest), which detects anomalies purely based on the concept of isolation without employing any distance or density measure---fundamentally different from all existing methods.As a result, iForest is able to exploit subsampling (i) to achieve a low linear time-complexity and a small memory-requirement and (ii) to deal with the effects of swamping and masking effectively. Our empirical evaluation shows that iForest outperforms ORCA, one-class SVM, LOF and Random Forests in terms of AUC, processing time, and it is robust against masking and swamping effects. iForest also works well in high dimensional problems containing a large number of irrelevant attributes, and when anomalies are not available in training sample.},
    journal = {ACM Trans. Knowl. Discov. Data},
    month = {03},
    articleno = {3},
    numpages = {39},
    keywords = {random tree ensemble, isolation forest, outlier detection, binary tree, isolation, Anomaly detection, ensemble methods}
}

@article{uq-review-2021,
    title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
    journal = {Information Fusion},
    volume = {76},
    pages = {243--297},
    year = {2021},
    issn = {1566-2535},
    doi = {10.1016/j.inffus.2021.05.008},
    url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
    author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
    keywords = {Artificial intelligence, Uncertainty quantification, Deep learning, Machine learning, Bayesian statistics, Ensemble learning},
    abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.}
}

@INPROCEEDINGS{uncertainty-disentanglement-2022,
    author={Valdenegro-Toro, Matias and Mori, Daniel Saromo},
    booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
    title={A Deeper Look into Aleatoric and Epistemic Uncertainty Disentanglement}, 
    year={2022},
    volume={},
    number={},
    pages={1508--1516},
    abstract={Neural networks are ubiquitous in many tasks, but trusting their predictions is an open issue. Uncertainty quantification is required for many applications, and disentangled aleatoric and epistemic uncertainties are best. In this paper, we generalize methods to produce disentangled uncertainties to work with different uncertainty quantification methods, and evaluate their capability to produce disentangled uncertainties. Our results show that: there is an interaction between learning aleatoric and epistemic uncertainty, which is unexpected and violates assumptions on aleatoric uncertainty, some methods like Flipout produce zero epistemic uncertainty, aleatoric uncertainty is unreliable in the out-of-distribution setting, and Ensembles provide overall the best disentangling quality. We also explore the error produced by the number of samples hyper-parameter in the sampling softmax function, recommending N > 100 samples. We expect that our formulation and results help practitioners and researchers choose uncertainty methods and expand the use of disentangled uncertainties, as well as motivate additional research into this topic.},
    keywords={},
    doi={10.1109/CVPRW56347.2022.00157},
    ISSN={2160-7516},
    month={06}
}

@inproceedings{bayesian-deep-uncertainty-2017,
    author = {Kendall, Alex and Gal, Yarin},
    title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {5580--5590},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}

@INPROCEEDINGS{nll-loss-1994,
    author={Nix, D.A. and Weigend, A.S.},
    booktitle={Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)}, 
    title={Estimating the mean and variance of the target probability distribution}, 
    year={1994},
    volume={1},
    number={},
    pages={55--60},
    abstract={Introduces a method that estimates the mean and the variance of the probability distribution of the target as a function of the input, given an assumed target error-distribution model. Through the activation of an auxiliary output unit, this method provides a measure of the uncertainty of the usual network output for each input pattern. The authors derive the cost function and weight-update equations for the example of a Gaussian target error distribution, and demonstrate the feasibility of the network on a synthetic problem where the true input-dependent noise level is known.},
    keywords={},
    doi={10.1109/ICNN.1994.374138},
    ISSN={},
    month={06}
}

@misc{reliable-variance-2019,
    doi = {10.48550/ARXIV.1906.03260},
    url = {https://arxiv.org/abs/1906.03260},
    author = {Detlefsen, Nicki S. and Jørgensen, Martin and Hauberg, Søren},
    keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Reliable training and estimation of variance networks},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{variational-variance-2020,
    doi = {10.48550/ARXIV.2006.04910},
    url = {https://arxiv.org/abs/2006.04910},
    author = {Stirn, Andrew and Knowles, David A.},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Variational Variance: Simple, Reliable, Calibrated Heteroscedastic Noise Variance Parameterization},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{beta-nll-2022,
    doi = {10.48550/ARXIV.2203.09168},
    url = {https://arxiv.org/abs/2203.09168},
    author = {Seitzer, Maximilian and Tavakoli, Arash and Antic, Dimitrije and Martius, Georg},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{ccn-sources-2009,
    AUTHOR = {Merikanto, J. and Spracklen, D. V. and Mann, G. W. and Pickering, S. J. and Carslaw, K. S.},
    TITLE = {Impact of nucleation on global CCN},
    JOURNAL = {Atmospheric Chemistry and Physics},
    VOLUME = {9},
    YEAR = {2009},
    NUMBER = {21},
    PAGES = {8601--8616},
    URL = {https://acp.copernicus.org/articles/9/8601/2009/},
    DOI = {10.5194/acp-9-8601-2009}
}

@Inbook{sparse-bayesian-1996,
    author="MacKay, David J. C.",
    editor="Heidbreder, Glenn R.",
    title="Bayesian Non-Linear Modeling for the Prediction Competition",
    bookTitle="Maximum Entropy and Bayesian Methods: Santa Barbara, California, U.S.A., 1993",
    year="1996",
    publisher="Springer Netherlands",
    address="Dordrecht",
    pages="221--234",
    abstract="The 1993 energy prediction competition involved the prediction of a series of building energy loads from a series of environmental input variables. Non-linear regression using `neural networks' is a popular technique for such modeling tasks. Since it is not obvious how large a time-window of inputs is appropriate, or what preprocessing of inputs is best, this can be viewed as a regression problem in which there are many possible input variables, some of which may actually be irrelevant to the prediction of the output variable. Because a finite data set will show random correlations between the irrelevant inputs and the output, any conventional neural network (even with regularisation or `weight decay') will not set the coefficients for these junk inputs to zero. Thus the irrelevant variables will hurt the model's performance.",
    isbn="978-94-015-8729-7",
    doi="10.1007/978-94-015-8729-7_18",
    url="https://doi.org/10.1007/978-94-015-8729-7_18"
}

@misc{svgp-2013,
    doi = {10.48550/ARXIV.1309.6835},
    url = {https://arxiv.org/abs/1309.6835},
    author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Gaussian Processes for Big Data},
    publisher = {arXiv},
    year = {2013},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{color-cet-2015,
    doi = {10.48550/ARXIV.1509.03700},
    url = {https://arxiv.org/abs/1509.03700},
    author = {Kovesi, Peter},
    keywords = {Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences, I.3.3},
    title = {Good Colour Maps: How to Design Them},
    publisher = {arXiv},
    year = {2015},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

 @misc{color-cet-2023,
    title = {Perceptually Uniform Colour Maps},
    url = {https://colorcet.com/},
    journal = {Color CET},
    publisher = {Centre for Exploration Targeting, The University of Western Australia},
    author = {Kovesi, Peter},
    urldate = {2023-02-28}
}

@misc{netcdf-1989,
    doi = {10.5065/D6H70CW6},
    url = {http://www.unidata.ucar.edu/software/netcdf/},
    author = {Rew,  Russ and Davis,  Glenn and Emmerson,  Steve and Cormack,  Cathy and Caron,  John and Pincus,  Robert and Hartnett,  Ed and Heimbigner,  Dennis and Appel,  Lynton and Fisher,  Ward},
    keywords = {Software,  Free computer software,  Open source software,  Systems software,  Application program interfaces,  Metadata},
    title = {Unidata NetCDF},
    publisher = {UCAR/NCAR - Unidata},
    year = {1989},
    copyright = {UCAR/Unidata Open Source Software License}
}

@inproceedings{nystroem-fourier-2012,
    title = "Nystr{\"o}m method vs random Fourier features: A theoretical and empirical comparison",
    abstract = "Both random Fourier features and the Nystr{\"o}m method have been successfully applied to efficient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features where the basis functions (i.e., cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystr{\"o}m method are randomly sampled from the training examples and are therefore data dependent. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystr{\"o}m method can yield impressively better generalization error bound than random Fourier features based approach. We empirically verify our theoretical findings on a wide range of large data sets.",
    author = "Tianbao Yang and Li, {Yu Feng} and Mehrdad Mahdavi and Rong Jin and Zhou, {Zhi Hua}",
    year = "2012",
    isbn = "9781627480031",
    series = "Advances in Neural Information Processing Systems",
    pages = "476--484",
    booktitle = "Advances in Neural Information Processing Systems 25",
    note = "26th Annual Conference on Neural Information Processing Systems 2012, NIPS 2012 ; Conference date: 03-12-2012 Through 06-12-2012",
    url = "https://proceedings.neurips.cc/paper/2012/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf",
    urldate = "2023-03-01"
}

@inproceedings{nystroem-kernel-2000,
    author = {Williams, Christopher K. I. and Seeger, Matthias},
    title = {Using the Nystr\"{o}m Method to Speed up Kernel Machines},
    year = {2000},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    abstract = {A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n3), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystr\"{o}m method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m &lt; n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m2n). We report experiments on the USPS and abalone data sets and show that we can set m ≪ n without any significant decrease in the accuracy of the solution.},
    booktitle = {Proceedings of the 13th International Conference on Neural Information Processing Systems},
    pages = {661--667},
    numpages = {7},
    location = {Denver, CO},
    series = {NIPS'00},
    url = {https://proceedings.neurips.cc/paper/2000/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf},
    urldate = {2023-03-01}
}

@article{uncertainty-metrics-2023,
    title = {Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons},
    journal = {Journal of Computational Physics},
    volume = {477},
    pages = {111902},
    year = {2023},
    issn = {0021-9991},
    doi = {10.1016/j.jcp.2022.111902},
    url = {https://www.sciencedirect.com/science/article/pii/S0021999122009652},
    author = {Apostolos F. Psaros and Xuhui Meng and Zongren Zou and Ling Guo and George Em Karniadakis},
    keywords = {Scientific machine learning, Stochastic partial differential equations, Uncertainty quantification, Physics-informed neural networks, Neural operator learning, Bayesian framework},
    abstract = {Neural networks (NNs) are currently changing the computational paradigm on how to combine data with mathematical laws in physics and engineering in a profound way, tackling challenging inverse and ill-posed problems not solvable with traditional methods. However, quantifying errors and uncertainties in NN-based inference is more complicated than in traditional methods. This is because in addition to aleatoric uncertainty associated with noisy data, there is also uncertainty due to limited data, but also due to NN hyperparameters, overparametrization, optimization and sampling errors as well as model misspecification. Although there are some recent works on uncertainty quantification (UQ) in NNs, there is no systematic investigation of suitable methods towards quantifying the total uncertainty effectively and efficiently even for function approximation, and there is even less work on solving partial differential equations and learning operator mappings between infinite-dimensional function spaces using NNs. In this work, we present a comprehensive framework that includes uncertainty modeling, new and existing solution methods, as well as evaluation metrics and post-hoc improvement approaches. To demonstrate the applicability and reliability of our framework, we present an extensive comparative study in which various methods are tested on prototype problems, including problems with mixed input-output data, and stochastic problems in high dimensions. In the Appendix, we include a comprehensive description of all the UQ methods employed. Further, to help facilitate the deployment of UQ in Scientific Machine Learning research and practice, we present and develop in [1] an open-source Python library (github.com/Crunch-UQ4MI/neuraluq), termed NeuralUQ, that is accompanied by an educational tutorial and additional computational experiments.}
}

@article{uncertainty-sharpness-2007,
    author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
    title = {Probabilistic forecasts, calibration and sharpness},
    journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
    volume = {69},
    number = {2},
    pages = {243--268},
    keywords = {Cross-validation, Density forecast, Ensemble prediction system, Ex post evaluation, Forecast verification, Model diagnostics, Posterior predictive assessment, Predictive distribution, Prequential principle, Probability integral transform, Proper scoring rule},
    doi = {10.1111/j.1467-9868.2007.00587.x},
    url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00587.x},
    eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2007.00587.x},
    abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
    year = {2007}
}

@InProceedings{uncertainty-calibration-2018,
    title = {Accurate Uncertainties for Deep Learning Using Calibrated Regression},
    author = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
    booktitle = {Proceedings of the 35th International Conference on Machine Learning},
    pages = {2796--2804},
    year = {2018},
    editor = {Dy, Jennifer and Krause, Andreas},
    volume = {80},
    series = {Proceedings of Machine Learning Research},
    month = {06},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v80/kuleshov18a/kuleshov18a.pdf},
    url = {https://proceedings.mlr.press/v80/kuleshov18a.html},
    urldate = {2023-03-09},
    abstract = {Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90\% credible interval may not contain the true outcome 90\% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.}
}

@misc{not-crude-uncertainty-2020,
    doi = {10.48550/ARXIV.2005.12496},
    url = {https://arxiv.org/abs/2005.12496},
    author = {Zelikman, Eric and Healy, Christopher and Zhou, Sharon and Avati, Anand},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {CRUDE: Calibrating Regression Uncertainty Distributions Empirically},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{uncertainty-dispersion-2019,
    doi = {10.48550/ARXIV.1905.11659},
    url = {https://arxiv.org/abs/1905.11659},
    author = {Levi, Dan and Gispan, Liran and Giladi, Niv and Fetaya, Ethan},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Evaluating and Calibrating Uncertainty Prediction in Regression Tasks},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@manual{python-3.7,
    title={Python: A dynamic, open source programming language},
    author={{Python Core Team}},
    organization={{Python Software Foundation}},
    year={2019},
    url={https://www.python.org/},
    note={Python version 3.7}
}

@software{f90nml-1.4,
    author  = {Marshall Ward and
                    HoWol76 and
                    James Penn and
                    Daniel S. Katz and
                    jenssss and
                    Huziy Oleksandr and
                    Martin Dix and
                    Dalon Work and
                    naught101 and
                    Andrew Kiss and
                    barpaum and
                    Maik Riechert and
                    Michael Lamparski and
                    Pascal Hebbeker and
                    Warrick Ball and
                    Mattia Almansi},
    title        = {marshallward/f90nml},
    month        = {05},
    year         = {2022},
    publisher    = {Zenodo},
    version      = {v1.4.1},
    doi          = {10.5281/zenodo.6598825},
    url          = {https://doi.org/10.5281/zenodo.6598825}
}

@article{regression-quantiles-1978,
    ISSN = {00129682, 14680262},
    URL = {http://www.jstor.org/stable/1913643},
    doi = {10.2307/1913643},
    abstract = {A simple minimization problem yielding the ordinary sample quantiles in the location model is shown to generalize naturally to the linear model generating a new class of statistics we term "regression quantiles." The estimator which minimizes the sum of absolute residuals is an important special case. Some equivariance properties and the joint asymptotic distribution of regression quantiles are established. These results permit a natural generalization of the linear model of certain well-known robust estimators of location. Estimators are suggested, which have comparable efficiency to least squares for Gaussian linear models while substantially out-performing the least-squares estimator over a wide class of non-Gaussian error distributions.},
    author = {Roger Koenker and Gilbert Bassett},
    journal = {Econometrica},
    number = {1},
    pages = {33--50},
    publisher = {[Wiley, Econometric Society]},
    title = {Regression Quantiles},
    urldate = {2023-03-26},
    volume = {46},
    year = {1978}
}

@article{deep-quantiles-2022,
    title = "Deep Quantile Regression for Uncertainty Estimation in Unsupervised and Supervised Lesion Detection",
    author = "Akrami, Haleh and Joshi, Anand and Aydore, Sergul and Leahy, Richard",
    journal = "Machine Learning for Biomedical Imaging",
    volume = "1",
    issue = "IPMI 2021 special issue",
    year = "2022",
    pages = "1--23",
    issn = "2766-905X",
    url = "https://melba-journal.org/2022:008",
    doi = "10.48550/arXiv.2109.09374"
}

@misc{interval-minimisation-2018,
      title={Tight Prediction Intervals Using Expanded Interval Minimization}, 
      author={Dongqi Su and Ying Yin Ting and Jason Ansel},
      year={2018},
      eprint={1806.11222},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      doi = {10.48550/arXiv.1806.11222}
}

@article{clrm-assuptions-1971,
    ISSN = {00202754, 14755661},
    doi = {10.2307/621706},
    URL = {http://www.jstor.org/stable/621706},
    abstract = {The paper is prompted by certain apparent deficiences both in the discussion of the regression model in instructional sources for geographers and in the actual empirical application of the model by geographical writers. In the first part of the paper the assumptions of the two regression models, the 'fixed X' and the 'random X', are outlined in detail, and the relative importance of each of the assumptions for the variety of purposes for which regression analysis may be employed is indicated. Where any of the critical assumptions of the model are seriously violated, variations on the basic model must be used and these are reviewed in the second half of the paper.},
    author = {Michael A. Poole and Patrick N. O'Farrell},
    journal = {Transactions of the Institute of British Geographers},
    number = {52},
    pages = {145--158},
    publisher = {[Royal Geographical Society (with the Institute of British Geographers), Wiley]},
    title = {The Assumptions of the Linear Regression Model},
    urldate = {2023-03-27},
    year = {1971}
}

@article{bootstrapping-1979,
    author = {B. Efron},
    title = {{Bootstrap Methods: Another Look at the Jackknife}},
    volume = {7},
    journal = {The Annals of Statistics},
    number = {1},
    publisher = {Institute of Mathematical Statistics},
    pages = {1--26},
    abstract = {We discuss the following problem: given a random sample $\mathbf{X} = (X_1, X_2, \cdots, X_n)$ from an unknown probability distribution $F$, estimate the sampling distribution of some prespecified random variable $R(\mathbf{X}, F)$, on the basis of the observed data $\mathbf{x}$. (Standard jackknife theory gives an approximate mean and variance in the case $R(\mathbf{X}, F) = \theta(\hat{F}) - \theta(F), \theta$ some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
    keywords = {bootstrap, discriminant analysis, error rate estimation, jackknife, Nonlinear regression, nonparametric variance estimation, Resampling, subsample values},
    year = {1979},
    doi = {10.1214/aos/1176344552},
    URL = {https://doi.org/10.1214/aos/1176344552}
}

@article{pizza-optimisation-2018,
    author = {Motevalizadeh, Ehsan and Mortazavi, Seyed Ali and Milani, Elnaz and Hooshmand-Dalir, Moosa Al-Reza},
    title = {Optimization of physicochemical and textural properties of pizza cheese fortified with soybean oil and carrot extract},
    journal = {Food Science \& Nutrition},
    volume = {6},
    number = {2},
    pages = {356--372},
    keywords = {carrot extract, optimization, physicochemical and textural properties, pizza cheese},
    doi = {10.1002/fsn3.563},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/fsn3.563},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/fsn3.563},
    abstract = {Abstract Response surface methodology (RSM) was used to optimize pizza cheese containing carrot extract. The effects of two important independent variables including soybean oil (5\%–20\%) and carrot extract (5\%–20\%) were studied on physicochemical and textural properties of pizza cheese containing carrot extract. According to the results, RSM was successfully used for optimizing formulation of pizza cheese containing carrot juice. Results of this study revealed that oil (A), carrot (B), AB, square term of carrot (B2), B, AB, square term of oil (A2), B2, AB, AB, A2B, A2, A2, A, A2, A2, AB, and AB2 had the most effect on moisture, acidity, stretch, L*, a*, b*, hardness, meltability, springiness, peroxide value (PV), cohesiveness, chewiness, gumminess, fracture force, adhesiveness force, stiffness, flavor, and overall acceptability, respectively. A formulation upon 20\% oil and 10.88\% carrot extract was found as the optimal formulation for pizza cheese containing carrot extract. At the optimal formulation, PV, L*, a*, b*, meltability, stretch, cohesiveness, springiness, gumminess, chewiness, adhesive force, flavor, texture, and overall acceptability at the optimum formulation were measured 2.23, 82.51, −3.69, 18.05, 17.86, 85.61, 0.41, 7.874, 23.7, 0.27, 0.61, 3.50, 3.95, and 3.65, respectively.},
    year = {2018}
}

@article{response-surface-modelling-1951,
    author = {Box, G. E. P. and Wilson, K. B.},
    title = {On the Experimental Attainment of Optimum Conditions},
    journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
    volume = {13},
    number = {1},
    pages = {1--38},
    doi = {10.1111/j.2517-6161.1951.tb00067.x},
    url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1951.tb00067.x},
    eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1951.tb00067.x},
    year = {1951}
}

@article{anesthesia-rsm-2015,
    title = {Response surface models in the field of anesthesia: A crash course},
    journal = {Acta Anaesthesiologica Taiwanica},
    volume = {53},
    number = {4},
    pages = {139--145},
    year = {2015},
    issn = {1875-4597},
    doi = {10.1016/j.aat.2015.06.005},
    url = {https://www.sciencedirect.com/science/article/pii/S1875459715000569},
    author = {Jing-Yang Liou and Mei-Yung Tsou and Chien-Kun Ting},
    keywords = {drug interaction, isobole, pharmacodynamics, response surface model},
    abstract = {Drug interaction is fundamental in performing anesthesia. A response surface model (RSM) is a very useful tool for investigating drug interactions. The methodology appeared many decades ago, but did not receive attention in the field of anesthesia until the 1990s. Drug investigations typically start with pharmacokinetics, but it is the effects on the body clinical anesthesiologists really care about. Typically, drug interactions are divided into additive, synergistic, or infra-additive. Traditional isobolographic analysis or concentration-effect curve shifts are limited to a single endpoint. Response surface holds the complete package of isobolograms and concentration effect curves in one equation for a given endpoint, e.g., loss of response to laryngoscopy. As a pharmacodynamic tool, RSM helps anesthesiologists guide their drug therapy by navigating the surface. We reviewed the most commonly used models: (1) the Greco model; (2) Reduced Greco model; (3) Minto model; and (4) the Hierarchy models. Each one has its unique concept and strengths. These models served as groundwork for researchers to modify the formula to fit their drug of interest. RSM usually work with two drugs, but three-drug models can be constructed at the expense of greatly increasing the complexity. A wide range of clinical applications are made possible with the help of pharmacokinetic simulation. Pharmacokinetic-pharmcodynamic modeling using the RSMs gives anesthesiologists the versatility to work with precision and safe drug interactions. Currently, RSMs have been used for predicting patient responses, estimating wake up time, pinpointing the optimal drug concentration, guide therapy with respect to patient's well-being, and aid in procedures that require rapid patient arousal such as awake craniotomy or Stagnara wake-up test. There is no other model that is universally better than the others. Researches are encouraged to find the best fitting model for different occasions with an objective measure.}
}

@article{subway-temperature-rsm-2019,
    title = {Response-surface-model based on influencing factor analysis of subway tunnel temperature},
    journal = {Building and Environment},
    volume = {160},
    pages = {106140},
    year = {2019},
    issn = {0360-1323},
    doi = {10.1016/j.buildenv.2019.05.018},
    url = {https://www.sciencedirect.com/science/article/pii/S0360132319303361},
    author = {Yue Zhang and Xiaofeng Li},
    keywords = {Subway environment, Tunnel temperature, Simulation, Response surface model, Design of experiment},
    abstract = {Subway tunnel temperature has the significant effects on the thermal environment in both tunnel and public station area. Taking good control of this parameter is meaningful for operating safely and effectively. However, the simulation of subway tunnel temperature is both complex and time-consuming. In this research, tunnel temperature was studied by numerical simulation. The three influencing factors concerning internal heat generation (passenger number, carriage weight, and Regenerative Braking System (RBS) efficiency), and two factors concerning transmission load (soil conductivity and thermal capacity) were analyzed. In order to figure out the overall effect of these five factors, a Response Surface Model (RSM) of the subway tunnel temperature was established by Box-Behnken design, and it is found that the response value of each experiment setup fits well with the simulated result. According to the RSM, if the passenger number, carriage weight, RBS efficiency, soil conductivity, and soil thermal capacity change from the possible minimum to maximum values, the tunnel temperature increases 3.34 °C, 1.76 °C, 9.26 °C, 1.02 °C, and 0.46 °C, respectively. Therefore, the passenger number and carriage weight have a positive impact on the tunnel temperature; the RBS efficiency, soil conductivity, and thermal capacity have a negative impact. RBS efficiency plays the most important role among the five factors. The response-surface model is an approximated mathematical model with multi-factors, it provides a quick way of predicting subway tunnel temperature in different situations, which could be helpful in both design and operation stages in subway.}
}

@Article{geothermal-rsm-2016,
    author={Ansari, Esmail and Hughes, Richard},
    title={Response surface method for assessing energy production from geopressured geothermal reservoirs},
    journal={Geothermal Energy},
    year={2016},
    month={11},
    day={02},
    volume={4},
    number={1},
    pages={15},
    abstract={Developing low-enthalpy geothermal resources along the US Gulf Coast is attractive for reducing global warming and providing clean energy. In this work, synthetic yet representative models for typical geopressured geothermal reservoirs located along the US Gulf Coast are considered. A Box--Behnken experimental design is used to select a small set of these models to perform detailed reservoir simulation runs. Full quadratic linear models are fit to the simulation results, and their sufficiency is confirmed by comparing them to kriging response surfaces. To achieve a higher degree of efficiency in using the response surfaces, Hammersley sequence sampling (HSS) method is used instead of traditional Monte Carlo sampling. HSS ensures that the factor space is sampled more uniformly and the response distribution is converged in less time. By evaluating these proxy models in the sampled factor space, the sensitivity and uncertainty of the response to the factors can be assessed. In this work, the sensitivity and uncertainty of engineered convection is assessed. For quantifying engineered convection, five uncertain reservoir attributes were selected. The response was defined as the net extracted enthalpy. In particular, two different designs for harvesting energy from geothermal reservoirs were compared using the response surfaces. In the modeled systems, results show that the regular design is more effective than the reverse design for extracting energy from geopressured geothermal reservoirs.},
    issn={2195-9706},
    doi={10.1186/s40517-016-0057-5},
    url={https://doi.org/10.1186/s40517-016-0057-5}
}

@article{ml-trends-2021,
    title = {Machine learning-based approach: global trends, research directions, and regulatory standpoints},
    journal = {Data Science and Management},
    volume = {4},
    pages = {19--29},
    year = {2021},
    issn = {2666-7649},
    doi = {10.1016/j.dsm.2021.12.002},
    url = {https://www.sciencedirect.com/science/article/pii/S2666764921000485},
    author = {Raffaele Pugliese and Stefano Regondi and Riccardo Marini},
    keywords = {Machine learning, Artificial intelligence, Research trends, Healthcare, Data governance, Nanotechnology, Cybersecurity},
    abstract = {The field of machine learning (ML) is sufficiently young that it is still expanding at an accelerating pace, lying at the crossroads of computer science and statistics, and at the core of artificial intelligence (AI) and data science. Recent progress in ML has been driven both by the development of new learning algorithms theory, and by the ongoing explosion in the availability of vast amount of data (often referred to as "big data") and low-cost computation. The adoption of ML-based approaches can be found throughout science, technology and industry, leading to more evidence-based decision-making across many walks of life, including healthcare, biomedicine, manufacturing, education, financial modeling, data governance, policing, and marketing. Although the past decade has witnessed the increasing interest in these fields, we are just beginning to tap the potential of these ML algorithms for studying systems that improve with experience. In this paper, we present a comprehensive view on geo worldwide trends (taking into account China, the USA, Israel, Italy, the UK, and the Middle East) of ML-based approaches highlighting the rapid growth in the last 5 years attributable to the introduction of related national policies. Furthermore, based on the literature review, we also discuss the potential research directions in this field, summarizing some popular application areas of machine learning technology, such as healthcare, cyber-security systems, sustainable agriculture, data governance, and nanotechnology, and suggest that the "dissemination of research" in the ML scientific community has undergone the exceptional growth in the time range of 2018–2020, reaching a value of 16,339 publications. Finally, we report the challenges and the regulatory standpoints for managing ML technology. Overall, we hope that this work will help to explain the geo trends of ML approaches and their applicability in various real-world domains, as well as serve as a reference point for both academia and industry professionals, particularly from a technical, ethical and regulatory point of view.}
}

@Article{ml-applications-2021,
    author={Sarker, Iqbal H.},
    title={Machine Learning: Algorithms, Real-World Applications and Research Directions},
    journal={SN Computer Science},
    year={2021},
    month={03},
    day={22},
    volume={2},
    number={3},
    pages={160},
    abstract={In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study's key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers in various real-world situations and application areas, particularly from the technical point of view.},
    issn={2661-8907},
    doi={10.1007/s42979-021-00592-x},
    url={https://doi.org/10.1007/s42979-021-00592-x}
}

@article{simulation-ml-2022,
    title = {Data-centric Engineering: integrating simulation, machine learning and statistics. Challenges and opportunities},
    journal = {Chemical Engineering Science},
    volume = {249},
    pages = {117271},
    year = {2022},
    issn = {0009-2509},
    doi = {10.1016/j.ces.2021.117271},
    url = {https://www.sciencedirect.com/science/article/pii/S0009250921008368},
    author = {Indranil Pan and Lachlan R. Mason and Omar K. Matar},
    keywords = {Digital twins, Artificial Intelligence, CFD, FEM, Data-centric Engineering, SimOps},
    abstract = {Recent advances in machine learning, coupled with low-cost computation, availability of cheap streaming sensors, data storage and cloud technologies, has led to widespread multi-disciplinary research activity with significant interest and investment from commercial stakeholders. Mechanistic models, based on physical equations, and purely data-driven statistical approaches represent two ends of the modelling spectrum. New hybrid, data-centric engineering approaches, leveraging the best of both worlds and integrating both simulations and data, are emerging as a powerful tool with a transformative impact on the physical disciplines. We review the key research trends and application scenarios in the emerging field of integrating simulations, machine learning, and statistics. We highlight the opportunities that such an integrated vision can unlock and outline the key challenges holding back its realisation. We also discuss the bottlenecks in the translational aspects of the field and the long-term upskilling requirements for the existing workforce and future university graduates.}
}

@article{virtual-laboratories-2022,
    author = "Arto Klami and Theodoros Damoulas and Ola Engkvist and Patrick Rinke and Samuel Kaski",
    title = "Virtual Laboratories: Transforming research with AI",
    year = "2022",
    month = "8",
    url = "https://www.techrxiv.org/articles/preprint/Virtual_Laboratories_Transforming_research_with_AI/20412540",
    doi = "10.36227/techrxiv.20412540.v1"
}

@article{ml-bias-discrimination-2017,
    author = {Michael Veale and Reuben Binns},
    title ={Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data},
    journal = {Big Data \& Society},
    volume = {4},
    number = {2},
    pages = {2053951717743530},
    year = {2017},
    doi = {10.1177/2053951717743530},
    URL = {https://doi.org/10.1177/2053951717743530},
    eprint = {https://doi.org/10.1177/2053951717743530},
    abstract = { Decisions based on algorithmic, machine learning models can be unfair, reproducing biases in historical data used to train them. While computational techniques are emerging to address aspects of these concerns through communities such as discrimination-aware data mining (DADM) and fairness, accountability and transparency machine learning (FATML), their practical implementation faces real-world challenges. For legal, institutional or commercial reasons, organisations might not hold the data on sensitive attributes such as gender, ethnicity, sexuality or disability needed to diagnose and mitigate emergent indirect discrimination-by-proxy, such as redlining. Such organisations might also lack the knowledge and capacity to identify and manage fairness issues that are emergent properties of complex sociotechnical systems. This paper presents and discusses three potential approaches to deal with such knowledge and information deficits in the context of fairer machine learning. Trusted third parties could selectively store data necessary for performing discrimination discovery and incorporating fairness constraints into model-building in a privacy-preserving manner. Collaborative online platforms would allow diverse organisations to record, share and access contextual and experiential knowledge to promote fairness in machine learning systems. Finally, unsupervised learning and pedagogically interpretable algorithms might allow fairness hypotheses to be built for further selective testing and exploration. Real-world fairness challenges in machine learning are not abstract, constrained optimisation problems, but are institutionally and contextually grounded. Computational fairness tools are useful, but must be researched and developed in and with the messy contexts that will shape their deployment, rather than just for imagined situations. Not doing so risks real, near-term algorithmic harm. }
}

@misc{prediction-inequity-2019,
    title={Predictive Inequity in Object Detection}, 
    author={Benjamin Wilson and Judy Hoffman and Jamie Morgenstern},
    year={2019},
    eprint={1902.11097},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    doi={10.48550/arXiv.1902.11097}
}

@misc{aversarial-attacks-2015,
    title={Explaining and Harnessing Adversarial Examples}, 
    author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
    year={2015},
    eprint={1412.6572},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    doi={10.48550/arXiv.1412.6572}
}

@article{hydrology-ml-2021,
    title = {Embedding machine learning techniques into a conceptual model to improve monthly runoff simulation: A nested hybrid rainfall-runoff modeling},
    journal = {Journal of Hydrology},
    volume = {598},
    pages = {126433},
    year = {2021},
    issn = {0022-1694},
    doi = {10.1016/j.jhydrol.2021.126433},
    url = {https://www.sciencedirect.com/science/article/pii/S0022169421004807},
    author = {Umut Okkan and Zeynep Beril Ersoy and Ahmet {Ali Kumanlioglu} and Okan Fistikoglu},
    keywords = {Conceptual rainfall-runoff modeling, Machine learning techniques, Nested hybrid models, Coupled models, Automatic calibration, Gediz river basin},
    abstract = {One of the frequently adopted hybridizations within the scope of rainfall-runoff modeling rests on directing various outputs simulated from the conceptual rainfall-runoff (CRR) models to machine learning (ML) techniques. In those coupled model exercises, after the parameter calibrations of the CRR models are made, their specific outputs constitute auxiliary inputs for the ML model training. However, in this parallel hybridization comprising two consecutive processes, performing the cascade calibration of CRR and ML models increases the computational complexity. Moreover, the mutual interaction between the parameters governing CRR and ML models is also not considered. In this study, to cope with the handicaps mentioned, artificial neural networks (ANN) and support vector regression (SVR) were separately embedded into a monthly lumped CRR model. The dynamic water balance model (dynwbm) was preferred as the CRR model. Then, all free parameters within these nested hybrid models were calibrated simultaneously. The ML parts within the nested schemes manipulate various output variants derived with three conceptual parameters for monthly runoff simulation. These new hybrid models equipped with an automatic calibration algorithm were applied at several locations in the Gediz River Basin of western Turkey. The performance measures regarding mean and high flows indicated that the nested hybrid models outperformed the standalone models (i.e., dynwbm, ANN, and SVR) and coupled model variants. Thus, the credibility of a novel modeling strategy, which takes advantage of the supplementary strengths of a conceptual model and different ML techniques, was demonstrated.}
}

@Article{hybrid-model-2022,
    AUTHOR = {Slater, L. and Arnal, L. and Boucher, M.-A. and Chang, A. Y.-Y. and Moulds, S. and Murphy, C. and Nearing, G. and Shalev, G. and Shen, C. and Speight, L. and Villarini, G. and Wilby, R. L. and Wood, A. and Zappa, M.},
    TITLE = {Hybrid forecasting: using statistics and machine learning to integrate predictions from dynamical models},
    JOURNAL = {Hydrology and Earth System Sciences Discussions},
    VOLUME = {2022},
    YEAR = {2022},
    PAGES = {1--35},
    URL = {https://hess.copernicus.org/preprints/hess-2022-334/},
    DOI = {10.5194/hess-2022-334}
}

@Article{biological-dnn-2021,
    author={Elmarakeby, Haitham A.
    and Hwang, Justin
    and Arafeh, Rand
    and Crowdis, Jett
    and Gang, Sydney
    and Liu, David
    and AlDubayan, Saud H.
    and Salari, Keyan
    and Kregel, Steven
    and Richter, Camden
    and Arnoff, Taylor E.
    and Park, Jihye
    and Hahn, William C.
    and Van Allen, Eliezer M.},
    title={Biologically informed deep neural network for prostate cancer discovery},
    journal={Nature},
    year={2021},
    month={10},
    day={01},
    volume={598},
    number={7880},
    pages={348--352},
    abstract={The determination of molecular features that mediate clinically aggressive phenotypes in prostate cancer remains a major biological and clinical challenge1,2. Recent advances in interpretability of machine learning models as applied to biomedical problems may enable discovery and prediction in clinical cancer genomics3--5. Here we developed P-NET---a biologically informed deep learning model---to stratify patients with prostate cancer by treatment-resistance state and evaluate molecular drivers of treatment resistance for therapeutic targeting through complete model interpretability. We demonstrate that P-NET can predict cancer state using molecular data with a performance that is superior to other modelling approaches. Moreover, the biological interpretability within P-NET revealed established and novel molecularly altered candidates, such as MDM4 and FGFR1, which were implicated in predicting advanced disease and validated in vitro. Broadly, biologically informed fully interpretable neural networks enable preclinical discovery and clinical prediction in prostate cancer and may have general applicability across cancer types.},
    issn={1476-4687},
    doi={10.1038/s41586-021-03922-4},
    url={https://doi.org/10.1038/s41586-021-03922-4}
}

@article{neural-architecture-search-2021,
    doi = {10.1088/2632-2153/ac3ffa},
    url = {https://dx.doi.org/10.1088/2632-2153/ac3ffa},
    year = {2021},
    month = {12},
    publisher = {IOP Publishing},
    volume = {3},
    number = {1},
    pages = {015013},
    author = {M F Kasim and D Watson-Parris and L Deaconu and S Oliver and P Hatfield and D H Froula and G Gregori and M Jarvis and S Khatiwala and J Korenaga and J Topp-Mugglestone and E Viezzer and S M Vinko},
    title = {Building high accuracy emulators for scientific simulations with deep neural architecture search},
    journal = {Machine Learning: Science and Technology},
    abstract = {Computer simulations are invaluable tools for scientific discovery. However, accurate simulations are often slow to execute, which limits their applicability to extensive parameter exploration, large-scale data analysis, and uncertainty quantification. A promising route to accelerate simulations by building fast emulators with machine learning requires large training datasets, which can be prohibitively expensive to obtain with slow simulations. Here we present a method based on neural architecture search to build accurate emulators even with a limited number of training data. The method successfully emulates simulations in 10 scientific cases including astrophysics, climate science, biogeochemistry, high energy density physics, fusion energy, and seismology, using the same super-architecture, algorithm, and hyperparameters. Our approach also inherently provides emulator uncertainty estimation, adding further confidence in their use. We anticipate this work will accelerate research involving expensive simulations, allow more extensive parameters exploration, and enable new, previously unfeasible computational discovery.}
}

@article{salib-2.0-2022,
    title = {Toward {SALib} 2.0: {Advancing} the accessibility and interpretability of global sensitivity analyses},
    volume = {4},
    url = {https://sesmo.org/article/view/18155},
    doi = {10.18174/sesmo.18155},
    journal = {Socio-Environmental Systems Modelling},
    author = {Iwanaga, Takuya and Usher, William and Herman, Jonathan},
    month = {05},
    year = {2022},
    pages = {18155},
}

@article{salib-2017,
    doi = {10.21105/joss.00097},
    url = {https://doi.org/10.21105/joss.00097},
    year  = {2017},
    month = {01},
    publisher = {The Open Journal},
    volume = {2},
    number = {9},
    author = {Jon Herman and Will Usher},
    title = {{SALib}: An open-source Python library for Sensitivity Analysis},
    journal = {The Journal of Open Source Software}
}

@article{tapenade-autodiff-2013,
    author = {Hascoet, Laurent and Pascual, Val\'{e}rie},
    title = {The Tapenade Automatic Differentiation Tool: Principles, Model, and Specification},
    year = {2013},
    issue_date = {April 2013},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {39},
    number = {3},
    issn = {0098-3500},
    url = {https://doi.org/10.1145/2450153.2450158},
    doi = {10.1145/2450153.2450158},
    abstract = {Tapenade is an Automatic Differentiation (AD) tool which, given a Fortran or C code that computes a function, creates a new code that computes its tangent or adjoint derivatives. Tapenade puts particular emphasis on adjoint differentiation, which computes gradients at a remarkably low cost. This article describes the principles of Tapenade, a subset of the general principles of AD. We motivate and illustrate with examples the AD model of Tapenade, that is, the structure of differentiated codes and the strategies used to make them more efficient. Along with this informal description, we formally specify this model by means of data-flow equations and rules of Operational Semantics, making this the reference specification of the tangent and adjoint modes of Tapenade. One benefit we expect from this formal specification is the capacity to formally study the AD model itself, especially for the adjoint mode and its sophisticated strategies. This article also describes the architectural choices of the implementation of Tapenade. We describe the current performance of Tapenade on a set of codes that include industrial-size applications. We present the extensions of the tool that are planned in a foreseeable future, deriving from our ongoing research on AD.},
    journal = {ACM Trans. Math. Softw.},
    month = {05},
    articleno = {20},
    numpages = {43},
    keywords = {adjoint compiler, Source transformation}
}

@inproceedings{sobolev-training-2017,
    author = {Czarnecki, Wojciech Marian and Osindero, Simon and Jaderberg, Max and Swirszcz, Grzegorz and Pascanu, Razvan},
    title = {Sobolev Training for Neural Networks},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {At the heart of deep learning we aim to use neural networks as function approxi-mators - training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input - for example when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {4281--4290},
    numpages = {10},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}

@article{metropolis-algorithm-1953,
    author = {Metropolis,Nicholas  and Rosenbluth,Arianna W.  and Rosenbluth,Marshall N.  and Teller,Augusta H.  and Teller,Edward },
    title = {Equation of State Calculations by Fast Computing Machines},
    journal = {The Journal of Chemical Physics},
    volume = {21},
    number = {6},
    pages = {1087--1092},
    year = {1953},
    doi = {10.1063/1.1699114},
    URL = {https://doi.org/10.1063/1.1699114},
    eprint = {https://doi.org/10.1063/1.1699114}
}

@article{metropolis-hastings-1970,
    author = {Hastings, W. K.},
    title = "Monte Carlo sampling methods using Markov chains and their applications",
    journal = {Biometrika},
    volume = {57},
    number = {1},
    pages = {97--109},
    year = {1970},
    month = {04},
    abstract = "A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.",
    issn = {0006-3444},
    doi = {10.1093/biomet/57.1.97},
    url = {https://doi.org/10.1093/biomet/57.1.97},
    eprint = {https://academic.oup.com/biomet/article-pdf/57/1/97/23940249/57-1-97.pdf},
}

 @incollection{icarus-1845,
    title={Icarus falling},
    author={Nanteuil, C\'elestin},
    year={1845},
    url={https://digitalcollections.nypl.org/items/92963b31-9658-9d74-e040-e00a18061361},
    urldate={2023-03-30},
    booktitle={The Miriam and Ira D. Wallach Division of Art, Prints and Photographs: Print Collection},
    publisher={The New York Public Library},
}

@incollection{prudent-dictionary-2023,
    title={Meaning of prudent in English},
    author={{Cambridge University Press}},
    year={2023},
    url={https://dictionary.cambridge.org/dictionary/english/prudent},
    urldate={2023-03-31},
    booktitle={Cambridge Academic Content Dictionary},
}

@article{autodiff-applications-2021,
    author = {Christopher Rackauckas},
    journal = {The Winnower},
    title = {Generalizing Automatic Differentiation to Automatic Sparsity, Uncertainty, Stability, and Parallelism},
    year = {2021},
    month = {05},
    url = {https://dx.doi.org/10.15200/winn.162133.38896},
    doi = {10.15200/winn.162133.38896},
    eprint = {https://www.stochasticlifestyle.com/generalizing-automatic-differentiation-to-automatic-sparsity-uncertainty-stability-and-parallelism/}
}

@misc{correlated-propagation-2016,
    doi = {10.48550/ARXIV.1610.08716},
    url = {https://arxiv.org/abs/1610.08716},
    author = {Giordano, Mos\`e},
    keywords = {Data Analysis, Statistics and Probability (physics.data-an), FOS: Physical sciences, FOS: Physical sciences},
    title = {Uncertainty propagation with functionally correlated quantities},
    publisher = {arXiv},
    year = {2016},
    copyright = {Creative Commons Attribution 4.0 International}
}

@software{uncertainties-python-2022,
    author  = {Eric O. LEBIGOT},
    title        = {lebigot/uncertainties},
    month        = {06},
    year         = {2022},
    publisher    = {GitHub},
    version      = {v3.1.7},
    url          = {https://github.com/lebigot/uncertainties},
    urldate      = {2023-04-01}
}

@ARTICLE{scipy-2020,
    author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
    title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
    journal = {Nature Methods},
    year    = {2020},
    volume  = {17},
    pages   = {261--272},
    adsurl  = {https://rdcu.be/b08Wh},
    doi     = {10.1038/s41592-019-0686-2},
}

@Article{numpy-2020,
    title         = {Array programming with {NumPy}},
    author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                    van der Walt and Ralf Gommers and Pauli Virtanen and David
                    Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                    Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                    and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                    Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                    R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                    G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                    Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                    Travis E. Oliphant},
    year          = {2020},
    month         = {09},
    journal       = {Nature},
    volume        = {585},
    number        = {7825},
    pages         = {357--362},
    doi           = {10.1038/s41586-020-2649-2},
    publisher     = {Springer Science and Business Media {LLC}},
    url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@Article{matplotlib-2007,
    Author    = {Hunter, J. D.},
    Title     = {Matplotlib: A 2D graphics environment},
    Journal   = {Computing in Science \& Engineering},
    Volume    = {9},
    Number    = {3},
    Pages     = {90--95},
    abstract  = {Matplotlib is a 2D graphics package used for Python for
    application development, interactive scripting, and publication-quality
    image generation across user interfaces and operating systems.},
    publisher = {IEEE COMPUTER SOC},
    doi       = {10.1109/MCSE.2007.55},
    year      = 2007
}

@software{pandas-software-2020,
    author       = {The pandas development team},
    title        = {pandas-dev/pandas: Pandas},
    month        = {02},
    year         = 2020,
    publisher    = {Zenodo},
    version      = {latest},
    doi          = {10.5281/zenodo.3509134},
    url          = {https://doi.org/10.5281/zenodo.3509134}
}

@InProceedings{pandas-2010,
    author    = { {W}es {M}c{K}inney },
    title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
    booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
    pages     = { 56--61 },
    year      = { 2010 },
    editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
    doi       = { 10.25080/Majora-92bf1922-00a }
}

@Article{neocognitron-1980,
    author={Fukushima, Kunihiko},
    title={Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
    journal={Biological Cybernetics},
    year={1980},
    month={04},
    day={01},
    volume={36},
    number={4},
    pages={193--202},
    abstract={A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
    issn={1432-0770},
    doi={10.1007/BF00344251},
    url={https://doi.org/10.1007/BF00344251}
}

@article{cnn-1990,
    author = {Wei Zhang and Kazuyoshi Itoh and Jun Tanida and Yoshiki Ichioka},
    journal = {Appl. Opt.},
    keywords = {Computer simulation; Fourier transforms; Matched filtering; Neural networks; Spatial light modulators; Synthetic discrimination functions},
    number = {32},
    pages = {4790--4797},
    publisher = {Optica Publishing Group},
    title = {Parallel distributed processing model with local space-invariant interconnections and its optical architecture},
    volume = {29},
    month = {11},
    year = {1990},
    url = {https://opg.optica.org/ao/abstract.cfm?URI=ao-29-32-4790},
    doi = {10.1364/AO.29.004790},
    abstract = {This paper proposes a parallel distributed processing model with local space-invariant interconnections, which is more readily implemented by optics and is able to classify patterns correctly, even if they have been shifted or distorted. Error backpropagation is used as a training algorithm. Computer simulation results presented indicate that the processing is effective and the network can deal with the shifted or distorted patterns. Moreover, the optical implementation architecture using matched filters for the model is discussed.},
}

@article{alexnet-2017,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    year = {2017},
    issue_date = {June 2017},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {60},
    number = {6},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3065386},
    doi = {10.1145/3065386},
    abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
    journal = {Commun. ACM},
    month = {05},
    pages = {84--90},
    numpages = {7}
}


@Article{python-ml-2020,
    AUTHOR = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
    TITLE = {Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence},
    JOURNAL = {Information},
    VOLUME = {11},
    YEAR = {2020},
    NUMBER = {4},
    ARTICLE-NUMBER = {193},
    URL = {https://www.mdpi.com/2078-2489/11/4/193},
    ISSN = {2078-2489},
    ABSTRACT = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.},
    DOI = {10.3390/info11040193}
}

@misc{tensorflow-whitepaper-2015,
    title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    note={Software available from tensorflow.org},
    author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
    year={2015},
    doi={10.48550/arXiv.1603.04467}
}

@misc{keras-2015,
    title={Keras},
    author={Chollet, Fran\c{c}ois and others},
    year={2015},
    publisher={GitHub},
    url={https://github.com/keras-team/keras},
    urldate={2023-04-02}
}

@inbook{pytorch-2019,
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    year = {2019},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
    booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
    articleno = {721},
    numpages = {12},
    url = {https://dl.acm.org/doi/10.5555/3454287.3455008},
    urldate = {2023-04-02}
}

@misc{tensorflow-distributions-2017,
    title={TensorFlow Distributions}, 
    author={Joshua V. Dillon and Ian Langmore and Dustin Tran and Eugene Brevdo and Srinivas Vasudevan and Dave Moore and Brian Patton and Alex Alemi and Matt Hoffman and Rif A. Saurous},
    year={2017},
    eprint={1711.10604},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    doi={10.48550/arXiv.1711.10604}
}

@ARTICLE{gpflow-2017,
    author = {Matthews, Alexander G. de G. and {van der Wilk}, Mark and Nickson, Tom and
    Fujii, Keisuke. and {Boukouvalas}, Alexis and {Le{\'o}n-Villagr{\'a}}, Pablo and
    Ghahramani, Zoubin and Hensman, James},
    title = {{GP}flow: A {G}aussian process library using {T}ensor{F}low},
    journal = {Journal of Machine Learning Research},
    year    = {2017},
    month = {04},
    volume  = {18},
    number  = {40},
    pages   = {1--6},
    url     = {http://jmlr.org/papers/v18/16-537.html},
    urldate = {2023-04-02}
}

@article{gpflow-2020,
    author = {{van der Wilk}, Mark and Dutordoir, Vincent and John, ST and
            Artemev, Artem and Adam, Vincent and Hensman, James},
    title = {A Framework for Interdomain and Multioutput {G}aussian Processes},
    year = {2020},
    journal = {arXiv:2003.01115},
    url = {https://arxiv.org/abs/2003.01115},
    doi = {10.48550/arXiv.2003.01115}
}

@article{rio-2019,
    doi = {10.48550/ARXIV.1906.00588},
    url = {https://arxiv.org/abs/1906.00588},
    author = {Qiu, Xin and Meyerson, Elliot and Miikkulainen, Risto},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@Inbook{global-sensitivity-2015,
    author = {Iooss, Bertrand and Lema{\^i}tre, Paul},
    editor = {Dellino, Gabriella and Meloni, Carlo},
    title = {A Review on Global Sensitivity Analysis Methods},
    bookTitle = {Uncertainty Management in Simulation-Optimization of Complex Systems: Algorithms and Applications},
    year = {2015},
    publisher = {Springer US},
    address = {Boston, MA},
    pages = {101--122},
    abstract = {This chapter makes a review, in a complete methodological framework, of various global sensitivity analysis methods of model output. Numerous statistical and probabilistic tools (regression, smoothing, tests, statistical learning, Monte Carlo, {\ldots}) aim at determining the model input variables which mostly contribute to an interest quantity depending on model output. This quantity can be for instance the variance of an output variable. Three kinds of methods are distinguished: the screening (coarse sorting of the most influential inputs among a large number), the measures of importance (quantitative sensitivity indices) and the deep exploration of the model behaviour (measuring the effects of inputs on their all variation range). A progressive application methodology is illustrated on a scholar application. A synthesis is given to place every method according to several axes, mainly the cost in number of model evaluations, the model complexity and the nature of brought information.},
    isbn = {978-1-4899-7547-8},
    doi = {10.1007/978-1-4899-7547-8_5},
    url = {https://doi.org/10.1007/978-1-4899-7547-8_5}
}

@article{air-quality-sensitivity-2003,
    author = {Hakami, Amir and Odman, M. Talat and Russell, Armistead G.},
    title = {High-Order, Direct Sensitivity Analysis of Multidimensional Air Quality Models},
    journal = {Environmental Science \& Technology},
    volume = {37},
    number = {11},
    pages = {2442--2452},
    year = {2003},
    doi = {10.1021/es020677h},
    note = {PMID: 12831030},
    URL = {https://doi.org/10.1021/es020677h},
    eprint = {https://doi.org/10.1021/es020677h}
}

@article{stats-proofs-2022,
    title={The Book of Statistical Proofs},
    DOI={10.5281/zenodo.5820411},
    publisher={Zenodo},
    author={Joram Soch and Thomas J. Faulkenberry and Kenneth Petrykowski and Carsten Allefeld and Ciarán D. McInerney},
    year={2022},
    month={01}
}

@Article{adifor-1992,
    author={Bischof, Christian and Carle, Alan and Corliss, George and Griewank, Andreas and Hovland, Paul},
    title={ADIFOR--Generating Derivative Codes from Fortran Programs},
    journal={Scientific Programming},
    year={1992},
    month={01},
    day={01},
    publisher={IOS Press},
    volume={1},
    pages={717832},
    abstract={The numerical methods employed in the solution of many scientific computing problems require the computation of derivatives of a function f R<sup>n</sup>{\&}{\#}x2192;R<sup>m</sup>. Both the accuracy and the computational requirements of the derivative computation are usually of critical importance for the robustness and speed of the numerical solution. Automatic Differentiation of FORtran (ADIFOR) is a source transformation tool that accepts Fortran 77 code for the computation of a function and writes portable Fortran 77 code for the computation of the derivatives. In contrast to previous approaches, ADIFOR views automatic differentiation as a source transformation problem. ADIFOR employs the data analysis capabilities of the ParaScope Parallel Programming Environment, which enable us to handle arbitrary Fortran 77 codes and to exploit the computational context in the computation of derivatives. Experimental results show that ADIFOR can handle real-life codes and that ADIFOR-generated codes are competitive with divided-difference approximations of derivatives. In addition, studies suggest that the source transformation approach to automatic differentiation may improve the time to compute derivatives by orders of magnitude.},
    issn={1058-9244},
    doi={10.1155/1992/717832},
    url={https://doi.org/10.1155/1992/717832}
}

@article{autodiff-taxonomu-1991,
    title = {A taxonomy of automatic differentiation tools},
    author = {Juedes, D W},
    abstractNote = {Many of the current automatic differentiation (AD) tools have similar characteristics. Unfortunately, the similarities between these various AD tools often cannot be easily ascertained by reading the corresponding documentation. To clarify this situation, a taxonomy of AD tools is presented. The taxonomy places AD tools into the Elemental, Extensional, Integral, Operational, and Symbolic classes. This taxonomy is used to classify twenty-nine AD tools. Each tool is examined individually with respect to the mode of differentiation used and the degree of derivatives computed. A list detailing the availability of the surveyed AD tools is provided in the Appendix. 54 refs., 3 figs., 1 tab.},
    url = {https://www.osti.gov/biblio/5015838},
    place = {United States},
    year = {1991},
    month = {1},
    urldate = {2023-04-03}
}


@misc{modeling-toolkit-2021,
    doi = {10.48550/ARXIV.2103.05244},
    url = {https://arxiv.org/abs/2103.05244},
    author = {Ma, Yingbo and Gowda, Shashi and Anantharaman, Ranjan and Laughman, Chris and Shah, Viral and Rackauckas, Chris},
    keywords = {Mathematical Software (cs.MS), Symbolic Computation (cs.SC), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {ModelingToolkit: A Composable Graph Transformation System For Equation-Based Modeling},
    publisher = {arXiv},
    year = {2021},
    copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@Article{pm-suicide-risk-2022,
    author={Hwang, In Young
    and Choi, Daein
    and Kim, Jihoon Andrew
    and Choi, Seulggie
    and Chang, Jooyoung
    and Goo, Ae Jin
    and Ko, Ahryoung
    and Lee, Gyeongsil
    and Kim, Kyae Hyung
    and Son, Joung Sik
    and Park, Sang Min},
    title={Association of short-term particulate matter exposure with suicide death among major depressive disorder patients: a time-stratified case-crossover analysis},
    journal={Scientific Reports},
    year={2022},
    month={05},
    day={19},
    volume={12},
    number={1},
    pages={8471},
    abstract={There is growing evidence that suggests a potential association between particulate matter (PM) and suicide. However, it is unclear that PM exposure and suicide death among major depressive disorder (MDD) patients, a high-risk group for suicide. We aimed to assess the effect of short-term exposure to PM on the risk of suicide in MDD patients who are at high risk for suicide. We investigated the risk of suicide among 922,062 newly-diagnosed MDD patients from 2004 to 2017 within the Korean National Health Insurance Service (NHIS) database. We identified 3,051 suicide cases from January 1, 2015, to December 31, 2017, within the death statistics database of the Korean National Statistical Office. PMs with aerodynamic diameter less than 2.5 $\mu$m (PM2.5), less than 10 $\mu$m (PM10), and 2.5--10 $\mu$m (PM2.5--10) were considered, which were provided from the National Ambient Air Monitoring System in South Korea. Time-stratified case-crossover analysis was performed to investigate the association of particulate matter exposure to suicide events. The risk of suicide was significantly high upon the high level of exposure to PM2.5, PM2.5--10 (coarse particle) and PM10 on lag 1 (p for trend{\thinspace}<{\thinspace}0.05). Short-term exposure to a high level of PM was associated with an elevated risk for suicide among MDD patients. There is a clear dose--response relationship between short-term PM exposures with suicide death among MDD patients. This result will be used as an essential basis for consideration when establishing an air pollution alarm system for reducing adverse health outcomes by PM.},
    issn={2045-2322},
    doi={10.1038/s41598-022-12421-z},
    url={https://doi.org/10.1038/s41598-022-12421-z}
}

@article{pm-depression-risk-2022,
    title = {Air pollution exposure and depression: A comprehensive updated systematic review and meta-analysis},
    journal = {Environmental Pollution},
    volume = {292},
    pages = {118245},
    year = {2022},
    issn = {0269-7491},
    doi = {10.1016/j.envpol.2021.118245},
    url = {https://www.sciencedirect.com/science/article/pii/S0269749121018273},
    author = {Elisa Borroni and Angela Cecilia Pesatori and Valentina Bollati and Massimiliano Buoli and Michele Carugno},
    keywords = {Depression, Air pollution, Particulate matter, Systematic review, Meta-analysis},
    abstract = {We provide a comprehensive and updated systematic review and meta-analysis of the association between air pollution exposure and depression, searching PubMed, Embase, and Web of Sciences for relevant articles published up to May 2021, and eventually including 39 studies. Meta-analyses were performed separately according to pollutant type [particulate matter with diameter ≤10 μm (PM10) and ≤2.5 μm (PM2.5), nitrogen dioxide (NO2), sulfur dioxide (SO2), ozone (O3), and carbon monoxide (CO)] and exposure duration [short- (<30 days) and long-term (≥30 days)]. Test for homogeneity based on Cochran's Q and I2 statistics were calculated and the restricted maximum likelihood (REML) random effect model was applied. We assessed overall quality of pooled estimates, influence of single studies on the meta-analytic estimates, sources of between-study heterogeneity, and publication bias. We observed an increased risk of depression associated with long-term exposure to PM2.5 (relative risk: 1.074, 95\% confidence interval: 1.021-1.129) and NO2 (1.037, 1.011-1.064), and with short-term exposure to PM10 (1.009, 1.006-1.012), PM2.5 (1.009, 1.007-1.011), NO2 (1.022, 1.012-1.033), SO2 (1.024, 1.010-1.037), O3 (1.011, 0.997-1.026), and CO (1.062, 1.020-1.105). The publication bias affecting half of the investigated associations and the high heterogeneity characterizing most of the meta-analytic estimates partly prevent to draw very firm conclusions. On the other hand, the coherence of all the estimates after excluding single studies in the sensitivity analysis supports the soundness of our results. This especially applies to the association between PM2.5 and depression, strengthened by the absence of heterogeneity and of relevant publication bias in both long- and short-term exposure studies. Should further investigations be designed, they should involve large sample sizes, well-defined diagnostic criteria for depression, and thorough control of potential confounding factors. Finally, studies dedicated to the comprehension of the mechanisms underlying the association between air pollution and depression remain necessary.}
}


@article{aerosol-health-2020,
    title={Human Inhalation Exposure to Aerosol and Health Effect: Aerosol Monitoring and Modelling Regional Deposited Doses},
    volume={17},
    ISSN={1660-4601},
    url={http://dx.doi.org/10.3390/ijerph17061923},
    DOI={10.3390/ijerph17061923},
    number={6},
    journal={International Journal of Environmental Research and Public Health},
    publisher={MDPI AG},
    author={Oh, Hyeon-Ju and Ma, Yoohan and Kim, Jongbok},
    year={2020},
    month={03},
    pages={1923}
}

@book{gp-ml-2005,
    author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
    title = {Gaussian Processes for Machine Learning},
    publisher = {The MIT Press},
    year = {2005},
    month = {11},
    abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
    isbn = {9780262256834},
    doi = {10.7551/mitpress/3206.001.0001},
    url = {https://gaussianprocess.org/gpml/},
    urldate = {2023-01-27}
}

@inproceedings{asvgp-2023,
    abstract = {A typical criticism of Gaussian processes is their unfavourable scaling in both compute and memory requirements. Sparse variational Gaussian processes based on inducing variables are commonly used to scale Gaussian processes to large dataset sizes; their inherent compute and memory requirements are dominated by the number of inducing variables used. However, in practise sparse GPs are still limited by the number of datapoints and the number of inducing points one can use to perform matrix operations, making it again challenging to model large complex datasets. In this work we propose a new class of inter-domain variational GP, constructed by projecting the GP onto a set of compactly supported B-Spline basis functions. The key benefit of our approach is that the compact support of the B-Spline basis admits the use of sparse linear algebra to significantly speed up matrix operations and drastically reduce the memory footprint.},
    author = {Jake Cunningham and Daniel de Souza and So Takao and Marc van der Wilk and Marc P. Deisenroth},
    booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics},
    date = {2023-04-27},
    title = {Actually Sparse Variational Gaussian Processes},
    year = {2023},
    url = {https://gp-seminar-series.github.io/neurips-2022/assets/camera_ready/48.pdf},
    urldate = {2023-04-03}
}

@INPROCEEDINGS{big-data-gp-2022,
    author={Jiang, Mengrui and Pedrielli, Giulia and Ng, Szu Hui},
    booktitle={2022 Winter Simulation Conference (WSC)}, 
    title={Gaussian Processes for High-Dimensional, Large Data Sets: A Review}, 
    year={2022},
    volume={},
    number={},
    pages={49--60},
    abstract={Gaussian processes, known to have versatile uses in several fields across engineering, science, economics, show important advantages to several alternative approaches while controlling model complexity. However, the use of this family of models is hindered for inputs that are high dimensional as well as large sample sizes due to the intractability of the likelihood function, and the growth of the variance covariance matrix. This article investigates state-of-art solutions to these challenges according classifying them into categories. The goal is to select several algorithms covering each category and perform empirical experiments to compare their performances on the same set of test functions. Our preliminary results focus on deterministic implementations of a set of selected approaches. The results of the experiments may serve as a guidance to future readers who want to study and use Gaussian process in problems with high dimensions and big data sets.},
    keywords={},
    doi={10.1109/WSC57314.2022.10015416},
    ISSN={1558-4305},
    month={12}
}

@book{machine-learning-1997,
    title={Machine Learning},
    author={Mitchell, T.M.},
    isbn={9780071154673},
    lccn={97007692},
    series={McGraw-Hill International Editions},
    year={1997},
    publisher={McGraw-Hill},
    url={https://www.cs.cmu.edu/~tom/mlbook.html},
    urldate={2023-04-05}
}

@article{ai-review-2008,
    author = {Oke, Sunday},
    year = {2008},
    month = {12},
    pages = {535--570},
    title = {A literature review on artificial intelligence},
    volume = {19},
    journal = {International Journal of Information and Management Sciences},
    url = {https://www.researchgate.net/publication/228618921_A_literature_review_on_artificial_intelligence#fullTextFileContent},
    urldate = {2023-04-05}
}

@Article{decision-trees-1986,
    author={Quinlan, J. R.},
    title={Induction of Decision Trees},
    journal={Machine Learning},
    year={1986},
    month={03},
    day={01},
    volume={1},
    number={1},
    pages={81--106},
    abstract={The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
    issn={1573-0565},
    doi={10.1023/A:1022643204877},
    url={https://doi.org/10.1023/A:1022643204877}
}

@INPROCEEDINGS{random-forests-1995,
    author={Tin Kam Ho},
    booktitle={Proceedings of 3rd International Conference on Document Analysis and Recognition}, 
    title={Random decision forests}, 
    year={1995},
    volume={1},
    pages={278--282},
    abstract={Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits.},
    doi={10.1109/ICDAR.1995.598994},
    month={08},
}

@book{statistical-learning-2009,
    title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition},
    author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
    isbn={9780387848587},
    lccn={2008941148},
    series={Springer Series in Statistics},
    url={https://books.google.fi/books?id=tVIjmNS3Ob8C},
    year={2009},
    publisher={Springer New York},
    url={https://hastie.su.domains/ElemStatLearn/},
    urldate={2023-04-06},
    doi={10.1007/978-0-387-84858-7}
}

@Article{continuous-tree-1992,
    author={Fayyad, Usama M. and Irani, Keki B.},
    title={On the handling of continuous-valued attributes in decision tree generation},
    journal={Machine Learning},
    year={1992},
    month={01},
    day={01},
    volume={8},
    number={1},
    pages={87--102},
    abstract={We present a result applicable to classification learning algorithms that generate decision trees or rules using the information entropy minimization heuristic for discretizing continuous-valued attributes. The result serves to give a better understanding of the entropy measure, to point out that the behavior of the information entropy heuristic possesses desirable properties that justify its usage in a formal sense, and to improve the efficiency of evaluating continuous-valued attributes for cut value selection. Along with the formal proof, we present empirical results that demonstrate the theoretically expected reduction in evaluation effort for training data sets from real-world domains.},
    issn={1573-0565},
    doi={10.1007/BF00994007},
    url={https://doi.org/10.1007/BF00994007}
}

@Article{ensemble-learning-2020,
    author={Dong, Xibin
    and Yu, Zhiwen
    and Cao, Wenming
    and Shi, Yifan
    and Ma, Qianli},
    title={A survey on ensemble learning},
    journal={Frontiers of Computer Science},
    year={2020},
    month={04},
    day={01},
    volume={14},
    number={2},
    pages={241--258},
    abstract={Despite significant successes achieved in knowledge discovery, traditional machine learning methods may fail to obtain satisfactory performances when dealing with complex data, such as imbalanced, high-dimensional, noisy data, etc. The reason behind is that it is difficult for these methods to capture multiple characteristics and underlying structure of data. In this context, it becomes an important topic in the data mining field that how to effectively construct an efficient knowledge discovery and mining model. Ensemble learning, as one research hot spot, aims to integrate data fusion, data modeling, and data mining into a unified framework. Specifically, ensemble learning firstly extracts a set of features with a variety of transformations. Based on these learned features, multiple learning algorithms are utilized to produce weak predictive results. Finally, ensemble learning fuses the informative knowledge from the above results obtained to achieve knowledge discovery and better predictive performance via voting schemes in an adaptive way. In this paper, we review the research progress of the mainstream approaches of ensemble learning and classify them based on different characteristics. In addition, we present challenges and possible research directions for each mainstream approach of ensemble learning, and we also give an extra introduction for the combination of ensemble learning with other machine learning hot spots such as deep learning, reinforcement learning, etc.},
    issn={2095-2236},
    doi={10.1007/s11704-019-8208-z},
    url={https://doi.org/10.1007/s11704-019-8208-z}
}

@inproceedings{ensemble-overfitting-1995,
    author = {Sollich, Peter and Krogh, Anders},
    title = {Learning with Ensembles: How over-Fitting Can Be Useful},
    year = {1995},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    abstract = {We study the characteristics of learning with ensembles. Solving exactly the simple model of an ensemble of linear students, we find surprisingly rich behaviour. For learning in large ensembles, it is advantageous to use under-regularized students, which actually over-fit the training data. Globally optimal performance can be obtained by choosing the training set sizes of the students appropriately. For smaller ensembles, optimization of the ensemble weights can yield significant improvements in ensemble generalization performance, in particular if the individual students are subject to noise in the training process. Choosing students with a wide range of regularization parameters makes this improvement robust against changes in the unknown level of noise in the training data.},
    booktitle = {Proceedings of the 8th International Conference on Neural Information Processing Systems},
    pages = {190--196},
    numpages = {7},
    location = {Denver, Colorado},
    series = {NIPS'95},
    url = {https://dl.acm.org/doi/10.5555/2998828.2998855},
    urldate = {2023-04-06},
}

@book{ml-pattern-2006,
    author = {Bishop, Christopher},
    title = {Pattern Recognition and Machine Learning},
    year = {2006},
    month = {01},
    abstract = {This leading textbook provides a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. This is the first machine learning textbook to include a comprehensive coverage of recent developments such as probabilistic graphical models and deterministic inference methods, and to emphasize a modern Bayesian perspective. It is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. This hard cover book has 738 pages in full colour, and there are 431 graded exercises.},
    publisher = {Springer},
    url = {https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/},
    urldate = {2023-04-08}
}

@Article{perceptron-1943,
    author={McCulloch, Warren S.
    and Pitts, Walter},
    title={A logical calculus of the ideas immanent in nervous activity},
    journal={The bulletin of mathematical biophysics},
    year={1943},
    month={12},
    day={01},
    volume={5},
    number={4},
    pages={115--133},
    abstract={Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
    issn={1522-9602},
    doi={10.1007/BF02478259},
    url={https://doi.org/10.1007/BF02478259}
}

@Article{mae-rmse-2005,
    author={Willmott, Cort J.
    and Matsuura, Kenji},
    title={Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance},
    journal={Climate Research},
    year={2005},
    volume={30},
    number={1},
    pages={79--82},
    abstract={ABSTRACT: The relative abilities of 2, dimensioned statistics{\&}{\#}151;the root-mean-square error (RMSE) and the mean absolute error (MAE){\&}{\#}151;to describe average model-performance error are examined. The RMSE is of special interest because it is widely reported in the climatic and environmental literature; nevertheless, it is an inappropriate and misinterpreted measure of average error. RMSE is inappropriate because it is a function of 3 characteristics of a set of errors, rather than of one (the average error). RMSE varies with the variability within the distribution of error magnitudes and with the square root of the number of errors (<I>n</I><SUP>1/2</SUP>), as well as with the average-error magnitude (MAE). Our findings indicate that MAE is a more natural measure of average error, and (unlike RMSE) is unambiguous. Dimensioned evaluations and inter-comparisons of average model-performance error, therefore, should be based on MAE.},
    doi={10.3354/cr030079},
    url={https://www.int-res.com/abstracts/cr/v30/n1/p79-82},
    url={https://doi.org/10.3354/cr030079}
}

@misc{gradient-descent-overview-2017,
    title={An overview of gradient descent optimization algorithms}, 
    author={Sebastian Ruder},
    year={2017},
    eprint={1609.04747},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    doi={10.48550/arXiv.1609.04747}
}

@INPROCEEDINGS{activation-functions-2020,
    author={Rasamoelina, Andrinandrasana David and Adjailia, Fouzia and Sinčák, Peter},
    booktitle={2020 IEEE 18th World Symposium on Applied Machine Intelligence and Informatics (SAMI)}, 
    title={A Review of Activation Function for Artificial Neural Network}, 
    year={2020},
    pages={281--286},
    abstract={The activation function plays an important role in the training and the performance of an Artificial Neural Network. They provide the necessary non-linear properties to any Artificial Neural Network. In this work, we provide a review of the most common and recent activation functions used in the hidden layer of an artificial Neural Network.},
    doi={10.1109/SAMI48414.2020.9108717},
    month={01}
}

@ARTICLE{deep-learning-2019,
    author={Shrestha, Ajay and Mahmood, Ausif},
    journal={IEEE Access}, 
    title={Review of Deep Learning Algorithms and Architectures}, 
    year={2019},
    volume={7},
    pages={53040--53065},
    abstract={Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.},
    keywords={},
    doi={10.1109/ACCESS.2019.2912200},
    ISSN={2169-3536}
}

@article{recurrent-nns-2019,
    author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
    title = {A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures},
    journal = {Neural Computation},
    volume = {31},
    number = {7},
    pages = {1235--1270},
    year = {2019},
    month = {07},
    abstract = {Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01199},
    url = {https://doi.org/10.1162/neco\_a\_01199},
    eprint = {https://direct.mit.edu/neco/article-pdf/31/7/1235/1053200/neco\_a\_01199.pdf},
}

@ARTICLE{cnn-review-2022,
    author={Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
    journal={IEEE Transactions on Neural Networks and Learning Systems}, 
    title={A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects}, 
    year={2022},
    volume={33},
    number={12},
    pages={6999--7019},
    abstract={A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN’s applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.},
    doi={10.1109/TNNLS.2021.3084827},
    ISSN={2162-2388},
    month={12},
}

@inproceedings{automl-2013,
    author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
    title = {Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms},
    year = {2013},
    isbn = {9781450321747},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2487575.2487629},
    doi = {10.1145/2487575.2487629},
    abstract = {Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
    booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {847--855},
    numpages = {9},
    keywords = {weka, model selection, hyperparameter optimization},
    location = {Chicago, Illinois, USA},
    series = {KDD '13}
}

@Inbook{auto-sklearn-2019,
    author="Feurer, Matthias
    and Klein, Aaron
    and Eggensperger, Katharina
    and Springenberg, Jost Tobias
    and Blum, Manuel
    and Hutter, Frank",
    editor="Hutter, Frank
    and Kotthoff, Lars
    and Vanschoren, Joaquin",
    title="Auto-sklearn: Efficient and Robust Automated Machine Learning",
    bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
    year="2019",
    publisher="Springer International Publishing",
    address="Cham",
    pages="113--134",
    abstract="The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on the Python machine learning package scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub Auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won six out of ten phases of the first ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of Auto-sklearn.",
    isbn="978-3-030-05318-5",
    doi="10.1007/978-3-030-05318-5_6",
    url="https://doi.org/10.1007/978-3-030-05318-5_6"
}

@misc{data-preprocessing-2007,
    author       = {S. B. Kotsiantis and
                    D. Kanellopoulos and
                    P. E. Pintelas},
    title        = {Data Preprocessing for Supervised Leaning},
    month        = {12},
    year         = {2007},
    publisher    = {Zenodo},
    version      = {14136},
    doi          = {10.5281/zenodo.1082415},
    url          = {https://doi.org/10.5281/zenodo.1082415}
}

@INPROCEEDINGS{feature-selection-2014,
    author={Khalid, Samina and Khalil, Tehmina and Nasreen, Shamila},
    booktitle={2014 Science and Information Conference}, 
    title={A survey of feature selection and feature extraction techniques in machine learning}, 
    year={2014},
    pages={372--378},
    abstract={Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection and feature extraction methods with respect to efficiency and effectiveness. In the field of machine learning and pattern recognition, dimensionality reduction is important area, where many approaches have been proposed. In this paper, some widely used feature selection and feature extraction techniques have analyzed with the purpose of how effectively these techniques can be used to achieve high performance of learning algorithms that ultimately improves predictive accuracy of classifier. An endeavor to analyze dimensionality reduction techniques briefly with the purpose to investigate strengths and weaknesses of some widely used dimensionality reduction methods is presented.},
    doi={10.1109/SAI.2014.6918213},
    month={08},
}

@article{pca-1901,
    author = {Karl Pearson F.R.S.},
    title = {LIII. On lines and planes of closest fit to systems of points in space},
    journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
    volume = {2},
    number = {11},
    pages = {559--572},
    year  = {1901},
    publisher = {Taylor & Francis},
    doi = {10.1080/14786440109462720},
    URL = {https://doi.org/10.1080/14786440109462720},
    eprint = {https://doi.org/10.1080/14786440109462720}
}

@InProceedings{kernel-pca-1997,
    author="Sch{\"o}lkopf, Bernhard
    and Smola, Alexander
    and M{\"u}ller, Klaus-Robert",
    editor="Gerstner, Wulfram
    and Germond, Alain
    and Hasler, Martin
    and Nicoud, Jean-Daniel",
    title="Kernel principal component analysis",
    booktitle="Artificial Neural Networks --- ICANN'97",
    year="1997",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="583--588",
    abstract="A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.",
    isbn="978-3-540-69620-9",
    doi="10.1007/BFb0020217"
}

@article{feature-pyramid-1984,
    title={Pyramid methods in image processing},
    author={Adelson, Edward H and Anderson, Charles H and Bergen, James R and Burt, Peter J and Ogden, Joan M},
    journal={RCA engineer},
    volume={29},
    number={6},
    pages={33--41},
    year={1984},
    url={https://www-bcs.mit.edu/pub_pdfs/RCA84.pdf},
    urldate={2023-04-08}
}

@InProceedings{ml-ensembles-2000,
    author="Dietterich, Thomas G.",
    title="Ensemble Methods in Machine Learning",
    booktitle="Multiple Classifier Systems",
    year="2000",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="1--15",
    abstract="Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",
    isbn="978-3-540-45014-6",
    doi="10.1007/3-540-45014-9_1"
}

@inproceedings{deep-ensembles-2017,
    author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
    title = {Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {6405--6416},
    numpages = {12},
    location = {Long Beach, California, USA},
    series = {NIPS'17},
    url = {https://dl.acm.org/doi/10.5555/3295222.3295387},
    urldate = {2023-04-10}
}

@article{dropout-2014,
    author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
    title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
    year = {2014},
    issue_date = {January 2014},
    publisher = {JMLR.org},
    volume = {15},
    number = {1},
    issn = {1532-4435},
    abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
    journal = {J. Mach. Learn. Res.},
    month = {01},
    pages = {1929--1958},
    numpages = {30},
    keywords = {deep learning, model combination, regularization, neural networks},
    url = {https://dl.acm.org/doi/abs/10.5555/2627435.2670313},
    urldate = {2023-04-10}
}

@misc{ensemble-uncertainty-2021,
    title={Ensemble-based Uncertainty Quantification: Bayesian versus Credal Inference}, 
    author={Mohammad Hossein Shaker and Eyke Hüllermeier},
    year={2021},
    eprint={2107.10384},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    doi={10.48550/arXiv.2107.10384}
}

@misc{ensemble-diversity-2015,
    title={Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks}, 
    author={Stefan Lee and Senthil Purushwalkam and Michael Cogswell and David Crandall and Dhruv Batra},
    year={2015},
    eprint={1511.06314},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    doi={10.48550/arXiv.1511.06314}
}

@article{ensemble-uncertainty-ood-2021,
    doi = {10.1088/2632-2153/ac0495},
    url = {https://dx.doi.org/10.1088/2632-2153/ac0495},
    year = {2021},
    month = {07},
    publisher = {IOP Publishing},
    volume = {2},
    number = {3},
    pages = {035030},
    author = {Lara Hoffmann and Ines Fortmeier and Clemens Elster},
    title = {Uncertainty quantification by ensemble learning for computational optical form measurements},
    journal = {Machine Learning: Science and Technology},
    abstract = {Uncertainty quantification by ensemble learning is explored in terms of an application known from the field of computational optical form measurements. The application requires solving a large-scale, nonlinear inverse problem. Ensemble learning is used to extend the scope of a recently developed deep learning approach for this problem in order to provide an uncertainty quantification of the solution to the inverse problem predicted by the deep learning method. By systematically inserting out-of-distribution errors as well as noisy data, the reliability of the developed uncertainty quantification is explored. Results are encouraging and the proposed application exemplifies the ability of ensemble methods to make trustworthy predictions on the basis of high-dimensional data in a real-world context.}
}

@software{matplotlib-adjust-text-2023,
    author       = {Ilya Flyamer and
                    Zhuyi Xue and
                    Colin and
                    Andy Li and
                    JasonMendoza2008 and
                    Josh L. Espinoza and
                    Nader Morshed and
                    Victor Vazquez and
                    Ryan Neff and
                    mski\_iksm and
                    scaine1},
    title        = {Phlya/adjustText},
    month        = {02},
    year         = {2023},
    publisher    = {Zenodo},
    version      = {0.8.0},
    doi          = {10.5281/zenodo.7648985},
    url          = {https://doi.org/10.5281/zenodo.7648985}
}

@misc{finite-difference-2022,
    title={Finite Difference Approximation},
    author={Jeffrey R. Chasnov},
    year={2022},
    month={07},
    day={18},
    publisher={Hong Kong University of Science and Technology},
    url={https://math.libretexts.org/@go/page/93752},
    urldate={2023-04-14}
}
