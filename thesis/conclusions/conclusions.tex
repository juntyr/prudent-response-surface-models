\chapter{Conclusions and Future Work} \label{txt:conclusions-chapter}

This thesis has explored how response surface models (RSMs) can be built and evaluated so that their predictions can be trusted and safely used in scientific research. As a first step, we have reviewed existing literature on response surface models (\Cref{txt:response-surface-models}), particularly in atmospheric sciences. Next, we have introduced a new dataset of SOSAA model trajectory runs (\Cref{txt:sosaa-data-chapter}) and evaluated several prediction models on this data (\Cref{txt:prediction-chapter}). Our experiments have reasserted that even when models perform very well on data that is similar to what they have been trained on, they may generalise poorly to unseen data with little warning.

\newpar With the motivation to make an RSM clearly communicate how confident and certain it is about a prediction, we have introduced a framework for building \textit{prudent} RSMs. In particular, we have proposed the Icarus RSM architecture (\Cref{txt:icarus-rsm}), which combines a prediction model with an uncertainty quantifier and an out-of-distribution (OOD) detector. The core principle of Icarus is to produce predictions with uncertainty levels that are conditioned on the RSM's confidence that an input is in-distribution and that making a valid prediction and uncertainty estimate is thus possible. We have also highlighted the possibility of propagating these uncertainty and confidence scores, an idea which is further explored below in \Cref{txt:future-icarus-propagation}.

We have explored the design space of OOD detectors by extensively reviewing prior work (\Cref{txt:novelty-detection}) and designing three easy-to-visualise toy examples (\Cref{txt:ood-detection-comparison}). We have compared different OOD detectors with these examples and found that auto-associative networks are a powerful method to learn which simplifying assumptions a predictor might make about the training data distribution. These assumptions define the boundaries of the conceptual in-distribution manifold (\Cref{txt:ood-detection-summary}). We have also explored how synthesising OOD samples can be used to train supervised classification methods for OOD detection (\Cref{txt:ood-synthesis-analysis}) and introduced $t$-poking and ID-OOD sample weighting as easy-to-implement improvements for these methods. Finally, we explored different methods to generate well-calibrated confidence scores.

Uncertainty Quantification is the third and final component of Icarus that we have explored in this thesis. In particular, we have reviewed several quantifiers (\Cref{txt:uncertainty-quantification}) and methods for evaluating them (\Cref{txt:uncertainty-methods}) from the literature. Our comparison shows that ensemble-based methods generally perform best, produce high correlations between the prediction error and uncertainty, and are statistically consistent.

Finally, we have combined our insights from exploring all components of Icarus individually to construct a proof-of-concept SOSAA RSM that is trained to predict the cloud condensation nuclei (CCN) concentration (\Cref{txt:icarus-evaluation-chapter}). We then stress-tested the implementation to gain insights into the promise and remaining limitations of implementing an Icarus-inspired RSM in practice. While a PADRE-RF-based design performed well on other SOSAA trajectory inputs and filtered out OOD inputs, we showed that the current implementation is insufficient to produce \textit{prudent} estimates on small input perturbations. To make SOSAA more accessible and enable easy experimentation with the prototype SOSAA RSM, we have also developed a graphical user interface that allows the user to configure, compile, and run the SOSAA model and to explore its results using the RSM.

\newpar This thesis has focused on building response surface models that can be safely integrated into scientific research. We have found that RSMs need to be \textit{prudent} and report their confidence and uncertainty to provide this safety and enable downstream use of the RSM predictions. We hope that our proposed Icarus architecture pushes these higher standards for RSMs and allows larger parts of the scientific community to exploit the opportunities of machine-learning-based RSMs.

Most importantly, however, we hope that this project empowers atmospheric science, conservation, and climate research. This interdisciplinary thesis has always been driven by the need to make modelling more accessible to decision-makers and the public to accelerate the transition to a sustainable future.

\section{Future Opportunities and Work}

Throughout this project, several ideas for how the Icarus architecture could be exploited for further benefit and how its components may be improved came up and were sometimes briefly looked into. However, we were unable to properly explore many of them since they either fell outside the scope of the feasibility study of the Icarus RSM architecture or did not fit within the allocated time schedule. This section presents the basics of these ideas, hoping that they inspire future applications and research.

\subsection{Improving the SOSAA RSM Components}

We have explored a variety of out-of-distribution detectors, prediction models, and uncertainty quantifiers in this thesis (see \Cref{txt:ood-detection-chapter}, \Cref{txt:prediction-chapter}, and \Cref{txt:uncertainty-chapter}). With the results from our experiments, when then constructed a baseline prototype for the SOSAA RSM in \Cref{txt:icarus-evaluation-chapter}. We expect that each of the components can be improved and hope that the toy cases and evaluation suites we have developed prove helpful in testing new implementations. In this subsection, we summarise the tips we can give from our research.

\subsubsection{Training and Evaluation Efficiency}

For building the prototype SOSAA RSM, we have focused on selecting cheap component implementations to train and evaluate. By keeping the computing time for both in the order of minutes or less, users can interact with the RSM to iteratively explore the response surface of SOSAA instead of resorting to asynchronous workflows. However, further reducing the time required to query the RSM may open up an even tighter feedback loop where a new trajectory run is fully approximated soon after the press of a button. Using simpler models, smaller ensembles, pre-trained models, or GPU-accelerated neural networks could all be explored for a real-time RSM. In the context of a user interface, such a low-fidelity version could provide a quick initial estimate, after which progressively more complex variants would be run to gradually give the user higher-quality results if they continue investigating this case.

\subsubsection{Chemically-Informed Prediction Model}

The prediction model used in the SOSAA RSM is a relatively general pairwise difference regression model (see \Cref{txt:padre-rf}) with a fixed architecture. The first option is to explore predictor architectures that are more specialised to the target domain. For instance, \textcite{biological-dnn-2021} have shown how a biologically informed deep neural network architecture can outperform generic methods at predicting prostate cancer. In particular, they train a network whose layers encode, in order from the input layer to the output layer, a patient's genetic profile, different genes, their combined pathways, biological processes, and finally, the patient outcome. Importantly, each neuron is assigned to a known process, and only those neurons are connected whose biological processes directly influence each other. These constraints make the learned neural network and its predictions immediately interpretable and make it easier for the network to deduce the underlying connections. A similar architecture but based on the chemical pathways present in SOSAA (which could be automatically generated based on the chemical mechanism specification that SOSAA uses) would be worth exploring.

\subsubsection{General Prediction Emulator}

The second option is to explore general models that also learn their own architecture. For example, \cite{neural-architecture-search-2021} have recently presented a super-architecture for deep neural networks. This architecture contains fifteen hidden layers whose size is parameterised by the dimension of the output signal. Each layer is connected to the next by an identity pass-through, a zero-layer, as well as several possible convolution and deconvolution layers. The super-architecture is able to reduce its size and complexity by switching off entire layers by only passing through the preceding layer's results and discarding the disabled layer with the zero-layer. Since the authors demonstrate the effectiveness of their architecture for emulating the results of climate models, it may also be applicable to the simpler SOSAA model and remove the need for manually trying out different model configurations.

\subsubsection{Training with Auto-Differentiation}

We have briefly introduced auto-differentiation in \Cref{txt:auto-differentiation} as a group of methods to automatically calculate the derivative of a formula while evaluating the formula itself. Today, auto-differentiation is most prominently used in neural network libraries to allow users to define networks only through their forward pass without needing to explicitly derive the backwards propagation as well (see \Cref{txt:neural-network}). For instance, we used auto-differentiation for the adversarial fast gradient sign method in \Cref{txt:adversarial-ood-synthesis-analysis}. However, auto-differentiation could also be applied directly to the SOSAA model source code, e.g. using the open-source Tapenade tool \cite{tapenade-autodiff-2013}. If this version were deployed, SOSAA could calculate the first-order derivatives (or even higher-order derivatives if the auto-differentiation is applied repeatedly) of the output with respect to some inputs on every model run at an additional computational cost. Alternatively, this modified SOSAA version could be used only to collect training data consisting of the predictions and derivatives. Similar to how Stochastic RSMs can be fitted with fewer samples if derivatives are available (see \Cref{txt:stochastic-rsm}), \textcite{sobolev-training-2017} have shown that their Sobolev training method for neural networks produces better models by forcing them to learn not just the target values but also their sensitivities, which also allows them to generalise better at the boundary of the in-distribution.

\subsubsection{Predicting the SOSAA State}

In this feasibility study, we deliberately simplified the prediction problem and trained models to only predict the CCN concentration (see \Cref{txt:ccn-target}), which can also be calculated from the outputs of the SOSAA model. Consequently, in this project, each CCN prediction is made independently of all others, unlike in SOSAA, where some internal state is shared between timesteps. However, a prediction model could also be trained on both the emission and meteorology inputs from the current timestep and the previous state vector and produce the subsequent one. While this state could replicate SOSAA's internal state, which contains the concentrations of thousands of chemical species, the model may also be encouraged to learn a more compressed representation. With such a per-timestep per-box prediction model, SOSAA could be rebuilt with box emulators instead of box models. In particular, this model architecture would provide the same analysis flexibility as SOSAA and have exponentially growing prediction uncertainties.

\subsection{Exploiting the Icarus RSM Confidence and Uncertainty} \label{txt:future-icarus-propagation}

The Icarus RSM architecture requires the RSM to report prediction confidence and uncertainty alongside the prediction value to ensure that the RSM output can be correctly interpreted even when the model is applied to out-of-distribution inputs. These two additional outputs become especially useful in downstream analysis, for which they can be propagated. This subsection highlights how sensitivity analysis and active learning are both easily supported with the Icarus RSM.

\subsubsection{Downstream Sensitivity Analysis}

Supporting efficient sensitivity analysis was among the applications that motivated this project. Similar to how local (specific to a particular input, e.g. \cite{air-quality-sensitivity-2003}) or global (across all model inputs, \cite{global-sensitivity-2015}) sensitivity analysis methods can be applied to SOSAA by gathering the outputs from many SOSAA runs, they can also be applied to the approximate predictions from the SOSAA RSM. In Python, the SaLib library \cite{salib-2017, salib-2.0-2022} provides a convenient interface to calculate the sensitivity of a model either by generating inputs for it or by reading in existing input-output pairs. The only prerequisite for simply running sensitivity analysis on the SOSAA RSM to analyse the SOSAA model is that we trust the RSM to model the response surface of SOSAA accurately.

However, many of the inputs that are emulated to calculate the sensitivity may be outside the RSM's training distribution or in areas of high variance. In this project, we have introduced the Icarus RSM architecture precisely to perform such downstream analysis even when some inputs are out-of-distribution or uncertain. Suppose the sensitivity analysis framework does not directly support propagating the confidence and uncertainty values. In that case, they can instead be propagated using the Monte Carlo method, as outlined in \Cref{txt:icarus-propagation}. Specifically, the sensitivity analysis can be performed several times. On each iteration, each sample's confidence value is interpreted as an inclusion probability, and its value is sampled from a normal distribution parameterised by the prediction mean and uncertainty. While this takes care of propagating RSM uncertainty into sensitivity result uncertainty, the confidence level can be propagated by simply averaging the confidence values of all samples, thus capturing the fraction of the inputs that are being disregarded as OOD.

\subsubsection{Downstream Active Learning}

Active Learning is an interactive process where additional training samples are chosen based on the existing model's performance to maximise its improvement. Especially if collecting the true labels for new training data is expensive, active learning can evaluate several possible new inputs and select the most impactful. The Icarus RSM reports both confidence and uncertainty values for every prediction, where the uncertainty is conditional on the confidence that the input is in-distribution. 

First, let us consider the case where a model was trained on a small subset of the input domain and thus classifies most semantically valid inputs as out-of-distribution. \Cref{txt:ood-synthesis-analysis} has already explored several methods for synthesising out-of-distribution inputs. Particularly adversarial methods as in \Cref{txt:adversarial-ood-synthesis-analysis} are built to generate OOD inputs that showcase the shortest path from in-distribution inputs to the OOD domain. Thus, they can be used to propose the new training inputs that would be most different from existing training data and expand the ID manifold the fastest. The OOD detector could be retrained after every few new-input selections to avoid sampling only OOD inputs that follow the same pattern and instead encourage choosing a diversity of OOD inputs.

Second, reducing the uncertainty of the RSM on in-distribution inputs is also beneficial. The Metropolis-Hastings algorithm \cite{metropolis-algorithm-1953, metropolis-hastings-1970} may be used to sample input points from the in-distribution such that inputs with a higher uncertainty are chosen more often. However, if this uncertainty is mostly aleatoric and thus irreducible (see \Cref{txt:aleatoic-epistemic-uncertainty}), it would be of little use to gather more training data for these inputs. Therefore, the uncertainty should first be disentangled as suggested by \textcite{uncertainty-disentanglement-2022} in \Cref{eq:uncertainty-disentanglement} such that input points with the highest epistemic uncertainty are chosen for additional training data gathering.
